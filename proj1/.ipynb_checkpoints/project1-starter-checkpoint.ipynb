{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "106f6940-0d69-4272-a39b-d7a5449d8e87",
   "metadata": {},
   "source": [
    "# Project 1\n",
    "\n",
    "In this project, you will explore using linear regression to solve a problem\n",
    "in a few different ways.  The particular problem is predicting the miles-per-gallon that a car gets based on a number of characteristics about that car.\n",
    "\n",
    "To complete this project, you will write Python code in places marked\n",
    "`# YOUR CODE HERE`.  There are also code cells in this notebook you must run\n",
    "to produce various kinds of plots and graphs.  There are also a number of cells\n",
    "marked with `# YOUR ANSWER HERE` where you will answer questions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7404cb-6be2-476c-8b4e-d7129cd92c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FILL THESE IN:\n",
    "\n",
    "# Name:\n",
    "# Honor Code Pledge: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac76a6d-b2a7-46b3-bf9a-87f9f1b95994",
   "metadata": {},
   "source": [
    "## Getting started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c37d6f-9a40-488a-9548-6cccd58f90c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b33c4b3-cb6f-4298-8fde-1d56e4b3fa78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "\n",
    "# Write code below to read the CSV file \"cardata.csv\" and put it into a\n",
    "# Pandas dataframe called `df`:\n",
    "\n",
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685c21f0-01c9-46d5-8a60-c959ee0ffa0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first few lines of this data:\n",
    "\n",
    "print(len(df)) # Should be 392\n",
    "df.head()  # Verify this looks ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8f92db-e2eb-423a-9f9f-46163a7a9e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only the numeric attributes\n",
    "\n",
    "# We want to write code to select out the numeric attributes (everything\n",
    "# except the name of the car).  Our 6 features for X will be\n",
    "# 'cylinders', 'displacement', 'horsepower', 'weight', 'acceleration', and 'modelyear'\n",
    "# while the feature we are predicting will be 'mpg'.\n",
    "\n",
    "# Write code below to create two new dataframes, one that will become our\n",
    "# X matrix and one that will become our vector of y values.  \n",
    "# The X matrix should be called df_X and the vector of y values \n",
    "# should be called df_y.\n",
    "\n",
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd5c106-516b-4d56-9a63-61badb5f4fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first few lines of this data:\n",
    "\n",
    "print(len(df_X)) # Should be 392\n",
    "print(len(df_y)) # Should be 392\n",
    "df_X.head()  # Verify this looks ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f24666-4296-4418-bcf2-a4920429c149",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the data\n",
    "\n",
    "# Run this cell to generate some plots of our input features displayed\n",
    "# against the feature we're trying to predict (mpg).\n",
    "\n",
    "fig, ax = plt.subplots(6, figsize=(8,30))\n",
    "ax[0].scatter(df['cylinders'], df['mpg'])\n",
    "ax[1].scatter(df['displacement'], df['mpg'])\n",
    "ax[2].scatter(df['horsepower'], df['mpg'])\n",
    "ax[3].scatter(df['weight'], df['mpg'])\n",
    "ax[4].scatter(df['acceleration'], df['mpg'])\n",
    "ax[5].scatter(df['modelyear'], df['mpg'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07d0871-61f8-44ec-a6e3-0dec1ce5960b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to numpy arrays\n",
    "\n",
    "# Write code below to create two new numpy arrays, one for df_X\n",
    "# called X_train_initial and one for df_y called y_train.\n",
    "# The name X_train_initial will make sense in a minute.\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "\n",
    "# Verify data:\n",
    "\n",
    "print(X_train_initial.shape) # Should be (392, 6)\n",
    "print(y_train.shape) # Should be (392,)  <--- note ONE dimensional!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2f6791-d23c-4470-88cc-2d0ec76a7095",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up dimensional variables.\n",
    "\n",
    "# We use m for the number of training examples, and \n",
    "# n for the number of features.  \n",
    "# Calculate these from the data (not hard-coded):\n",
    "\n",
    "# YOUR CODE HERE:\n",
    "\n",
    "\n",
    "# Verify data:\n",
    "\n",
    "print(m, n)  # should print 392 6, meaning 392 training examples and 6 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac2c9f5-51b3-4890-ba0a-463380d72ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write code below to add a column of ones to the X_train_initial\n",
    "# matrix, and store this in a new matrix X_train:\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "\n",
    "# Verify new dimensions:\n",
    "\n",
    "print(X_train.shape) # Should be (392, 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8ed657-5429-42f8-986d-6431789adece",
   "metadata": {},
   "source": [
    "## Part A: Solving linear regression the \"easy\" way\n",
    "\n",
    "Linear regression happens to be one of the few machine learning\n",
    "algorithms where it is possible to directly \"solve\" it: we can \n",
    "compute the true values for the $w$ vector that will minimize the cost\n",
    "function $J(\\boldsymbol{w})$ by using the calculation:\n",
    "\n",
    "\n",
    "$$\\boldsymbol{w} = (X^T X)^{-1}X^T y$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d08cdf1-77c4-47fc-80f3-b0eb47caaacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write code below to compute the w vector directly using\n",
    "# the equation above.  Store this in a vector called w_direct.\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "\n",
    "# Verify:\n",
    "\n",
    "print(w_direct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1509080e-b59c-40df-b67c-309b74843cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below, write a sentence about how to interpret these \n",
    "# numbers in w_direct, in particular, (1) why are some negative\n",
    "# and some positive, and (2) what is the special interpretation of\n",
    "# w_direct[0]?\n",
    "\n",
    "# YOUR ANSWER HERE:\n",
    "#\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b013465e-2638-43e4-994d-443704b75908",
   "metadata": {},
   "source": [
    "## Part B: Linear regression the \"hard\" way\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdcc2124-e1df-462d-815e-cd44995f6726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the make_prediction function below to make a\n",
    "# prediction for one example (one feature vector x_data)\n",
    "# and parameter vector w.  \n",
    "# Do this using the dot product (np.dot() function), rather than a loop.\n",
    "\n",
    "def make_prediction(x_data, w):\n",
    "    \"\"\"\n",
    "    x_data: array of features (n+1)\n",
    "    w: array of weights (n+1)\n",
    "    returns: scalar\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d8b06b-d250-46da-9442-f1066dd48623",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the compute_cost function below to compute the\n",
    "# total cost over the entire data set X_data and y_data,\n",
    "# given parameters vector w.\n",
    "# Do not use matrix computations here; call your make_prediction() function\n",
    "# that you defined above.  You should have one loop.\n",
    "\n",
    "def compute_cost(X_data, y_data, w):\n",
    "    \"\"\"\n",
    "    X_data: matrix (m, n+1)\n",
    "    y_data: array of true y values (m)\n",
    "    w: array of weights (n+1)\n",
    "    returns: scalar\n",
    "    \"\"\"\n",
    "        \n",
    "    # YOUR CODE HERE\n",
    "    pass\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86bb7183-e83f-49a0-b7d8-cb944b7e2697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write code below to compute the cost of our training \n",
    "# data on the directly-computed values of w.  That is, compute\n",
    "# J(w_direct) and store this in a variable called w_direct_cost.\n",
    "# We know we can't do any better than this cost!\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "\n",
    "# Verify:\n",
    "\n",
    "print(\"Minimum cost:\", w_direct_cost) # should be less than 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6594861-4e11-4da3-b634-eb7fee42b977",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the compute_gradient function below to compute\n",
    "# the complete gradient for the function J(w).  \n",
    "# Do not use matrix computations here; call your make_prediction() function\n",
    "# that you defined above.  You should have two nested loops.\n",
    "\n",
    "def compute_gradient(X_data, y_data, w):\n",
    "    \"\"\"\n",
    "    X_data: matrix (m, n+1)\n",
    "    y_data: array of true y values (m)\n",
    "    w: array of weights (n+1)\n",
    "    returns: array of gradients (n+1)\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48732932-3e40-4d9a-8f1a-e23209c2ed28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write code here to perform gradient descent, using your\n",
    "# functions above.  You should use three new variables in your\n",
    "# code:\n",
    "# - w_manual: which is the vector of weights that gradient\n",
    "#   descent is designed to find:\n",
    "# - w_manual_cost, which is the cost of these weights,\n",
    "# - J_list, which is the list of \n",
    "#   costs determined by compute_cost() [like in the in-class lab we did].\n",
    "\n",
    "# Setup these vars:\n",
    "w_manual = np.zeros(n+1)  # n+1 weights\n",
    "w_manual_cost = 0\n",
    "J_list = []\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "\n",
    "# Verify:\n",
    "    \n",
    "print(\"Final w:\", w_manual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458ce1c8-815b-480a-8bf0-5ecc99c6721b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the cost as a function of number of iterations of the\n",
    "# gradient descent algorithm.\n",
    "\n",
    "plt.scatter(range(0, len(J_list)), J_list)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743f7b0e-eef3-4649-a5c5-c08a36fa1d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep playing around with gradient descent until you have a good\n",
    "# learning curve in the plot above (something that appears to flatten out).\n",
    "# Then answer the questions below. \n",
    "\n",
    "# What was your initial choice for alpha?  Your final choice?  How did\n",
    "# you arrive at these choices?\n",
    "#\n",
    "# YOUR ANSWER HERE\n",
    "#\n",
    "# How many iterations of gradient descent did you need until convergence?\n",
    "#\n",
    "# YOUR ANSWER HERE\n",
    "#\n",
    "# What was your final vector of weights? (w_manual)\n",
    "#\n",
    "# YOUR ANSWER HERE\n",
    "#\n",
    "# What was your final cost of these weights? (w_manual_cost)\n",
    "#\n",
    "# YOUR ANSWER HERE\n",
    "#\n",
    "# What was your final vector of weights from Part A? (w_direct)\n",
    "#\n",
    "# YOUR ANSWER HERE\n",
    "#\n",
    "# What cost of these weights? (w_direct_cost)\n",
    "#\n",
    "# YOUR ANSWER HERE\n",
    "#\n",
    "# Why do you think the final weights and costs from parts A and B are so different?\n",
    "#\n",
    "# YOUR ANSWER HERE\n",
    "#\n",
    "#\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba890ae-126b-4c76-af2a-37ec64336c51",
   "metadata": {},
   "source": [
    "## Part C: Linear regression with feature scaling and matrix computations\n",
    "\n",
    " In this section, you will perform linear regression one more time,\n",
    " but adding in feature scaling and matrix computations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0509bd88-9b00-4e2d-a53b-0b2d1ccd8957",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write code here to take your original data and scale\n",
    "# all the features using Z-score scaling.\n",
    "# I recommend doing this using the pandas df_X dataframe,\n",
    "# then producing a new numpy matrix at the end.\n",
    "# Call this matrix X_train_normed_initial.\n",
    "#\n",
    "# Hint: this first line of this matrix should be:\n",
    "# [ 1.48205303,  1.07591459,  0.66328508,  0.61974833, -1.2836176, -1.6232409 ]\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f0a32b-6182-48c8-9787-be4085bd0c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify normed data:\n",
    "\n",
    "print(X_train_normed_initial.shape) # Should be (392, 6)\n",
    "\n",
    "# Write code below to add a column of ones to the X_train_normed_initial\n",
    "# matrix, and produce a new matrix X_train_normed.\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "\n",
    "\n",
    "# Verify new dimensions:\n",
    "\n",
    "print(X_train_normed.shape) # Should be (392, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2de1136-1611-4a83-86e5-07d28ae032c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write code below to compute the w vector directly using\n",
    "# the same matrix equation from Part A, but using the normed\n",
    "# training data (X_train_normed), not X_train.\n",
    "# Store this in a vector called w_direct_normed.  \n",
    "# NOTE: Because the X matrix is now scaled (normalized), w_direct_normed\n",
    "# will have very different numbers than w_direct.\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "\n",
    "# Verify:\n",
    "\n",
    "print(w_direct_normed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238c9fe6-4c12-4e66-a2c2-9111cd7f4270",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write code below to compute the cost of this new\n",
    "# vector of weights (w_direct_normed), and store it in a variable\n",
    "# called w_direct_normed_cost.  Verify this cost is the same\n",
    "# cost as w_direct_cost (from Part A).  Again, don't forget to \n",
    "# compute the cost for w_direct_normed using X_train_normed.\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "\n",
    "# Verify:\n",
    "\n",
    "print(w_direct_cost)\n",
    "print(w_direct_normed_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe35b83-e793-4048-b39a-0999ea8ca818",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rewrite the compute_cost function as a new version\n",
    "# (compute_cost_fast) to compute the\n",
    "# total cost over the entire data set X_data and y_data,\n",
    "# given parameters vector w, but now use matrix computations\n",
    "# rather than loops.  Do not call make_prediction.\n",
    "\n",
    "def compute_cost_fast(X_data, y_data, w):\n",
    "    \"\"\"\n",
    "    X_data: matrix (m, n+1)\n",
    "    y_data: array of true y values (m)\n",
    "    w: array of weights (n+1)\n",
    "    returns: scalar\n",
    "    \"\"\"\n",
    "        \n",
    "    # YOUR CODE HERE\n",
    "    pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf58ff6-231c-49ba-900b-22e900eb1f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rewrite the compute_gradient function as a new version\n",
    "# (compute_gradient_fast) to compute\n",
    "# the complete gradient for the function J(w).\n",
    "\n",
    "def compute_gradient_fast(X_data, y_data, w):\n",
    "    \"\"\"\n",
    "    X_data: matrix (m, n+1)\n",
    "    y_data: array of true y values (m)\n",
    "    w: array of weights (n+1)\n",
    "    returns: array of gradients (n+1)\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    pass\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bbee33f-f0b9-40db-a786-ce24dc9b39c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write code here to perform gradient descent, using your\n",
    "# NEW FAST functions above.  You should use three new variables in your\n",
    "# code:\n",
    "# - w_manual_normed: which is the vector of weights that gradient\n",
    "#   descent is designed to find:\n",
    "# - w_manual_normed_cost, which is the cost of these weights,\n",
    "# - J_list_fast, which is the list of \n",
    "#   costs determined by compute_cost() [like in the in-class lab we did].\n",
    "\n",
    "# Setup these vars:\n",
    "w_manual_normed = np.zeros(n+1)  # n+1 weights\n",
    "w_manual_normed_cost = 0\n",
    "J_list_fast = []\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "\n",
    "# Verify:\n",
    "    \n",
    "print(\"Final w:\", w_manual_normed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4064cac-227a-41b5-9bd5-c3ca406986f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot the cost as a function of number of iterations of the\n",
    "# gradient descent algorithm.\n",
    "\n",
    "plt.scatter(range(0, len(J_list_fast)), J_list_fast)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35bfb791-51bc-4ecf-8e82-38cf8f4b1a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep playing around with gradient descent until you have a good\n",
    "# learning curve in the plot above (something that appears to flatten out).\n",
    "# Then answer the questions below. \n",
    "\n",
    "# What was your initial choice for alpha?  Your final choice?  How did\n",
    "# you arrive at these choices?\n",
    "#\n",
    "# YOUR ANSWER HERE \n",
    "#\n",
    "# How did your final choice for alpha here (Part C) differ from Part B?\n",
    "#\n",
    "# YOUR ANSWER HERE \n",
    "#\n",
    "# How many iterations of gradient descent did you need until convergence\n",
    "# here (Part C) versus Part A?\n",
    "#\n",
    "# YOUR ANSWER HERE \n",
    "#\n",
    "# What was your final vector of weights? (w_manual_normed)\n",
    "#\n",
    "# YOUR ANSWER HERE \n",
    "#\n",
    "# What was your final cost of these weights? (w_manual_cost_normed)\n",
    "#\n",
    "# YOUR ANSWER HERE \n",
    "#\n",
    "# What was your final vector of calculated directly ? (w_direct_normed)\n",
    "#\n",
    "# YOUR ANSWER HERE \n",
    "#\n",
    "# What cost of these weights? (w_direct_cost_normed)\n",
    "#\n",
    "# YOUR ANSWER HERE \n",
    "#\n",
    "# Why do you think the final weights from both sections of Part C are so similar?\n",
    "#\n",
    "#\n",
    "#\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62bf0e1-7e74-4be0-8660-e521d583496f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final checkpoint\n",
    "\n",
    "# All of these should print OK and match up with what you have above:\n",
    "\n",
    "print(\"Part A\")\n",
    "print(\"Weights:\", w_direct)\n",
    "print(\"Cost:\", w_direct_cost)\n",
    "print()\n",
    "print(\"Part B\")\n",
    "print(\"Weights:\", w_manual)\n",
    "print(\"Cost:\", w_manual_cost)\n",
    "print()\n",
    "print(\"Part C\")\n",
    "print(\"Weights (calculated directly):\", w_direct_normed)\n",
    "print(\"Cost:\", w_direct_normed_cost)\n",
    "print()\n",
    "print(\"Weights (calculated with gradient descent):\", w_manual_normed)\n",
    "print(\"Cost:\", w_manual_normed_cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c70f6f-f884-4e12-8f50-1bcd275455e5",
   "metadata": {},
   "source": [
    "# Part D (Bonus; up to 5 extra credit points)\n",
    "\n",
    "Notice that some of the features in the initial plots at the beginning of the notebook\n",
    "seem to have non-linear correlations with `mpg`.  Try to add some non-linear features\n",
    "to the data and see if you can decrease the cost even further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86fd3a72-e5dd-4de0-8c82-594d75a770ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
