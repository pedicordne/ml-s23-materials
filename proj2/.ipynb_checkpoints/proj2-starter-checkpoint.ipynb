{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abfb2a24-05ab-4b1d-8fc2-7448458663bd",
   "metadata": {},
   "source": [
    "# Project 2\n",
    "\n",
    "## Introduction\n",
    "\n",
    "<img src=goodplace2.png align=right width=500>\n",
    "\n",
    "The TV show *The Good Place* is centered around a number of humans who have died and find themselves in the afterlife.  In this conception\n",
    "of the afterlife, humans are sent to \"the Good Place\" or \"the Bad Place\" after death.  All humans are assigned a numerical score based on the morality of their conduct in life, and only those with the very highest scores are sent to the \"Good Place\", where they enjoy eternal happiness; all others experience an eternity of torture in the \"Bad Place.\"\n",
    "\n",
    "In this project, you will explore using logistic regression to predict whether someone will end up in the \"Good Place or the \"Bad Place\" based on an\n",
    "extremely scaled down version of their conduct in life.  In particular, we have data for 1000 people about how often they:\n",
    "\n",
    "- Let someone merge in front of them in traffic\n",
    "- Didn't tip their server at a restaurant\n",
    "- Held a door open for someone who was walking behind them\n",
    "- Littered\n",
    "\n",
    "These will be our four features for the problem.  Our data set consists of these four features tallied for 1000 different people.\n",
    "\n",
    "To complete this project, you will write Python code in places marked\n",
    "`# YOUR CODE HERE`.  There are also code cells in this notebook you must run\n",
    "to produce various kinds of plots and graphs.  There are also a number of cells\n",
    "marked with `# YOUR ANSWER HERE` where you will answer questions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4b502d1-f3c7-4bed-9683-4643bbe74731",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name: Nikki Pedicord\n",
    "# Honor Pledge: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "3a37aed2-9e9c-43c6-a7fb-0d93fddff848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "f10cb214-0b0d-4fbc-b6e9-38a4b4e066e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>letMerge</th>\n",
       "      <th>noTip</th>\n",
       "      <th>heldDoor</th>\n",
       "      <th>littered</th>\n",
       "      <th>goodbad</th>\n",
       "      <th>noisygoodbad</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>102</td>\n",
       "      <td>7</td>\n",
       "      <td>good</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>24</td>\n",
       "      <td>40</td>\n",
       "      <td>295</td>\n",
       "      <td>224</td>\n",
       "      <td>bad</td>\n",
       "      <td>bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>42</td>\n",
       "      <td>8</td>\n",
       "      <td>356</td>\n",
       "      <td>182</td>\n",
       "      <td>bad</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>194</td>\n",
       "      <td>20</td>\n",
       "      <td>485</td>\n",
       "      <td>193</td>\n",
       "      <td>good</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>196</td>\n",
       "      <td>24</td>\n",
       "      <td>376</td>\n",
       "      <td>175</td>\n",
       "      <td>good</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>37</td>\n",
       "      <td>3</td>\n",
       "      <td>150</td>\n",
       "      <td>137</td>\n",
       "      <td>bad</td>\n",
       "      <td>bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>15</td>\n",
       "      <td>17</td>\n",
       "      <td>439</td>\n",
       "      <td>234</td>\n",
       "      <td>bad</td>\n",
       "      <td>bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>67</td>\n",
       "      <td>22</td>\n",
       "      <td>480</td>\n",
       "      <td>113</td>\n",
       "      <td>good</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>57</td>\n",
       "      <td>20</td>\n",
       "      <td>470</td>\n",
       "      <td>54</td>\n",
       "      <td>good</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>141</td>\n",
       "      <td>4</td>\n",
       "      <td>447</td>\n",
       "      <td>74</td>\n",
       "      <td>good</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     letMerge  noTip  heldDoor  littered goodbad noisygoodbad\n",
       "0          13      0       102         7    good         good\n",
       "1          24     40       295       224     bad          bad\n",
       "2          42      8       356       182     bad         good\n",
       "3         194     20       485       193    good         good\n",
       "4         196     24       376       175    good         good\n",
       "..        ...    ...       ...       ...     ...          ...\n",
       "995        37      3       150       137     bad          bad\n",
       "996        15     17       439       234     bad          bad\n",
       "997        67     22       480       113    good         good\n",
       "998        57     20       470        54    good         good\n",
       "999       141      4       447        74    good         good\n",
       "\n",
       "[1000 rows x 6 columns]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read data\n",
    "\n",
    "# Write code below to read the CSV file \"data1.csv\" and put it into a\n",
    "# Pandas dataframe called `df`:\n",
    "\n",
    "# YOUR CODE HERE\n",
    "df = pd.read_csv(\"data1.csv\")\n",
    "df  # The last expression in a cell is auto-evaluated and printed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a7ce96-70c8-4ae6-aecf-2d06692fe2e8",
   "metadata": {},
   "source": [
    "## Explanation of the data file\n",
    "\n",
    "Each row of the file represents data about a person.  \n",
    "\n",
    "The first four columns should be self-explanatory: they tell how often a person did a\n",
    "certain activity (explained above).  There are two columns at the end saying whether they\n",
    "ended up in the \"Good Place\" or the \"Bad Place.\"\n",
    "\n",
    "The first of the two (`goodbad`) is calculated \"perfectly\" from a formula I came up with (that I'm keeping secret!)\n",
    "\"Perfectly\" meaning that the formula itself probably isn't perfect, but the good/bad column is calculated\n",
    "directly mathematically from the formula, based on the four features.\n",
    "\n",
    "The second of the two (`noisygoodbad`) is also calculated perfectly from the formula, but\n",
    "with some \"noise\" thrown in.  In other words, I've switched a few of the goods to bads and vice versa, to\n",
    "simulate a real-world situation (where the Good Place/Bad Place determination is based not only on this\n",
    "data, but other data as well that we don't have access to)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "1e3786e4-a047-4030-bf20-e97b083b15df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>letMerge</th>\n",
       "      <th>noTip</th>\n",
       "      <th>heldDoor</th>\n",
       "      <th>littered</th>\n",
       "      <th>goodbad</th>\n",
       "      <th>noisygoodbad</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>102</td>\n",
       "      <td>7</td>\n",
       "      <td>good</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>24</td>\n",
       "      <td>40</td>\n",
       "      <td>295</td>\n",
       "      <td>224</td>\n",
       "      <td>bad</td>\n",
       "      <td>bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>42</td>\n",
       "      <td>8</td>\n",
       "      <td>356</td>\n",
       "      <td>182</td>\n",
       "      <td>bad</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>194</td>\n",
       "      <td>20</td>\n",
       "      <td>485</td>\n",
       "      <td>193</td>\n",
       "      <td>good</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>196</td>\n",
       "      <td>24</td>\n",
       "      <td>376</td>\n",
       "      <td>175</td>\n",
       "      <td>good</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>37</td>\n",
       "      <td>3</td>\n",
       "      <td>150</td>\n",
       "      <td>137</td>\n",
       "      <td>bad</td>\n",
       "      <td>bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>15</td>\n",
       "      <td>17</td>\n",
       "      <td>439</td>\n",
       "      <td>234</td>\n",
       "      <td>bad</td>\n",
       "      <td>bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>67</td>\n",
       "      <td>22</td>\n",
       "      <td>480</td>\n",
       "      <td>113</td>\n",
       "      <td>good</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>57</td>\n",
       "      <td>20</td>\n",
       "      <td>470</td>\n",
       "      <td>54</td>\n",
       "      <td>good</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>141</td>\n",
       "      <td>4</td>\n",
       "      <td>447</td>\n",
       "      <td>74</td>\n",
       "      <td>good</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     letMerge  noTip  heldDoor  littered goodbad noisygoodbad\n",
       "0          13      0       102         7    good         good\n",
       "1          24     40       295       224     bad          bad\n",
       "2          42      8       356       182     bad         good\n",
       "3         194     20       485       193    good         good\n",
       "4         196     24       376       175    good         good\n",
       "..        ...    ...       ...       ...     ...          ...\n",
       "995        37      3       150       137     bad          bad\n",
       "996        15     17       439       234     bad          bad\n",
       "997        67     22       480       113    good         good\n",
       "998        57     20       470        54    good         good\n",
       "999       141      4       447        74    good         good\n",
       "\n",
       "[1000 rows x 6 columns]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the first few and last few lines of this data:\n",
    "\n",
    "print(len(df)) # Should be 1000\n",
    "df  # Verify this looks ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "88a4df79-02ae-47de-aef3-c381bd00a97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select our X and y data\n",
    "\n",
    "# First, we will split the data frame above into a four-column frame\n",
    "# with the input features (X's) and a one-column frame with the target\n",
    "# feature (y), which we will use the noisy column (noisygoodbad).\n",
    "\n",
    "# Write code below to create df_X with just the four X feature columns,\n",
    "# and df_y that has just the noisygoodbad column.\n",
    "\n",
    "# Then **normalize** the X values with Z-score normalization as in \n",
    "# project 1.\n",
    "\n",
    "# YOUR CODE HERE\n",
    "df_X = df[['letMerge', 'noTip', 'heldDoor', 'littered']]\n",
    "df_y = df['noisygoodbad']\n",
    "\n",
    "mean_df_X = df_X.mean()\n",
    "\n",
    "std_df_X = df_X.std()\n",
    "std_df_X\n",
    "\n",
    "zscore = (df_X - mean_df_X) / std_df_X\n",
    "df_X = zscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "5db28502-695c-4caa-994c-d5bd1f929bfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>letMerge</th>\n",
       "      <th>noTip</th>\n",
       "      <th>heldDoor</th>\n",
       "      <th>littered</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.161413</td>\n",
       "      <td>-1.267048</td>\n",
       "      <td>-1.350553</td>\n",
       "      <td>-1.620752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.989370</td>\n",
       "      <td>2.629464</td>\n",
       "      <td>0.136883</td>\n",
       "      <td>1.396890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.707846</td>\n",
       "      <td>-0.487746</td>\n",
       "      <td>0.607005</td>\n",
       "      <td>0.812830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.669471</td>\n",
       "      <td>0.681208</td>\n",
       "      <td>1.601198</td>\n",
       "      <td>0.965798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.700751</td>\n",
       "      <td>1.070859</td>\n",
       "      <td>0.761143</td>\n",
       "      <td>0.715487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>-0.786047</td>\n",
       "      <td>-0.974810</td>\n",
       "      <td>-0.980621</td>\n",
       "      <td>0.187052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>-1.130132</td>\n",
       "      <td>0.388969</td>\n",
       "      <td>1.246679</td>\n",
       "      <td>1.535952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>-0.316840</td>\n",
       "      <td>0.876033</td>\n",
       "      <td>1.562663</td>\n",
       "      <td>-0.146696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>-0.473242</td>\n",
       "      <td>0.681208</td>\n",
       "      <td>1.485594</td>\n",
       "      <td>-0.967161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>0.840538</td>\n",
       "      <td>-0.877397</td>\n",
       "      <td>1.308335</td>\n",
       "      <td>-0.689037</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     letMerge     noTip  heldDoor  littered\n",
       "0   -1.161413 -1.267048 -1.350553 -1.620752\n",
       "1   -0.989370  2.629464  0.136883  1.396890\n",
       "2   -0.707846 -0.487746  0.607005  0.812830\n",
       "3    1.669471  0.681208  1.601198  0.965798\n",
       "4    1.700751  1.070859  0.761143  0.715487\n",
       "..        ...       ...       ...       ...\n",
       "995 -0.786047 -0.974810 -0.980621  0.187052\n",
       "996 -1.130132  0.388969  1.246679  1.535952\n",
       "997 -0.316840  0.876033  1.562663 -0.146696\n",
       "998 -0.473242  0.681208  1.485594 -0.967161\n",
       "999  0.840538 -0.877397  1.308335 -0.689037\n",
       "\n",
       "[1000 rows x 4 columns]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sanity check for df_X\n",
    "\n",
    "df_X  # Should print a data frame with 1000 rows and 4 columns.\n",
    "# First row should be [ -1.161413 -1.267048 -1.350553 -1.620752]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "1f31299c-bed6-44be-bd11-bb5eb84ba006",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      good\n",
       "1       bad\n",
       "2      good\n",
       "3      good\n",
       "4      good\n",
       "       ... \n",
       "995     bad\n",
       "996     bad\n",
       "997    good\n",
       "998    good\n",
       "999    good\n",
       "Name: noisygoodbad, Length: 1000, dtype: object"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sanity check for df_y\n",
    "\n",
    "df_y  # Should be a column of goods and bads, starting with good, bad, good, good, good...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "6667ab3c-7106-4b57-b612-4756034136bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      1\n",
       "1      0\n",
       "2      1\n",
       "3      1\n",
       "4      1\n",
       "      ..\n",
       "995    0\n",
       "996    0\n",
       "997    1\n",
       "998    1\n",
       "999    1\n",
       "Name: noisygoodbad, Length: 1000, dtype: int32"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we need to switch df_y to not have good/bad strings, but rather 0's and 1's.\n",
    "# Use this line of code:\n",
    "\n",
    "df_y = (df_y == 'good').astype(int)\n",
    "\n",
    "# Sanity check: should now be a column of ones and zeros, with 1=good, 0=bad.  \n",
    "df_y   # Should begin 1, 0, 1, 1, 1, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "a6d6faa7-5308-4884-ac49-ed5af1eb15a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 4)\n",
      "(1000,)\n"
     ]
    }
   ],
   "source": [
    "# Sanity check shapes:\n",
    "\n",
    "print(df_X.shape) # Should be (1000, 4)\n",
    "print(df_y.shape) # Should be (1000,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "a7ea4ac9-0c3c-470b-8425-2da67ca44c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing.\n",
    "\n",
    "# We want to write code to split the data frame above into a few\n",
    "# new data frames.  In particular, are going to have a TRAINING SET\n",
    "# and a TESTING SET for this project.  We will use 80% of the data for \n",
    "# training, and the remaining 20% for testing.  \n",
    "\n",
    "# In the real world, we would split the data randomly, but so we all\n",
    "# end up with the same results, we will use the first 80% of the data\n",
    "# for training, and the last 20% for testing (in order of how the rows\n",
    "# show up in the file).  Note that there are 1000 people (rows in \n",
    "# the file), so the first 800 rows will be training, and the last 200\n",
    "# will be testing.\n",
    "\n",
    "# Write code here to create FOUR NUMPY ndarrays:\n",
    "\n",
    "# - X_train: first 800 lines of df_X\n",
    "# - X_test: last 200 lines of df_X\n",
    "# - y_train: first 800 lines of df_y\n",
    "# - y_test: last 200 lines of df_y\n",
    "\n",
    "# Then, add a column of ones to the left side of X_train and X_test.\n",
    "\n",
    "X_train0 = df_X[:800].to_numpy()\n",
    "X_test0 = df_X.tail(200).to_numpy()\n",
    "y_train = df_y.iloc[:800].to_numpy()\n",
    "y_test = df_y.tail(200).to_numpy()\n",
    "\n",
    "X_train = np.insert(X_train0, 0, 1, axis=1)\n",
    "X_test = np.insert(X_test0, 0, 1, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "7a3f59aa-ba82-4ec5-9234-b390140e4800",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800, 5)\n",
      "(800,)\n",
      "(200, 5)\n",
      "(200,)\n"
     ]
    }
   ],
   "source": [
    "# Sanity checks:\n",
    "\n",
    "print(X_train.shape) # Should be (800, 5)\n",
    "print(y_train.shape) # Should be (800,) \n",
    "print(X_test.shape) # Should be (200, 5)\n",
    "print(y_test.shape) # Should be (200,) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "129456c3-f616-41bd-a93e-79d585030e51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 training examples:\n",
      "[[ 1.         -1.16141295 -1.26704849 -1.35055312 -1.62075162  1.        ]\n",
      " [ 1.         -0.98937031  2.62946412  0.13688262  1.3968899   0.        ]\n",
      " [ 1.         -0.70784599 -0.48774597  0.6070048   0.81283025  1.        ]\n",
      " [ 1.          1.66947051  0.68120782  1.6011976   0.96579826  1.        ]\n",
      " [ 1.          1.70075099  1.07085908  0.76114322  0.71548698  1.        ]\n",
      " [ 1.          0.74669634  0.77862063 -0.31782571  0.24267678  0.        ]\n",
      " [ 1.          0.60593418 -0.97481004  0.76114322 -1.28700325  1.        ]\n",
      " [ 1.         -0.86424839 -1.26704849  1.06942006 -0.78638069  1.        ]\n",
      " [ 1.          2.38892156 -0.6825716  -0.75712021 -0.49435087  1.        ]\n",
      " [ 1.         -1.36473607  1.55792315 -0.66463716 -0.52216323  0.        ]]\n",
      "\n",
      "First 10 testing examples:\n",
      "[[ 1.          0.48081226 -0.6825716   0.05210649  0.59033134  1.        ]\n",
      " [ 1.         -0.87988863 -0.29292034 -0.387188    0.0758026   0.        ]\n",
      " [ 1.          1.09078163  1.75274879 -0.80336173  0.72939316  0.        ]\n",
      " [ 1.         -0.2855595  -0.39033315 -1.35055312  0.08970878  0.        ]\n",
      " [ 1.         -0.73912647  1.07085908 -1.58946767  1.16048481  0.        ]\n",
      " [ 1.          0.8092573  -1.26704849  0.40662486 -0.99497343  1.        ]\n",
      " [ 1.          1.18462307  1.8501616  -0.84960326 -1.12012907  1.        ]\n",
      " [ 1.         -1.23961415 -0.77998441  0.54534943 -0.50825705  1.        ]\n",
      " [ 1.         -0.16043758 -0.87739723 -1.65882996  0.71548698  0.        ]\n",
      " [ 1.          0.48081226 -1.16963567 -0.70317176  1.20220335  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Show first few rows of training/testing data:  (will be useful to have these later)\n",
    "\n",
    "print(\"First 10 training examples:\")\n",
    "print(np.hstack([X_train, y_train.reshape(-1, 1)])[0:10])\n",
    "print()\n",
    "print(\"First 10 testing examples:\")\n",
    "print(np.hstack([X_test, y_test.reshape(-1, 1)])[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f575209-8473-461d-99d3-51e482549cc7",
   "metadata": {},
   "source": [
    "## Part A\n",
    "\n",
    "Like in Part A of the previous project, we will rely on an external method to create\n",
    "a logistic regression model for us, then we will see if we can replicate it ourselves.\n",
    "\n",
    "Below is code that uses scikit-learn to do this for us.  Don't worry too much about what it does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "102fdfea-65d7-4803-9ea4-c2fa5f56e9a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\nikki\\anaconda3\\lib\\site-packages (1.0.2)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\nikki\\anaconda3\\lib\\site-packages (from scikit-learn) (1.1.0)\n",
      "Requirement already satisfied: scipy>=1.1.0 in c:\\users\\nikki\\anaconda3\\lib\\site-packages (from scikit-learn) (1.7.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\nikki\\anaconda3\\lib\\site-packages (from scikit-learn) (2.2.0)\n",
      "Requirement already satisfied: numpy>=1.14.6 in c:\\users\\nikki\\anaconda3\\lib\\site-packages (from scikit-learn) (1.21.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "w found through scikit-learn: [ 2.17303834  6.40434357 -6.47511472  8.86631945 -9.64155641]\n"
     ]
    }
   ],
   "source": [
    "# Download and install scikit-learn if not already done:\n",
    "%pip install scikit-learn\n",
    "\n",
    "# Import the logistic regression functionality from scikit-learn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Create a logistic regression model and train it on our training data:\n",
    "model = LogisticRegression(random_state=0, penalty='none', fit_intercept=False).fit(X_train, y_train)\n",
    "\n",
    "# If the line above gives an error about penalty=None, try switching that part to penalty='none' instead.\n",
    "\n",
    "# model.coef_ contains the w vector that this logistic regression model was able to find, so\n",
    "# we'll treat it as the \"best\" w and see if we can match it manually.\n",
    "w_direct = model.coef_[0]\n",
    "\n",
    "print(\"w found through scikit-learn:\", w_direct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "6cc5e005-4294-4c93-a8a7-227c6db31309",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below, write a sentence about how to interpret these \n",
    "# numbers in w_direct, in particular, (1) why are some negative\n",
    "# and some positive, and (2) what is the special interpretation of\n",
    "# w_direct[0]?\n",
    "\n",
    "# YOUR ANSWER HERE:\n",
    "#it is the weight of each variable \n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3031723e-d89c-4613-8eb7-a8bd7cd16b64",
   "metadata": {},
   "source": [
    "## Part B\n",
    "\n",
    "In this part you will write code for binary logistic regression by hand, including the model,\n",
    "the loss function, the cost function, and gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "79ea1eb0-3006-4789-a7da-50bd839697f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define sigmoid function\n",
    "\n",
    "# Write code here to define the sigmoid function 1/(1+e^-x).  \n",
    "# IMPORTANT: Use the np.exp function to raise e to a power.  \n",
    "\n",
    "def sigmoid(x):\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    ans = 1 / ( 1 + np.exp(-x))\n",
    "    \n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "a0d7adf2-20fe-4f18-a59f-863ffaf8f5e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5 0.2689414213699951 0.6224593312018546\n"
     ]
    }
   ],
   "source": [
    "# Sanity check\n",
    "\n",
    "print(sigmoid(0), sigmoid(-1), sigmoid(0.5))\n",
    "\n",
    "# should print 0.5 0.2689414213699951 0.6224593312018546"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "fb4f2961-7789-4ff7-8aec-35e140601a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function called run_model below to run the logistic\n",
    "# regression model on one feature vector (x_data).\n",
    "# In other words, this function should compute 1/(1 + e^(-wx))\n",
    "# where x is x_data.  But do this by calling your sigmoid function\n",
    "# and the dot product function (np.dot()).\n",
    "\n",
    "def run_model(x_data, w):\n",
    "    \"\"\"\n",
    "    x_data: array of features (n+1)\n",
    "    w: array of weights (n+1)\n",
    "    returns: scalar\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    wx = np.dot(x_data, w)\n",
    "    ans = sigmoid(wx)\n",
    "    \n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "9d36a583-5503-4b8b-ace8-851d3b0e5f32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.         -1.16141295 -1.26704849 -1.35055312 -1.62075162]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9986297185506662"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sanity check: run the model from Part A on the first testing example\n",
    "print(X_train[0])\n",
    "run_model(X_train[0], w_direct)  # should be 0.9986297185506662"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "111adf69-bbe2-4bab-a09e-161deb3aea1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## QUESTION:\n",
    "\n",
    "# The run_model function only returns numbers in a certain range.  What is this range\n",
    "# and why does this function not return numbers outside of that range?\n",
    "\n",
    "# ANSWER:\n",
    "# 0 to 1\n",
    "# \n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "5638376f-586d-4fdc-bb96-4a1d4d322993",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function called make_prediction that will\n",
    "# actually predict the class 0 or 1 for a feature vector x_data.\n",
    "# To do this, just call run_model and check if the return\n",
    "# value is > or < than 0.5\n",
    "\n",
    "def make_prediction(x_data, w):\n",
    "    \"\"\"\n",
    "    x_data: array of features (n+1)\n",
    "    w: array of weights (n+1)\n",
    "    returns: 0 or 1\n",
    "    \"\"\"\n",
    "    \n",
    "    if run_model(x_data, w) > .5:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "f7e4e0e9-0efc-46c1-a012-2da1a7b05ad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class for test example 0 -> 1\n",
      "Predicted class for test example 1 -> 0\n",
      "Predicted class for test example 2 -> 0\n",
      "Predicted class for test example 3 -> 0\n",
      "Predicted class for test example 4 -> 0\n",
      "Predicted class for test example 5 -> 1\n",
      "Predicted class for test example 6 -> 1\n",
      "Predicted class for test example 7 -> 1\n",
      "Predicted class for test example 8 -> 0\n",
      "Predicted class for test example 9 -> 0\n"
     ]
    }
   ],
   "source": [
    "# Sanity check: classify the first few testing examples using the model from Part A\n",
    "\n",
    "for i in range(10):\n",
    "    print(\"Predicted class for test example\", i, \"->\", make_prediction(X_test[i], w_direct))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "3309fc50-bf06-4b31-9740-6f13f1b5eb3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## QUESTION:\n",
    "\n",
    "# Given the output immediately above, what is the accuracy of the model in Part A (since we used w_direct\n",
    "# above) just based on these 10 training examples?  (Answer as a percent; in other words\n",
    "# the percentage of those 10 testing examples that were predicted correctly).\n",
    "\n",
    "# ANSWER:\n",
    "#\n",
    "#100% accurate\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "56291ae2-12bf-498b-9500-df3f1d66a934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function called compute_accuracy that takes a \n",
    "# set of X values and a set of y values and a parameter vector\n",
    "# w.  This function should predict the class for each example x\n",
    "# in X_data and based on the true y values (y_data), compute\n",
    "# the accuracy on this data set.\n",
    "\n",
    "# To do this, call make_prediction on each row of X_data\n",
    "# and compare the output against the corresponding value in y_data.\n",
    "# Count how many predictions are correct and divide by the total.\n",
    "\n",
    "def compute_accuracy(X_data, y_data, w):\n",
    "    \"\"\"\n",
    "    X_data: matrix of features (flexible rows, n+1 cols)\n",
    "    y_data: vector of true classes (same number of rows as X_data)\n",
    "    w: array of weights (n+1)\n",
    "    returns: percentage of rows in X_data classified correctly\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    for i in range(len(X_data)):\n",
    "        result = make_prediction(X_data[i], w)\n",
    "        if result == y_data[i]:\n",
    "            count += 1\n",
    "        \n",
    "    return count / len(X_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "bbcece4e-4d1c-4627-ac5d-bf70f9b9b9ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9625\n",
      "0.945\n"
     ]
    }
   ],
   "source": [
    "# Sanity check\n",
    "\n",
    "train_acc_partA = compute_accuracy(X_train, y_train, w_direct)\n",
    "test_acc_partA = compute_accuracy(X_test, y_test, w_direct)\n",
    "\n",
    "print(train_acc_partA)  # should be 0.9625\n",
    "print(test_acc_partA)  # should be 0.945"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "ff4b3493-862b-42f9-b0cc-5dfa761729fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "## QUESTION:\n",
    "\n",
    "# Which of the two numbers above do we report as the \"true\" accuracy of our model,\n",
    "# and why do we typically not report the other (or not give it as much importance)?\n",
    "\n",
    "# ANSWER: the test number, because we \"trained\" our model to be accurate\n",
    "#\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "9f7840f3-d429-4302-baa9-5d90c1435740",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the compute_loss function below to compute the\n",
    "# loss over *one* training example, given the true y value\n",
    "# and the predicted y value (y_hat).  \n",
    "# This is the logistic regression loss function.\n",
    "\n",
    "def compute_loss(y, y_hat):\n",
    "    \"\"\"\n",
    "    y: 0 or 1 \n",
    "    y_hat: decimal number between 0 and 1\n",
    "    returns: scalar\n",
    "    \"\"\"\n",
    "    \n",
    "    if y == 0:\n",
    "        return -(np.log(y_hat))\n",
    "    else:\n",
    "        return -(1 - np.log(y_hat))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "b9d6a2e2-9c16-45fa-a359-0f83a9d3cb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the compute_cost function below to compute the\n",
    "# total cost over the entire data set X_data and y_data,\n",
    "# given parameters vector w.\n",
    "# Call your run_model() and compute_loss() functions \n",
    "# that you defined above.  You should have one loop.\n",
    "# DO NOT CALL MAKE_PREDICTION; it's not needed here.\n",
    "\n",
    "def compute_cost(X_data, y_data, w):\n",
    "    \"\"\"\n",
    "    X_data: matrix (m, n+1)\n",
    "    y_data: array of true y values (m)\n",
    "    w: array of weights (n+1)\n",
    "    returns: scalar\n",
    "    \"\"\"\n",
    "    total = 0\n",
    "    for i in range(len(X_data)):\n",
    "        y_hat = run_model(X_data[i], w)\n",
    "        y = y_data[i]\n",
    "        final = y * compute_loss(y, y_hat) + (1 - y) * compute_loss(y, y_hat)\n",
    "        total += final\n",
    "        \n",
    "    return ((-1/len(X_data)) * total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "40f04248-f046-4865-9246-9d27d63afd0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-5.240383505296133\n"
     ]
    }
   ],
   "source": [
    "# Sanity check: compute the loss for w_direct from Part A:\n",
    "\n",
    "w_direct_cost = compute_cost(X_train, y_train, w_direct)  # This is the minimum cost we can ever get!  Should be less than 0.1.\n",
    "print(w_direct_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "ca88aeed-7b1a-40fc-a742-ba127c2a670f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## QUESTION:\n",
    "\n",
    "# compute_cost() above returns a single number, but based on the formula in J(w),\n",
    "# this function can only ever return numbers in a fixed range.  What is that range\n",
    "# and why are only numbers in this range ever returned?\n",
    "\n",
    "# ANSWER: \n",
    "#\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "ed2c1a63-7376-41db-8235-cfee2ef8dd2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the compute_gradient function below to compute\n",
    "# the complete gradient for the function J(w).  \n",
    "# Do not use matrix computations here; call your run_model() function\n",
    "# that you defined above.  You should have two nested loops.\n",
    "\n",
    "def compute_gradient(X_data, y_data, w):\n",
    "    \"\"\"\n",
    "    X_data: matrix (m, n+1)\n",
    "    y_data: array of true y values (m)\n",
    "    w: array of weights (n+1)\n",
    "    returns: array of gradients (n+1)\n",
    "    \"\"\"\n",
    "    m = len(X_data)\n",
    "    gradient_w = np.zeros(shape= (m-1, 5))\n",
    "    \n",
    "    \n",
    "    for i in range((len(X_data)) - 1):\n",
    "        for j in range(len(X_data[i])):\n",
    "            x = X_data[i][j]\n",
    "            y = y_data[i]\n",
    "            y_hat = run_model(X_data[i], w)\n",
    "            grad_w = (y_hat - y) * x \n",
    "            gradient_w[i][j] = grad_w\n",
    "            gradient = gradient_w.sum(axis=0) / m\n",
    "            \n",
    "            \n",
    "    \n",
    "    return gradient\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "0a018db7-d682-4d99-b2d6-ec9102dc351f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.53658433e-04,  2.43570206e-04, -2.54498694e-04,  4.89903747e-04,\n",
       "        1.54858997e-05])"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_gradient(X_train, y_train, w_direct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "7645ea34-7117-4971-abca-4b7084038f01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  0\n",
      "Current w [  2.42758438   7.15813643  -7.23010825   9.91773928 -10.77389415]\n",
      "Current cost:  -5.240383505296133\n",
      "Iteration  1\n",
      "Current w [  2.42743072   7.15789286  -7.22985375   9.91724938 -10.77390964]\n",
      "Current cost:  -5.240269921017316\n",
      "Iteration  2\n",
      "Current w [  2.42727931   7.15765082  -7.22960116   9.91676566 -10.77391684]\n",
      "Current cost:  -5.240155316377032\n",
      "Iteration  3\n",
      "Current w [  2.42713009   7.15741026  -7.22935042   9.91628796 -10.77391598]\n",
      "Current cost:  -5.240039718843508\n",
      "Iteration  4\n",
      "Current w [  2.42698301   7.15717114  -7.22910149   9.91581612 -10.77390728]\n",
      "Current cost:  -5.239923155153153\n",
      "Iteration  5\n",
      "Current w [  2.426838     7.15693343  -7.22885431   9.91534998 -10.77389096]\n",
      "Current cost:  -5.2398056513302995\n",
      "Iteration  6\n",
      "Current w [  2.42669502   7.15669708  -7.22860883   9.91488938 -10.77386723]\n",
      "Current cost:  -5.2396872327065775\n",
      "Iteration  7\n",
      "Current w [  2.42655402   7.15646207  -7.228365     9.91443419 -10.7738363 ]\n",
      "Current cost:  -5.239567923939608\n",
      "Iteration  8\n",
      "Current w [  2.42641494   7.15622835  -7.22812279   9.91398425 -10.77379836]\n",
      "Current cost:  -5.239447749031273\n",
      "Iteration  9\n",
      "Current w [  2.42627773   7.15599588  -7.22788213   9.91353942 -10.77375361]\n",
      "Current cost:  -5.239326731345419\n",
      "Iteration  10\n",
      "Current w [  2.42614234   7.15576464  -7.22764299   9.91309957 -10.77370223]\n",
      "Current cost:  -5.239204893625124\n",
      "Iteration  11\n",
      "Current w [  2.42600874   7.15553459  -7.22740533   9.91266456 -10.77364441]\n",
      "Current cost:  -5.239082258009497\n",
      "Iteration  12\n",
      "Current w [  2.42587687   7.1553057   -7.2271691    9.91223427 -10.77358033]\n",
      "Current cost:  -5.238958846049879\n",
      "Iteration  13\n",
      "Current w [  2.42574669   7.15507794  -7.22693427   9.91180856 -10.77351016]\n",
      "Current cost:  -5.23883467872585\n",
      "Iteration  14\n",
      "Current w [  2.42561815   7.15485128  -7.22670079   9.91138731 -10.77343407]\n",
      "Current cost:  -5.238709776460543\n",
      "Iteration  15\n",
      "Current w [  2.42549121   7.15462568  -7.22646862   9.91097041 -10.77335222]\n",
      "Current cost:  -5.2385841591357005\n",
      "Iteration  16\n",
      "Current w [  2.42536584   7.15440113  -7.22623773   9.91055774 -10.77326478]\n",
      "Current cost:  -5.2384578461062326\n",
      "Iteration  17\n",
      "Current w [  2.42524199   7.15417758  -7.22600809   9.91014919 -10.77317189]\n",
      "Current cost:  -5.238330856214434\n",
      "Iteration  18\n",
      "Current w [  2.42511963   7.15395502  -7.22577965   9.90974464 -10.77307371]\n",
      "Current cost:  -5.238203207803781\n",
      "Iteration  19\n",
      "Current w [  2.42499871   7.15373342  -7.22555239   9.90934398 -10.77297039]\n",
      "Current cost:  -5.238074918732324\n",
      "Iteration  20\n",
      "Current w [  2.4248792    7.15351275  -7.22532627   9.90894712 -10.77286206]\n",
      "Current cost:  -5.23794600638581\n",
      "Iteration  21\n",
      "Current w [  2.42476106   7.15329299  -7.22510125   9.90855396 -10.77274887]\n",
      "Current cost:  -5.2378164876903\n",
      "Iteration  22\n",
      "Current w [  2.42464426   7.15307411  -7.22487732   9.90816438 -10.77263095]\n",
      "Current cost:  -5.237686379124598\n",
      "Iteration  23\n",
      "Current w [  2.42452877   7.15285609  -7.22465443   9.90777831 -10.77250844]\n",
      "Current cost:  -5.237555696732216\n",
      "Iteration  24\n",
      "Current w [  2.42441454   7.1526389   -7.22443256   9.90739564 -10.77238146]\n",
      "Current cost:  -5.237424456133066\n",
      "Iteration  25\n",
      "Current w [  2.42430156   7.15242254  -7.22421167   9.90701628 -10.77225013]\n",
      "Current cost:  -5.237292672534864\n",
      "Iteration  26\n",
      "Current w [  2.42418978   7.15220696  -7.22399175   9.90664015 -10.77211458]\n",
      "Current cost:  -5.237160360744162\n",
      "Iteration  27\n",
      "Current w [  2.42407918   7.15199216  -7.22377276   9.90626716 -10.77197492]\n",
      "Current cost:  -5.237027535177082\n",
      "Iteration  28\n",
      "Current w [  2.42396973   7.1517781   -7.22355468   9.90589722 -10.77183127]\n",
      "Current cost:  -5.236894209869834\n",
      "Iteration  29\n",
      "Current w [  2.42386139   7.15156478  -7.22333749   9.90553026 -10.77168373]\n",
      "Current cost:  -5.2367603984888635\n",
      "Iteration  30\n",
      "Current w [  2.42375415   7.15135218  -7.22312115   9.90516619 -10.77153243]\n",
      "Current cost:  -5.236626114340757\n",
      "Iteration  31\n",
      "Current w [  2.42364796   7.15114026  -7.22290564   9.90480494 -10.77137746]\n",
      "Current cost:  -5.236491370381892\n",
      "Iteration  32\n",
      "Current w [  2.42354281   7.15092902  -7.22269094   9.90444643 -10.77121892]\n",
      "Current cost:  -5.236356179227768\n",
      "Iteration  33\n",
      "Current w [  2.42343868   7.15071844  -7.22247703   9.9040906  -10.77105691]\n",
      "Current cost:  -5.236220553162168\n",
      "Iteration  34\n",
      "Current w [  2.42333552   7.1505085   -7.22226389   9.90373736 -10.77089154]\n",
      "Current cost:  -5.236084504146022\n",
      "Iteration  35\n",
      "Current w [  2.42323333   7.15029919  -7.22205149   9.90338665 -10.77072289]\n",
      "Current cost:  -5.235948043826007\n",
      "Iteration  36\n",
      "Current w [  2.42313207   7.15009048  -7.22183981   9.90303841 -10.77055106]\n",
      "Current cost:  -5.235811183542961\n",
      "Iteration  37\n",
      "Current w [  2.42303172   7.14988236  -7.22162884   9.90269256 -10.77037613]\n",
      "Current cost:  -5.235673934340075\n",
      "Iteration  38\n",
      "Current w [  2.42293226   7.14967482  -7.22141855   9.90234904 -10.7701982 ]\n",
      "Current cost:  -5.23553630697078\n",
      "Iteration  39\n",
      "Current w [  2.42283367   7.14946784  -7.22120892   9.9020078  -10.77001734]\n",
      "Current cost:  -5.2353983119065415\n",
      "Iteration  40\n",
      "Current w [  2.42273592   7.14926141  -7.22099994   9.90166877 -10.76983364]\n",
      "Current cost:  -5.235259959344353\n",
      "Iteration  41\n",
      "Current w [  2.422639     7.14905551  -7.22079159   9.90133189 -10.76964718]\n",
      "Current cost:  -5.235121259214029\n",
      "Iteration  42\n",
      "Current w [  2.42254288   7.14885012  -7.22058384   9.90099711 -10.76945803]\n",
      "Current cost:  -5.234982221185381\n",
      "Iteration  43\n",
      "Current w [  2.42244754   7.14864525  -7.22037669   9.90066437 -10.76926627]\n",
      "Current cost:  -5.234842854675087\n",
      "Iteration  44\n",
      "Current w [  2.42235297   7.14844087  -7.22017012   9.90033362 -10.76907198]\n",
      "Current cost:  -5.234703168853474\n",
      "Iteration  45\n",
      "Current w [  2.42225914   7.14823696  -7.21996411   9.9000048  -10.76887522]\n",
      "Current cost:  -5.234563172651072\n",
      "Iteration  46\n",
      "Current w [  2.42216604   7.14803353  -7.21975864   9.89967787 -10.76867607]\n",
      "Current cost:  -5.234422874764958\n",
      "Iteration  47\n",
      "Current w [  2.42207365   7.14783055  -7.2195537    9.89935277 -10.76847459]\n",
      "Current cost:  -5.234282283665018\n",
      "Iteration  48\n",
      "Current w [  2.42198194   7.14762801  -7.21934927   9.89902945 -10.76827085]\n",
      "Current cost:  -5.234141407599977\n",
      "Iteration  49\n",
      "Current w [  2.42189091   7.14742591  -7.21914535   9.89870787 -10.7680649 ]\n",
      "Current cost:  -5.234000254603213\n",
      "Iteration  50\n",
      "Current w [  2.42180054   7.14722423  -7.21894191   9.89838799 -10.76785683]\n",
      "Current cost:  -5.233858832498572\n",
      "Iteration  51\n",
      "Current w [  2.42171081   7.14702296  -7.21873895   9.89806975 -10.76764667]\n",
      "Current cost:  -5.233717148905884\n",
      "Iteration  52\n",
      "Current w [  2.4216217    7.1468221   -7.21853645   9.89775312 -10.7674345 ]\n",
      "Current cost:  -5.23357521124637\n",
      "Iteration  53\n",
      "Current w [  2.42153321   7.14662162  -7.21833439   9.89743805 -10.76722037]\n",
      "Current cost:  -5.233433026747949\n",
      "Iteration  54\n",
      "Current w [  2.42144531   7.14642153  -7.21813277   9.89712451 -10.76700433]\n",
      "Current cost:  -5.233290602450336\n",
      "Iteration  55\n",
      "Current w [  2.42135798   7.1462218   -7.21793158   9.89681244 -10.76678644]\n",
      "Current cost:  -5.233147945210038\n",
      "Iteration  56\n",
      "Current w [  2.42127123   7.14602245  -7.21773079   9.89650182 -10.76656675]\n",
      "Current cost:  -5.233005061705227\n",
      "Iteration  57\n",
      "Current w [  2.42118502   7.14582344  -7.21753041   9.89619261 -10.76634532]\n",
      "Current cost:  -5.232861958440432\n",
      "Iteration  58\n",
      "Current w [  2.42109936   7.14562478  -7.21733041   9.89588476 -10.76612219]\n",
      "Current cost:  -5.232718641751194\n",
      "Iteration  59\n",
      "Current w [  2.42101422   7.14542645  -7.2171308    9.89557825 -10.7658974 ]\n",
      "Current cost:  -5.232575117808473\n",
      "Iteration  60\n",
      "Current w [  2.42092959   7.14522846  -7.21693155   9.89527303 -10.76567102]\n",
      "Current cost:  -5.232431392623051\n",
      "Iteration  61\n",
      "Current w [  2.42084546   7.14503078  -7.21673266   9.89496908 -10.76544308]\n",
      "Current cost:  -5.2322874720497605\n",
      "Iteration  62\n",
      "Current w [  2.42076182   7.14483342  -7.21653412   9.89466637 -10.76521362]\n",
      "Current cost:  -5.2321433617916\n",
      "Iteration  63\n",
      "Current w [  2.42067865   7.14463636  -7.21633592   9.89436485 -10.7649827 ]\n",
      "Current cost:  -5.231999067403723\n",
      "Iteration  64\n",
      "Current w [  2.42059595   7.1444396   -7.21613805   9.8940645  -10.76475035]\n",
      "Current cost:  -5.231854594297429\n",
      "Iteration  65\n",
      "Current w [  2.42051371   7.14424313  -7.2159405    9.89376529 -10.76451661]\n",
      "Current cost:  -5.231709947743853\n",
      "Iteration  66\n",
      "Current w [  2.4204319    7.14404694  -7.21574326   9.89346719 -10.76428153]\n",
      "Current cost:  -5.231565132877766\n",
      "Iteration  67\n",
      "Current w [  2.42035053   7.14385104  -7.21554633   9.89317016 -10.76404514]\n",
      "Current cost:  -5.2314201547010954\n",
      "Iteration  68\n",
      "Current w [  2.42026957   7.1436554   -7.21534969   9.8928742  -10.76380747]\n",
      "Current cost:  -5.231275018086475\n",
      "Iteration  69\n",
      "Current w [  2.42018903   7.14346002  -7.21515334   9.89257925 -10.76356858]\n",
      "Current cost:  -5.231129727780642\n",
      "Iteration  70\n",
      "Current w [  2.42010889   7.1432649   -7.21495727   9.89228531 -10.76332848]\n",
      "Current cost:  -5.230984288407759\n",
      "Iteration  71\n",
      "Current w [  2.42002914   7.14307004  -7.21476148   9.89199234 -10.76308723]\n",
      "Current cost:  -5.2308387044726405\n",
      "Iteration  72\n",
      "Current w [  2.41994977   7.14287542  -7.21456594   9.89170031 -10.76284484]\n",
      "Current cost:  -5.2306929803638935\n",
      "Iteration  73\n",
      "Current w [  2.41987078   7.14268104  -7.21437067   9.89140921 -10.76260135]\n",
      "Current cost:  -5.230547120356997\n",
      "Iteration  74\n",
      "Current w [  2.41979215   7.14248689  -7.21417565   9.89111901 -10.7623568 ]\n",
      "Current cost:  -5.2304011286172365\n",
      "Iteration  75\n",
      "Current w [  2.41971387   7.14229298  -7.21398087   9.89082969 -10.76211122]\n",
      "Current cost:  -5.230255009202653\n",
      "Iteration  76\n",
      "Current w [  2.41963594   7.14209928  -7.21378633   9.89054123 -10.76186463]\n",
      "Current cost:  -5.230108766066841\n",
      "Iteration  77\n",
      "Current w [  2.41955836   7.14190581  -7.21359203   9.89025359 -10.76161706]\n",
      "Current cost:  -5.229962403061665\n",
      "Iteration  78\n",
      "Current w [  2.4194811    7.14171255  -7.21339795   9.88996677 -10.76136855]\n",
      "Current cost:  -5.229815923939994\n",
      "Iteration  79\n",
      "Current w [  2.41940416   7.1415195   -7.21320409   9.88968074 -10.76111912]\n",
      "Current cost:  -5.229669332358262\n",
      "Iteration  80\n",
      "Current w [  2.41932754   7.14132666  -7.21301044   9.88939549 -10.76086879]\n",
      "Current cost:  -5.229522631878995\n",
      "Iteration  81\n",
      "Current w [  2.41925123   7.14113401  -7.21281701   9.88911098 -10.76061761]\n",
      "Current cost:  -5.229375825973278\n",
      "Iteration  82\n",
      "Current w [  2.41917521   7.14094156  -7.21262377   9.88882721 -10.76036558]\n",
      "Current cost:  -5.2292289180231855\n",
      "Iteration  83\n",
      "Current w [  2.41909949   7.1407493   -7.21243074   9.88854415 -10.76011273]\n",
      "Current cost:  -5.229081911324093\n",
      "Iteration  84\n",
      "Current w [  2.41902406   7.14055723  -7.2122379    9.88826179 -10.75985909]\n",
      "Current cost:  -5.2289348090869066\n",
      "Iteration  85\n",
      "Current w [  2.41894891   7.14036534  -7.21204525   9.88798011 -10.75960469]\n",
      "Current cost:  -5.228787614440376\n",
      "Iteration  86\n",
      "Current w [  2.41887403   7.14017363  -7.21185278   9.88769909 -10.75934953]\n",
      "Current cost:  -5.228640330433142\n",
      "Iteration  87\n",
      "Current w [  2.41879941   7.1399821   -7.21166049   9.88741871 -10.75909366]\n",
      "Current cost:  -5.228492960035901\n",
      "Iteration  88\n",
      "Current w [  2.41872506   7.13979074  -7.21146837   9.88713897 -10.75883708]\n",
      "Current cost:  -5.22834550614342\n",
      "Iteration  89\n",
      "Current w [  2.41865097   7.13959954  -7.21127642   9.88685984 -10.75857982]\n",
      "Current cost:  -5.228197971576504\n",
      "Iteration  90\n",
      "Current w [  2.41857712   7.13940852  -7.21108464   9.88658131 -10.75832191]\n",
      "Current cost:  -5.228050359083975\n",
      "Iteration  91\n",
      "Current w [  2.41850351   7.13921765  -7.21089303   9.88630336 -10.75806335]\n",
      "Current cost:  -5.227902671344507\n",
      "Iteration  92\n",
      "Current w [  2.41843014   7.13902694  -7.21070156   9.88602598 -10.75780416]\n",
      "Current cost:  -5.227754910968489\n",
      "Iteration  93\n",
      "Current w [  2.41835701   7.13883638  -7.21051026   9.88574916 -10.75754438]\n",
      "Current cost:  -5.2276070804998005\n",
      "Iteration  94\n",
      "Current w [  2.4182841    7.13864598  -7.2103191    9.88547288 -10.75728401]\n",
      "Current cost:  -5.227459182417526\n",
      "Iteration  95\n",
      "Current w [  2.41821141   7.13845572  -7.21012809   9.88519713 -10.75702308]\n",
      "Current cost:  -5.22731121913768\n",
      "Iteration  96\n",
      "Current w [  2.41813894   7.13826561  -7.20993722   9.88492189 -10.75676159]\n",
      "Current cost:  -5.227163193014804\n",
      "Iteration  97\n",
      "Current w [  2.41806668   7.13807564  -7.20974649   9.88464716 -10.75649958]\n",
      "Current cost:  -5.227015106343644\n",
      "Iteration  98\n",
      "Current w [  2.41799463   7.13788581  -7.2095559    9.88437291 -10.75623704]\n",
      "Current cost:  -5.226866961360613\n",
      "Iteration  99\n",
      "Current w [  2.41792278   7.13769612  -7.20936544   9.88409915 -10.75597401]\n",
      "Current cost:  -5.226718760245379\n",
      "Iteration  100\n",
      "Current w [  2.41785113   7.13750656  -7.20917511   9.88382585 -10.75571049]\n",
      "Current cost:  -5.226570505122319\n",
      "Iteration  101\n",
      "Current w [  2.41777967   7.13731714  -7.20898491   9.88355301 -10.7554465 ]\n",
      "Current cost:  -5.226422198061958\n",
      "Iteration  102\n",
      "Current w [  2.41770839   7.13712784  -7.20879483   9.88328062 -10.75518205]\n",
      "Current cost:  -5.226273841082347\n",
      "Iteration  103\n",
      "Current w [  2.41763731   7.13693867  -7.20860487   9.88300866 -10.75491716]\n",
      "Current cost:  -5.226125436150486\n",
      "Iteration  104\n",
      "Current w [  2.4175664    7.13674962  -7.20841503   9.88273713 -10.75465185]\n",
      "Current cost:  -5.225976985183579\n",
      "Iteration  105\n",
      "Current w [  2.41749567   7.1365607   -7.20822531   9.882466   -10.75438612]\n",
      "Current cost:  -5.225828490050365\n",
      "Iteration  106\n",
      "Current w [  2.41742511   7.1363719   -7.20803569   9.88219529 -10.75411998]\n",
      "Current cost:  -5.225679952572383\n",
      "Iteration  107\n",
      "Current w [  2.41735472   7.13618321  -7.20784619   9.88192497 -10.75385346]\n",
      "Current cost:  -5.225531374525158\n",
      "Iteration  108\n",
      "Current w [  2.4172845    7.13599464  -7.2076568    9.88165503 -10.75358657]\n",
      "Current cost:  -5.225382757639445\n",
      "Iteration  109\n",
      "Current w [  2.41721444   7.13580619  -7.20746751   9.88138547 -10.75331931]\n",
      "Current cost:  -5.225234103602328\n",
      "Iteration  110\n",
      "Current w [  2.41714453   7.13561784  -7.20727833   9.88111628 -10.75305169]\n",
      "Current cost:  -5.225085414058404\n",
      "Iteration  111\n",
      "Current w [  2.41707478   7.13542961  -7.20708924   9.88084745 -10.75278374]\n",
      "Current cost:  -5.224936690610874\n",
      "Iteration  112\n",
      "Current w [  2.41700518   7.13524148  -7.20690026   9.88057897 -10.75251545]\n",
      "Current cost:  -5.224787934822569\n",
      "Iteration  113\n",
      "Current w [  2.41693573   7.13505346  -7.20671137   9.88031083 -10.75224685]\n",
      "Current cost:  -5.224639148217049\n",
      "Iteration  114\n",
      "Current w [  2.41686642   7.13486554  -7.20652258   9.88004303 -10.75197793]\n",
      "Current cost:  -5.224490332279576\n",
      "Iteration  115\n",
      "Current w [  2.41679725   7.13467773  -7.20633387   9.87977555 -10.75170872]\n",
      "Current cost:  -5.224341488458146\n",
      "Iteration  116\n",
      "Current w [  2.41672823   7.13449001  -7.20614526   9.87950839 -10.75143922]\n",
      "Current cost:  -5.224192618164393\n",
      "Iteration  117\n",
      "Current w [  2.41665933   7.1343024   -7.20595674   9.87924155 -10.75116943]\n",
      "Current cost:  -5.224043722774601\n",
      "Iteration  118\n",
      "Current w [  2.41659057   7.13411488  -7.20576831   9.87897501 -10.75089938]\n",
      "Current cost:  -5.223894803630527\n",
      "Iteration  119\n",
      "Current w [  2.41652194   7.13392746  -7.20557996   9.87870877 -10.75062907]\n",
      "Current cost:  -5.223745862040388\n",
      "Iteration  120\n",
      "Current w [  2.41645344   7.13374013  -7.20539169   9.87844283 -10.7503585 ]\n",
      "Current cost:  -5.223596899279628\n",
      "Iteration  121\n",
      "Current w [  2.41638506   7.1335529   -7.20520351   9.87817717 -10.75008769]\n",
      "Current cost:  -5.223447916591842\n",
      "Iteration  122\n",
      "Current w [  2.4163168    7.13336575  -7.20501541   9.87791179 -10.74981664]\n",
      "Current cost:  -5.22329891518953\n",
      "Iteration  123\n",
      "Current w [  2.41624866   7.1331787   -7.20482739   9.87764668 -10.74954537]\n",
      "Current cost:  -5.223149896254962\n",
      "Iteration  124\n",
      "Current w [  2.41618064   7.13299174  -7.20463944   9.87738184 -10.74927388]\n",
      "Current cost:  -5.223000860940888\n",
      "Iteration  125\n",
      "Current w [  2.41611273   7.13280486  -7.20445157   9.87711726 -10.74900217]\n",
      "Current cost:  -5.2228518103713615\n",
      "Iteration  126\n",
      "Current w [  2.41604493   7.13261807  -7.20426377   9.87685293 -10.74873026]\n",
      "Current cost:  -5.222702745642392\n",
      "Iteration  127\n",
      "Current w [  2.41597724   7.13243136  -7.20407605   9.87658886 -10.74845816]\n",
      "Current cost:  -5.222553667822779\n",
      "Iteration  128\n",
      "Current w [  2.41590966   7.13224474  -7.2038884    9.87632503 -10.74818586]\n",
      "Current cost:  -5.22240457795469\n",
      "Iteration  129\n",
      "Current w [  2.41584219   7.1320582   -7.20370082   9.87606144 -10.74791338]\n",
      "Current cost:  -5.222255477054422\n",
      "Iteration  130\n",
      "Current w [  2.41577481   7.13187175  -7.20351331   9.87579809 -10.74764073]\n",
      "Current cost:  -5.2221063661130405\n",
      "Iteration  131\n",
      "Current w [  2.41570754   7.13168537  -7.20332587   9.87553496 -10.7473679 ]\n",
      "Current cost:  -5.221957246097031\n",
      "Iteration  132\n",
      "Current w [  2.41564037   7.13149907  -7.2031385    9.87527206 -10.74709491]\n",
      "Current cost:  -5.221808117948899\n",
      "Iteration  133\n",
      "Current w [  2.41557329   7.13131285  -7.20295119   9.87500938 -10.74682176]\n",
      "Current cost:  -5.2216589825878295\n",
      "Iteration  134\n",
      "Current w [  2.41550631   7.13112671  -7.20276394   9.87474692 -10.74654846]\n",
      "Current cost:  -5.221509840910216\n",
      "Iteration  135\n",
      "Current w [  2.41543942   7.13094064  -7.20257677   9.87448466 -10.74627502]\n",
      "Current cost:  -5.221360693790346\n",
      "Iteration  136\n",
      "Current w [  2.41537262   7.13075465  -7.20238965   9.87422262 -10.74600143]\n",
      "Current cost:  -5.221211542080854\n",
      "Iteration  137\n",
      "Current w [  2.41530591   7.13056873  -7.20220259   9.87396077 -10.74572771]\n",
      "Current cost:  -5.221062386613343\n",
      "Iteration  138\n",
      "Current w [  2.41523929   7.13038289  -7.2020156    9.87369913 -10.74545386]\n",
      "Current cost:  -5.220913228198896\n",
      "Iteration  139\n",
      "Current w [  2.41517275   7.13019712  -7.20182867   9.87343768 -10.74517989]\n",
      "Current cost:  -5.220764067628586\n",
      "Iteration  140\n",
      "Current w [  2.4151063    7.13001142  -7.20164179   9.87317642 -10.74490579]\n",
      "Current cost:  -5.220614905674033\n",
      "Iteration  141\n",
      "Current w [  2.41503993   7.12982579  -7.20145498   9.87291535 -10.74463158]\n",
      "Current cost:  -5.220465743087848\n",
      "Iteration  142\n",
      "Current w [  2.41497364   7.12964023  -7.20126822   9.87265446 -10.74435727]\n",
      "Current cost:  -5.220316580604117\n",
      "Iteration  143\n",
      "Current w [  2.41490743   7.12945474  -7.20108152   9.87239375 -10.74408284]\n",
      "Current cost:  -5.22016741893894\n",
      "Iteration  144\n",
      "Current w [  2.4148413    7.12926932  -7.20089487   9.87213322 -10.74380832]\n",
      "Current cost:  -5.220018258790775\n",
      "Iteration  145\n",
      "Current w [  2.41477524   7.12908397  -7.20070828   9.87187286 -10.7435337 ]\n",
      "Current cost:  -5.219869100840994\n",
      "Iteration  146\n",
      "Current w [  2.41470926   7.12889868  -7.20052174   9.87161267 -10.74325899]\n",
      "Current cost:  -5.21971994575424\n",
      "Iteration  147\n",
      "Current w [  2.41464336   7.12871346  -7.20033526   9.87135265 -10.74298419]\n",
      "Current cost:  -5.219570794178909\n",
      "Iteration  148\n",
      "Current w [  2.41457752   7.1285283   -7.20014883   9.87109279 -10.74270931]\n",
      "Current cost:  -5.219421646747495\n",
      "Iteration  149\n",
      "Current w [  2.41451176   7.12834321  -7.19996245   9.8708331  -10.74243435]\n",
      "Current cost:  -5.219272504077049\n",
      "Final w: [  2.41444606   7.12815819  -7.19977613   9.87057356 -10.74215931]\n"
     ]
    }
   ],
   "source": [
    "# Write code here to perform gradient descent, using your\n",
    "# functions above.  You should use three new variables in your\n",
    "# code:\n",
    "# - w_manual: which is the vector of weights that gradient\n",
    "#   descent is designed to find:\n",
    "# - w_manual_cost, which is the cost of these weights,\n",
    "# - J_list, which is the list of \n",
    "#   costs determined by compute_cost() [like in the in-class lab we did].\n",
    "\n",
    "# Setup these vars:\n",
    "w_manual = w_direct # n+1 weights\n",
    "w_manual_cost = 0\n",
    "J_list = []\n",
    "alpha = 1\n",
    "\n",
    "for i in range(0, 150):\n",
    "    w_manual_cost = compute_cost(X_train, y_train, w_manual)\n",
    "\n",
    "    print(\"Iteration \", i)\n",
    "    print(\"Current w\", w_manual)\n",
    "    print(\"Current cost: \", w_manual_cost)\n",
    "\n",
    "    J_list.append(w_manual_cost)\n",
    "\n",
    "    gradient = compute_gradient(X_train, y_train, w_manual)\n",
    "    \n",
    "    w1 = w_manual[0] - alpha * gradient[0]\n",
    "    w2 = w_manual[1] - alpha * gradient[1]\n",
    "    w3 = w_manual[2] - alpha * gradient[2]\n",
    "    w4 = w_manual[3] - alpha * gradient[3]\n",
    "    w5 = w_manual[4] - alpha * gradient[4]\n",
    "    w_manual[0] = w1\n",
    "    w_manual[1] = w2\n",
    "    w_manual[2] = w3\n",
    "    w_manual[3] = w4\n",
    "    w_manual[4] = w5\n",
    "    \n",
    "\n",
    "print(\"Final w:\", w_manual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "95163d93-73ab-4fed-8cc6-0a464f0c74cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the cost as a function of number of iterations of the\n",
    "# gradient descent algorithm.\n",
    "\n",
    "plt.scatter(range(0, len(J_list)), J_list)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b65c2f-022c-41b8-b9ae-a47c3e847e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep playing around with gradient descent until you have a good\n",
    "# learning curve in the plot above (something that appears to flatten out).\n",
    "# Then answer the questions below. \n",
    "\n",
    "# What was your initial choice for alpha?  Your final choice?  How did\n",
    "# you arrive at these choices?\n",
    "#\n",
    "# 1\n",
    "#\n",
    "# How many iterations of gradient descent did you need until convergence?\n",
    "#\n",
    "# 150\n",
    "#\n",
    "# What was your final vector of weights? (w_manual)\n",
    "#\n",
    "# [  2.41444606   7.12815819  -7.19977613   9.87057356 -10.74215931]\n",
    "#\n",
    "# What was your final cost of these weights? (w_manual_cost)\n",
    "#\n",
    "# -5.219272504077049\n",
    "#\n",
    "# What was your final vector of weights from Part A? (w_direct)\n",
    "#\n",
    "# [ 2.17303834  6.40434357 -6.47511472  8.86631945 -9.64155641]\n",
    "#\n",
    "# What cost of these weights? (w_direct_cost)\n",
    "#\n",
    "# YOUR ANSWER HERE\n",
    "#\n",
    "# How close are your weights from Part B to the \"correct\" weights from Part A?\n",
    "#\n",
    "# not that close\n",
    "#\n",
    "#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b703235e-8ccc-4842-8b80-c28b1a8a38ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write code here to compute the accuracy of the new model (the one you just trained)\n",
    "# on the training and testing data sets.  \n",
    "# Save these values to two variables called train_acc_partB and test_acc_partB.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(train_acc_partB)  \n",
    "print(test_acc_partB)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4c9d1b-8cc9-40b9-b588-4a9ca859b2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## QUESTION:\n",
    "\n",
    "# How does the accuracy of the model you created by hand in Part B compare to \n",
    "# the accuracy of the model from Part A created by scikit-learn?\n",
    "\n",
    "# ANSWER:\n",
    "#mine is not that close, so probably not that accurate.\n",
    "#\n",
    "#\n",
    "#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "bdf7cf7e-37dc-4fb5-a08b-876e1140c63f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part A\n",
      "Weights: [  2.41444606   7.12815819  -7.19977613   9.87057356 -10.74215931]\n",
      "Cost: -5.240383505296133\n",
      "Training accuracy: 0.9625\n",
      "Testing accuracy: 0.945\n",
      "\n",
      "Part B\n",
      "Weights: [  2.41444606   7.12815819  -7.19977613   9.87057356 -10.74215931]\n",
      "Cost: -5.219272504077049\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_acc_partB' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [175]\u001b[0m, in \u001b[0;36m<cell line: 14>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWeights:\u001b[39m\u001b[38;5;124m\"\u001b[39m, w_manual)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCost:\u001b[39m\u001b[38;5;124m\"\u001b[39m, w_manual_cost)\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining accuracy:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mtrain_acc_partB\u001b[49m)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTesting accuracy:\u001b[39m\u001b[38;5;124m\"\u001b[39m, test_acc_partB)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_acc_partB' is not defined"
     ]
    }
   ],
   "source": [
    "# Final checkpoint\n",
    "\n",
    "# All of these should print OK and match up with what you have above:\n",
    "\n",
    "print(\"Part A\")\n",
    "print(\"Weights:\", w_direct)\n",
    "print(\"Cost:\", w_direct_cost)\n",
    "print(\"Training accuracy:\", train_acc_partA)\n",
    "print(\"Testing accuracy:\", test_acc_partA)\n",
    "print()\n",
    "print(\"Part B\")\n",
    "print(\"Weights:\", w_manual)\n",
    "print(\"Cost:\", w_manual_cost)\n",
    "print(\"Training accuracy:\", train_acc_partB)\n",
    "print(\"Testing accuracy:\", test_acc_partB)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4bf88d4-1db9-48a1-bca7-44e87b9c2dc3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
