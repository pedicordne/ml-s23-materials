{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e56e29b-6e7e-4781-b7fb-906d3e768580",
   "metadata": {},
   "source": [
    "# Project 3: Language detection\n",
    "\n",
    "In this project, you will create a 2-layer neural network by hand to detect\n",
    "what language a sentence is written in.  I have provided you a data set\n",
    "containing sentences in English, Spanish, French, German, Italian, and Portuguese,\n",
    "and your neural network will use the frequency of the letters in the sentence\n",
    "to detect the language.  \n",
    "\n",
    "To make life simple, your network will only process two languages at a time,\n",
    "for example, English-vs-Spanish; we do this so we can create a network that\n",
    "does binary classification rather than multi-category classification.\n",
    "\n",
    "## Features\n",
    "\n",
    "The features we will use in this project are the relative frequencies of the\n",
    "26 letters of the English alphabet in each sentence.  Each of the six languages\n",
    "in our data set uses letters differently; for instance, see here:\n",
    "\n",
    "http://letterfrequency.org/letter-frequency-by-language/\n",
    "\n",
    "For example, while both Spanish and English have \"e\" as their most common letter,\n",
    "the relative popularities of other letters differ in small or large ways.  Take a\n",
    "look at the letter \"t\": in English, it's the 2nd-most-common letter, but in Spanish,\n",
    "it barely makes the top third.  \"h\" is another letter that is used more frequently in English\n",
    "than in Spanish.  The letters \"w\" and \"k\" show the opposite pattern: these letters appear\n",
    "very rarely in Spanish but are common in English.\n",
    "\n",
    "**Caveat**: While all six languages use essentially the same letters, there are extra\n",
    "letters (plus accents) that we will have to handle.  Though linguistically questionable,\n",
    "we will use a Python library to convert any letter not in the English alphabet to the closest\n",
    "English letter.  For instance, in Spanish, \"á\" will be converted to \"a,\" \"ñ\" will be converted\n",
    "to \"n,\" etc.\n",
    "\n",
    "## Data\n",
    "\n",
    "The dataset contains a selection of 200 sentences in each of the six languages (1200 sentences in all).\n",
    "Each sentence is between 20 and 200 characters long.  **There is much more data available, but running this\n",
    "on individual laptops limits our ability to process more data.  Running your code on larger training sets\n",
    "will get you bonus points.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd3f631-4098-4914-ba1f-23148980f2c8",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Let's examine our data set a little."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2226b780-a240-45eb-972b-38c6ce45a6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "\n",
    "#!pip install unidecode\n",
    "#!pip install scikit-learn\n",
    "from unidecode import unidecode\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46d40713-5354-47e9-8b85-5dd050ab26ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lang</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>deu</td>\n",
       "      <td>Sie ging über die sieben Berge zu den sieben Z...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>deu</td>\n",
       "      <td>Es ist der gleiche Esel, aber ein anderer Sattel.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>deu</td>\n",
       "      <td>Darf ich an deinem Rechner meine Netzpost nach...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>deu</td>\n",
       "      <td>Wie viel macht sieben mal drei?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>deu</td>\n",
       "      <td>Sie hat ihn schreien hören.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1195</th>\n",
       "      <td>spa</td>\n",
       "      <td>Me recuerdo lo que vi.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1196</th>\n",
       "      <td>spa</td>\n",
       "      <td>No hablo francés. Tampoco hablo inglés.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1197</th>\n",
       "      <td>spa</td>\n",
       "      <td>Seguí así, que lograrás muchas cosas.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1198</th>\n",
       "      <td>spa</td>\n",
       "      <td>Es mejor estar con gente inteligente en el inf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1199</th>\n",
       "      <td>spa</td>\n",
       "      <td>Me lo encontré mientras estaba en Japón.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1200 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     lang                                               text\n",
       "0     deu  Sie ging über die sieben Berge zu den sieben Z...\n",
       "1     deu  Es ist der gleiche Esel, aber ein anderer Sattel.\n",
       "2     deu  Darf ich an deinem Rechner meine Netzpost nach...\n",
       "3     deu                    Wie viel macht sieben mal drei?\n",
       "4     deu                        Sie hat ihn schreien hören.\n",
       "...   ...                                                ...\n",
       "1195  spa                             Me recuerdo lo que vi.\n",
       "1196  spa            No hablo francés. Tampoco hablo inglés.\n",
       "1197  spa              Seguí así, que lograrás muchas cosas.\n",
       "1198  spa  Es mejor estar con gente inteligente en el inf...\n",
       "1199  spa           Me lo encontré mientras estaba en Japón.\n",
       "\n",
       "[1200 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take a look at the data\n",
    "\n",
    "all_data = pd.read_csv(\"six-languages.csv\")\n",
    "all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb0f1879-3471-4820-a93d-6b7b18b42ae2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lang</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>deu</td>\n",
       "      <td>Vier Familien kamen im Feuer ums Leben.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>deu</td>\n",
       "      <td>„Ich dachte, Tom wäre Deutscher.“ – „Nein, wei...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>deu</td>\n",
       "      <td>Tom sprang über den Bach hinüber.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>deu</td>\n",
       "      <td>Es stimmt etwas nicht.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>deu</td>\n",
       "      <td>Das Rindfleischetikettierungsüberwachungsaufga...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    lang                                               text\n",
       "151  deu            Vier Familien kamen im Feuer ums Leben.\n",
       "16   deu  „Ich dachte, Tom wäre Deutscher.“ – „Nein, wei...\n",
       "26   deu                  Tom sprang über den Bach hinüber.\n",
       "105  deu                             Es stimmt etwas nicht.\n",
       "27   deu  Das Rindfleischetikettierungsüberwachungsaufga..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lang</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>328</th>\n",
       "      <td>eng</td>\n",
       "      <td>They can add a literal translation.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317</th>\n",
       "      <td>eng</td>\n",
       "      <td>I need to go for a minute.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>eng</td>\n",
       "      <td>I think I should've helped Tom.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>eng</td>\n",
       "      <td>We must never forget what happened here.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281</th>\n",
       "      <td>eng</td>\n",
       "      <td>What's the deepest lake in the world?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    lang                                      text\n",
       "328  eng       They can add a literal translation.\n",
       "317  eng                I need to go for a minute.\n",
       "202  eng           I think I should've helped Tom.\n",
       "218  eng  We must never forget what happened here.\n",
       "281  eng     What's the deepest lake in the world?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lang</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>514</th>\n",
       "      <td>fra</td>\n",
       "      <td>Je n'ai pas encore trouvé ce que je cherche.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438</th>\n",
       "      <td>fra</td>\n",
       "      <td>Ils écrivent très proprement, n'est-ce pas?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>460</th>\n",
       "      <td>fra</td>\n",
       "      <td>Sami est imprévisible.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>507</th>\n",
       "      <td>fra</td>\n",
       "      <td>Vous étiez accrochées à la branche.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>554</th>\n",
       "      <td>fra</td>\n",
       "      <td>Émilie est chez elle.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    lang                                          text\n",
       "514  fra  Je n'ai pas encore trouvé ce que je cherche.\n",
       "438  fra   Ils écrivent très proprement, n'est-ce pas?\n",
       "460  fra                        Sami est imprévisible.\n",
       "507  fra           Vous étiez accrochées à la branche.\n",
       "554  fra                         Émilie est chez elle."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lang</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>604</th>\n",
       "      <td>ita</td>\n",
       "      <td>Quel negozio è chiuso il lunedì.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>605</th>\n",
       "      <td>ita</td>\n",
       "      <td>Scegliete uno o l'altro.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>734</th>\n",
       "      <td>ita</td>\n",
       "      <td>Non sono così sicura che Tom abbia ragione.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>796</th>\n",
       "      <td>ita</td>\n",
       "      <td>Rispettava le precedenze.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>665</th>\n",
       "      <td>ita</td>\n",
       "      <td>L'estate sta finendo?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    lang                                         text\n",
       "604  ita             Quel negozio è chiuso il lunedì.\n",
       "605  ita                     Scegliete uno o l'altro.\n",
       "734  ita  Non sono così sicura che Tom abbia ragione.\n",
       "796  ita                    Rispettava le precedenze.\n",
       "665  ita                        L'estate sta finendo?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lang</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>859</th>\n",
       "      <td>por</td>\n",
       "      <td>Não gosto de misturar negócios com diversão.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>828</th>\n",
       "      <td>por</td>\n",
       "      <td>Ele disse que foi ele fez isso.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>991</th>\n",
       "      <td>por</td>\n",
       "      <td>Você não deve passar por cima dos outros para ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>825</th>\n",
       "      <td>por</td>\n",
       "      <td>A liberdade é inútil a menos que você a use.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>993</th>\n",
       "      <td>por</td>\n",
       "      <td>Meteu suas ações no mercado americano.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    lang                                               text\n",
       "859  por       Não gosto de misturar negócios com diversão.\n",
       "828  por                    Ele disse que foi ele fez isso.\n",
       "991  por  Você não deve passar por cima dos outros para ...\n",
       "825  por       A liberdade é inútil a menos que você a use.\n",
       "993  por             Meteu suas ações no mercado americano."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lang</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1069</th>\n",
       "      <td>spa</td>\n",
       "      <td>Mary aún no se ha operado de anginas.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1080</th>\n",
       "      <td>spa</td>\n",
       "      <td>Terminaré en una hora.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1020</th>\n",
       "      <td>spa</td>\n",
       "      <td>Usted ya no me agrada.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1118</th>\n",
       "      <td>spa</td>\n",
       "      <td>En caso de necesidad, cualquiera con la intenc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1017</th>\n",
       "      <td>spa</td>\n",
       "      <td>Ayúdanos y te damos el pasaporte.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     lang                                               text\n",
       "1069  spa              Mary aún no se ha operado de anginas.\n",
       "1080  spa                             Terminaré en una hora.\n",
       "1020  spa                             Usted ya no me agrada.\n",
       "1118  spa  En caso de necesidad, cualquiera con la intenc...\n",
       "1017  spa                  Ayúdanos y te damos el pasaporte."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display a few sentences from each language (at random).\n",
    "# The languages are denoted by three-letter abbreviations:\n",
    "\n",
    "# deu = German\n",
    "# eng = English\n",
    "# fra = French\n",
    "# ita = Italian\n",
    "# por = Portuguese\n",
    "# spa = Spanish\n",
    "\n",
    "LANGUAGES = ['deu', 'eng', 'fra', 'ita', 'por', 'spa']\n",
    "\n",
    "for lang in LANGUAGES:\n",
    "    display(all_data[all_data['lang'] == lang].sample(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc4344b-c485-497b-9179-fe86f3fdba90",
   "metadata": {},
   "source": [
    "## Data cleaning\n",
    "\n",
    "Here, we will remove punctuation, spaces, convert everything to lowercase, and\n",
    "convert all characters to the 26-letter English alphabet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2b2e796-e158-4038-a873-98e215bf46ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before modifying:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lang</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>deu</td>\n",
       "      <td>Der harte Winter hat seine Spuren hinterlassen.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>eng</td>\n",
       "      <td>I like to go to the park and watch the childre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>459</th>\n",
       "      <td>fra</td>\n",
       "      <td>Tom a embrassé son chien.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>659</th>\n",
       "      <td>ita</td>\n",
       "      <td>Gli uomini d'affari vanno spesso a questo rist...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>859</th>\n",
       "      <td>por</td>\n",
       "      <td>Não gosto de misturar negócios com diversão.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1059</th>\n",
       "      <td>spa</td>\n",
       "      <td>Por favor hable más alto.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     lang                                               text\n",
       "59    deu    Der harte Winter hat seine Spuren hinterlassen.\n",
       "259   eng  I like to go to the park and watch the childre...\n",
       "459   fra                          Tom a embrassé son chien.\n",
       "659   ita  Gli uomini d'affari vanno spesso a questo rist...\n",
       "859   por       Não gosto de misturar negócios com diversão.\n",
       "1059  spa                          Por favor hable más alto."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After converting:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lang</th>\n",
       "      <th>text</th>\n",
       "      <th>text_alpha</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>deu</td>\n",
       "      <td>Der harte Winter hat seine Spuren hinterlassen.</td>\n",
       "      <td>derhartewinterhatseinespurenhinterlassen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>eng</td>\n",
       "      <td>I like to go to the park and watch the childre...</td>\n",
       "      <td>iliketogototheparkandwatchthechildrenintheplay...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>459</th>\n",
       "      <td>fra</td>\n",
       "      <td>Tom a embrassé son chien.</td>\n",
       "      <td>tomaembrassesonchien</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>659</th>\n",
       "      <td>ita</td>\n",
       "      <td>Gli uomini d'affari vanno spesso a questo rist...</td>\n",
       "      <td>gliuominidaffarivannospessoaquestoristorante</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>859</th>\n",
       "      <td>por</td>\n",
       "      <td>Não gosto de misturar negócios com diversão.</td>\n",
       "      <td>naogostodemisturarnegocioscomdiversao</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1059</th>\n",
       "      <td>spa</td>\n",
       "      <td>Por favor hable más alto.</td>\n",
       "      <td>porfavorhablemasalto</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     lang                                               text  \\\n",
       "59    deu    Der harte Winter hat seine Spuren hinterlassen.   \n",
       "259   eng  I like to go to the park and watch the childre...   \n",
       "459   fra                          Tom a embrassé son chien.   \n",
       "659   ita  Gli uomini d'affari vanno spesso a questo rist...   \n",
       "859   por       Não gosto de misturar negócios com diversão.   \n",
       "1059  spa                          Por favor hable más alto.   \n",
       "\n",
       "                                             text_alpha  \n",
       "59             derhartewinterhatseinespurenhinterlassen  \n",
       "259   iliketogototheparkandwatchthechildrenintheplay...  \n",
       "459                                tomaembrassesonchien  \n",
       "659        gliuominidaffarivannospessoaquestoristorante  \n",
       "859               naogostodemisturarnegocioscomdiversao  \n",
       "1059                               porfavorhablemasalto  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With frequencies:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lang</th>\n",
       "      <th>text</th>\n",
       "      <th>text_alpha</th>\n",
       "      <th>freq_a</th>\n",
       "      <th>freq_b</th>\n",
       "      <th>freq_c</th>\n",
       "      <th>freq_d</th>\n",
       "      <th>freq_e</th>\n",
       "      <th>freq_f</th>\n",
       "      <th>freq_g</th>\n",
       "      <th>freq_h</th>\n",
       "      <th>freq_i</th>\n",
       "      <th>freq_j</th>\n",
       "      <th>freq_k</th>\n",
       "      <th>freq_l</th>\n",
       "      <th>freq_m</th>\n",
       "      <th>freq_n</th>\n",
       "      <th>freq_o</th>\n",
       "      <th>freq_p</th>\n",
       "      <th>freq_q</th>\n",
       "      <th>freq_r</th>\n",
       "      <th>freq_s</th>\n",
       "      <th>freq_t</th>\n",
       "      <th>freq_u</th>\n",
       "      <th>freq_v</th>\n",
       "      <th>freq_w</th>\n",
       "      <th>freq_x</th>\n",
       "      <th>freq_y</th>\n",
       "      <th>freq_z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>deu</td>\n",
       "      <td>Der harte Winter hat seine Spuren hinterlassen.</td>\n",
       "      <td>derhartewinterhatseinespurenhinterlassen</td>\n",
       "      <td>0.075000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.075000</td>\n",
       "      <td>0.075000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>eng</td>\n",
       "      <td>I like to go to the park and watch the childre...</td>\n",
       "      <td>iliketogototheparkandwatchthechildrenintheplay...</td>\n",
       "      <td>0.057143</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.028571</td>\n",
       "      <td>0.042857</td>\n",
       "      <td>0.128571</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.042857</td>\n",
       "      <td>0.085714</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.014286</td>\n",
       "      <td>0.028571</td>\n",
       "      <td>0.057143</td>\n",
       "      <td>0.014286</td>\n",
       "      <td>0.085714</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.028571</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.042857</td>\n",
       "      <td>0.028571</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.014286</td>\n",
       "      <td>0.014286</td>\n",
       "      <td>0.014286</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.028571</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>459</th>\n",
       "      <td>fra</td>\n",
       "      <td>Tom a embrassé son chien.</td>\n",
       "      <td>tomaembrassesonchien</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>659</th>\n",
       "      <td>ita</td>\n",
       "      <td>Gli uomini d'affari vanno spesso a questo rist...</td>\n",
       "      <td>gliuominidaffarivannospessoaquestoristorante</td>\n",
       "      <td>0.113636</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022727</td>\n",
       "      <td>0.068182</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.022727</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.113636</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022727</td>\n",
       "      <td>0.022727</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.113636</td>\n",
       "      <td>0.022727</td>\n",
       "      <td>0.022727</td>\n",
       "      <td>0.068182</td>\n",
       "      <td>0.113636</td>\n",
       "      <td>0.068182</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.022727</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>859</th>\n",
       "      <td>por</td>\n",
       "      <td>Não gosto de misturar negócios com diversão.</td>\n",
       "      <td>naogostodemisturarnegocioscomdiversao</td>\n",
       "      <td>0.081081</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.054054</td>\n",
       "      <td>0.054054</td>\n",
       "      <td>0.081081</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.054054</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.081081</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.054054</td>\n",
       "      <td>0.054054</td>\n",
       "      <td>0.189189</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.081081</td>\n",
       "      <td>0.108108</td>\n",
       "      <td>0.054054</td>\n",
       "      <td>0.027027</td>\n",
       "      <td>0.027027</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1059</th>\n",
       "      <td>spa</td>\n",
       "      <td>Por favor hable más alto.</td>\n",
       "      <td>porfavorhablemasalto</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     lang                                               text  \\\n",
       "59    deu    Der harte Winter hat seine Spuren hinterlassen.   \n",
       "259   eng  I like to go to the park and watch the childre...   \n",
       "459   fra                          Tom a embrassé son chien.   \n",
       "659   ita  Gli uomini d'affari vanno spesso a questo rist...   \n",
       "859   por       Não gosto de misturar negócios com diversão.   \n",
       "1059  spa                          Por favor hable más alto.   \n",
       "\n",
       "                                             text_alpha    freq_a  freq_b  \\\n",
       "59             derhartewinterhatseinespurenhinterlassen  0.075000    0.00   \n",
       "259   iliketogototheparkandwatchthechildrenintheplay...  0.057143    0.00   \n",
       "459                                tomaembrassesonchien  0.100000    0.05   \n",
       "659        gliuominidaffarivannospessoaquestoristorante  0.113636    0.00   \n",
       "859               naogostodemisturarnegocioscomdiversao  0.081081    0.00   \n",
       "1059                               porfavorhablemasalto  0.200000    0.05   \n",
       "\n",
       "        freq_c    freq_d    freq_e    freq_f    freq_g    freq_h    freq_i  \\\n",
       "59    0.000000  0.025000  0.200000  0.000000  0.000000  0.075000  0.075000   \n",
       "259   0.028571  0.042857  0.128571  0.000000  0.042857  0.085714  0.071429   \n",
       "459   0.050000  0.000000  0.150000  0.000000  0.000000  0.050000  0.050000   \n",
       "659   0.000000  0.022727  0.068182  0.045455  0.022727  0.000000  0.113636   \n",
       "859   0.054054  0.054054  0.081081  0.000000  0.054054  0.000000  0.081081   \n",
       "1059  0.000000  0.000000  0.050000  0.050000  0.000000  0.050000  0.000000   \n",
       "\n",
       "        freq_j    freq_k    freq_l    freq_m    freq_n    freq_o    freq_p  \\\n",
       "59    0.000000  0.000000  0.025000  0.000000  0.125000  0.000000  0.025000   \n",
       "259   0.014286  0.028571  0.057143  0.014286  0.085714  0.071429  0.028571   \n",
       "459   0.000000  0.000000  0.000000  0.100000  0.100000  0.100000  0.000000   \n",
       "659   0.000000  0.000000  0.022727  0.022727  0.090909  0.113636  0.022727   \n",
       "859   0.000000  0.000000  0.000000  0.054054  0.054054  0.189189  0.000000   \n",
       "1059  0.000000  0.000000  0.100000  0.050000  0.000000  0.150000  0.050000   \n",
       "\n",
       "        freq_q    freq_r    freq_s    freq_t    freq_u    freq_v    freq_w  \\\n",
       "59    0.000000  0.125000  0.100000  0.100000  0.025000  0.000000  0.025000   \n",
       "259   0.000000  0.042857  0.028571  0.100000  0.014286  0.014286  0.014286   \n",
       "459   0.000000  0.050000  0.150000  0.050000  0.000000  0.000000  0.000000   \n",
       "659   0.022727  0.068182  0.113636  0.068182  0.045455  0.022727  0.000000   \n",
       "859   0.000000  0.081081  0.108108  0.054054  0.027027  0.027027  0.000000   \n",
       "1059  0.000000  0.100000  0.050000  0.050000  0.000000  0.050000  0.000000   \n",
       "\n",
       "      freq_x    freq_y  freq_z  \n",
       "59       0.0  0.000000     0.0  \n",
       "259      0.0  0.028571     0.0  \n",
       "459      0.0  0.000000     0.0  \n",
       "659      0.0  0.000000     0.0  \n",
       "859      0.0  0.000000     0.0  \n",
       "1059     0.0  0.000000     0.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Remove all punctuation from the sentences, and convert all characters\n",
    "# to the 26-letter English alphabet.\n",
    "# We do this by first removing all characters from each sentence that \n",
    "\n",
    "ALL_LETTERS = \"abcdefghijklmnopqrstuvwxyz\"\n",
    "\n",
    "# Illustrate with some data.\n",
    "# Print one example from each language before modifying.\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "print(\"Before modifying:\")\n",
    "sample = pd.DataFrame()\n",
    "for lang in LANGUAGES:\n",
    "    sample = pd.concat([sample, all_data[all_data['lang'] == lang].sample(1, random_state=10)])\n",
    "display(sample)\n",
    "    \n",
    "all_data_freq = all_data.copy()\n",
    "\n",
    "# Remove punctuation & spaces, convert to lowercase, convert to 26-letter English alphabet.\n",
    "\n",
    "all_data_freq['text_alpha'] = all_data_freq['text'].map(lambda str: unidecode(\"\".join(c for c in str.lower() if c.isalpha())))\n",
    "print(\"\\nAfter converting:\")\n",
    "sample = pd.DataFrame()\n",
    "for lang in LANGUAGES:\n",
    "    sample = pd.concat([sample, all_data_freq[all_data_freq['lang'] == lang].sample(1, random_state=10)])\n",
    "display(sample)\n",
    "    \n",
    "# Find letter frequencies.\n",
    "\n",
    "for letter in ALL_LETTERS:\n",
    "    all_data_freq['freq_' + letter] = all_data_freq['text_alpha'].map(lambda str: str.lower().count(letter)/len(str))\n",
    "    \n",
    "print(\"With frequencies:\")\n",
    "sample = pd.DataFrame()\n",
    "for lang in LANGUAGES:\n",
    "    sample = pd.concat([sample, all_data_freq[all_data_freq['lang'] == lang].sample(1, random_state=10)])\n",
    "display(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c5f522-f152-4af6-b72f-eaa33585f9bc",
   "metadata": {},
   "source": [
    "## Part A: Single layer neural network\n",
    "\n",
    "Here, you will write code to create a single layer neural network that can distinguish between two\n",
    "languages of your choice (from the six options)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e21d9138-84a9-4cce-b97a-5badf3acfaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Part A.\n",
    "\n",
    "# Choose your languages.  Edit the variables below to set one language\n",
    "# to be the \"positive class\" (LANG1) and one to be the \"negative class\" (LANG0).\n",
    "# Use the three-letter abbreviation: deu, eng, fra, ita, por, spa.\n",
    "\n",
    "LANG1 = 'spa'  # positive category\n",
    "LANG0 = 'eng'  # negative category\n",
    "\n",
    "# SANITY CHECKS BELOW ARE GIVEN FOR SPANISH/ENGLISH, but after you make sure those\n",
    "# two languages are working, you can play around with others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2522912c-11b8-4866-a703-0db890bb2647",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lang</th>\n",
       "      <th>freq_a</th>\n",
       "      <th>freq_b</th>\n",
       "      <th>freq_c</th>\n",
       "      <th>freq_d</th>\n",
       "      <th>freq_e</th>\n",
       "      <th>freq_f</th>\n",
       "      <th>freq_g</th>\n",
       "      <th>freq_h</th>\n",
       "      <th>freq_i</th>\n",
       "      <th>freq_j</th>\n",
       "      <th>freq_k</th>\n",
       "      <th>freq_l</th>\n",
       "      <th>freq_m</th>\n",
       "      <th>freq_n</th>\n",
       "      <th>freq_o</th>\n",
       "      <th>freq_p</th>\n",
       "      <th>freq_q</th>\n",
       "      <th>freq_r</th>\n",
       "      <th>freq_s</th>\n",
       "      <th>freq_t</th>\n",
       "      <th>freq_u</th>\n",
       "      <th>freq_v</th>\n",
       "      <th>freq_w</th>\n",
       "      <th>freq_x</th>\n",
       "      <th>freq_y</th>\n",
       "      <th>freq_z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>0</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.138889</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>0</td>\n",
       "      <td>0.078947</td>\n",
       "      <td>0.026316</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.026316</td>\n",
       "      <td>0.078947</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.026316</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>0.026316</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.078947</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.184211</td>\n",
       "      <td>0.026316</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.026316</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>0</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>0.074074</td>\n",
       "      <td>0.296296</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.074074</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>0</td>\n",
       "      <td>0.060606</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>0.151515</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.060606</td>\n",
       "      <td>0.060606</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>0.060606</td>\n",
       "      <td>0.060606</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>0.060606</td>\n",
       "      <td>0.212121</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1195</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.235294</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1196</th>\n",
       "      <td>1</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.093750</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.093750</td>\n",
       "      <td>0.156250</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1197</th>\n",
       "      <td>1</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1198</th>\n",
       "      <td>1</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.212121</td>\n",
       "      <td>0.015152</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.075758</td>\n",
       "      <td>0.015152</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.015152</td>\n",
       "      <td>0.151515</td>\n",
       "      <td>0.106061</td>\n",
       "      <td>0.015152</td>\n",
       "      <td>0.015152</td>\n",
       "      <td>0.060606</td>\n",
       "      <td>0.060606</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.015152</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1199</th>\n",
       "      <td>1</td>\n",
       "      <td>0.121212</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>0.060606</td>\n",
       "      <td>0.151515</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.060606</td>\n",
       "      <td>0.060606</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      lang    freq_a    freq_b    freq_c    freq_d    freq_e    freq_f  \\\n",
       "200      0  0.111111  0.000000  0.000000  0.083333  0.083333  0.000000   \n",
       "201      0  0.078947  0.026316  0.052632  0.026316  0.078947  0.052632   \n",
       "202      0  0.000000  0.000000  0.000000  0.083333  0.125000  0.000000   \n",
       "203      0  0.037037  0.000000  0.037037  0.074074  0.296296  0.037037   \n",
       "204      0  0.060606  0.000000  0.000000  0.030303  0.151515  0.000000   \n",
       "...    ...       ...       ...       ...       ...       ...       ...   \n",
       "1195     1  0.000000  0.000000  0.058824  0.058824  0.235294  0.000000   \n",
       "1196     1  0.125000  0.062500  0.062500  0.000000  0.062500  0.031250   \n",
       "1197     1  0.166667  0.000000  0.066667  0.000000  0.066667  0.000000   \n",
       "1198     1  0.045455  0.000000  0.030303  0.000000  0.212121  0.015152   \n",
       "1199     1  0.121212  0.030303  0.030303  0.000000  0.181818  0.000000   \n",
       "\n",
       "        freq_g    freq_h    freq_i    freq_j    freq_k    freq_l    freq_m  \\\n",
       "200   0.000000  0.055556  0.000000  0.000000  0.027778  0.000000  0.055556   \n",
       "201   0.052632  0.026316  0.105263  0.000000  0.000000  0.105263  0.026316   \n",
       "202   0.000000  0.125000  0.125000  0.000000  0.041667  0.083333  0.041667   \n",
       "203   0.000000  0.037037  0.111111  0.000000  0.000000  0.074074  0.037037   \n",
       "204   0.000000  0.060606  0.060606  0.000000  0.030303  0.030303  0.060606   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "1195  0.000000  0.000000  0.058824  0.000000  0.000000  0.058824  0.058824   \n",
       "1196  0.031250  0.062500  0.031250  0.000000  0.000000  0.093750  0.031250   \n",
       "1197  0.066667  0.033333  0.066667  0.000000  0.000000  0.033333  0.033333   \n",
       "1198  0.030303  0.000000  0.075758  0.015152  0.000000  0.045455  0.015152   \n",
       "1199  0.000000  0.000000  0.030303  0.030303  0.000000  0.030303  0.060606   \n",
       "\n",
       "        freq_n    freq_o    freq_p    freq_q    freq_r    freq_s    freq_t  \\\n",
       "200   0.083333  0.138889  0.000000  0.000000  0.083333  0.000000  0.083333   \n",
       "201   0.052632  0.078947  0.000000  0.000000  0.000000  0.000000  0.184211   \n",
       "202   0.041667  0.083333  0.041667  0.000000  0.000000  0.041667  0.083333   \n",
       "203   0.037037  0.000000  0.000000  0.000000  0.000000  0.111111  0.111111   \n",
       "204   0.060606  0.090909  0.000000  0.000000  0.030303  0.060606  0.212121   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "1195  0.000000  0.117647  0.000000  0.058824  0.117647  0.000000  0.000000   \n",
       "1196  0.093750  0.156250  0.031250  0.000000  0.031250  0.062500  0.031250   \n",
       "1197  0.000000  0.066667  0.000000  0.033333  0.066667  0.200000  0.000000   \n",
       "1198  0.151515  0.106061  0.015152  0.015152  0.060606  0.060606  0.090909   \n",
       "1199  0.151515  0.090909  0.030303  0.000000  0.060606  0.060606  0.090909   \n",
       "\n",
       "        freq_u    freq_v    freq_w  freq_x    freq_y  freq_z  \n",
       "200   0.027778  0.000000  0.055556     0.0  0.111111     0.0  \n",
       "201   0.026316  0.000000  0.026316     0.0  0.000000     0.0  \n",
       "202   0.041667  0.041667  0.000000     0.0  0.000000     0.0  \n",
       "203   0.000000  0.000000  0.000000     0.0  0.000000     0.0  \n",
       "204   0.030303  0.000000  0.000000     0.0  0.030303     0.0  \n",
       "...        ...       ...       ...     ...       ...     ...  \n",
       "1195  0.117647  0.058824  0.000000     0.0  0.000000     0.0  \n",
       "1196  0.000000  0.000000  0.000000     0.0  0.000000     0.0  \n",
       "1197  0.100000  0.000000  0.000000     0.0  0.000000     0.0  \n",
       "1198  0.015152  0.000000  0.000000     0.0  0.000000     0.0  \n",
       "1199  0.000000  0.000000  0.000000     0.0  0.000000     0.0  \n",
       "\n",
       "[400 rows x 27 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# I have written some code to produce a data set from the one above with only\n",
    "# our two chosen languages.  It also removes the full sentences (which we don't need),\n",
    "# since we're only training on the \"freq_\" columns.\n",
    "\n",
    "def make_dataset(complete_data, pos_cat, neg_cat):\n",
    "    data = complete_data.copy()\n",
    "    data = data.drop(columns=['text', 'text_alpha'])\n",
    "    data = data[data['lang'].isin([pos_cat, neg_cat])]\n",
    "    data['lang'] = (data['lang'] == pos_cat).astype(int)\n",
    "    return data\n",
    "\n",
    "data = make_dataset(all_data_freq, LANG1, LANG0)\n",
    "display(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "ab60c839-e90b-47a3-b200-14953a849918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 26)\n",
      "(200, 26)\n",
      "(200,)\n",
      "(200,)\n",
      "(array([0, 1]), array([100, 100], dtype=int64))\n",
      "(array([0, 1]), array([100, 100], dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "## Create testing and training sets.\n",
    "\n",
    "# In the real world, we would shuffle the data to make a random training/testing split,\n",
    "# but here, since we have 200 examples, we're going to use the first half of each\n",
    "# language for training and the last half for testing.\n",
    "\n",
    "# In our data set above, the first half of the examples are one class and the last half\n",
    "# are the other class (although depending on the languages you chose, 0 might be first and 1\n",
    "# second, or the other way around, though it doesn't matter).  \n",
    "\n",
    "# So create a training set of rows 0-99 and 200-299,\n",
    "# and a testing set of rows 100-199 and 300-399\n",
    "# \n",
    "\n",
    "# Make training set:\n",
    "X = data.copy().drop(columns=['lang'])  # get rid of the lang column\n",
    "y = data.copy()['lang']  # keep only the lang column\n",
    "                      \n",
    "X_train = pd.concat([X.iloc[0:100], X.iloc[200:300]]).to_numpy()\n",
    "X_test = pd.concat([X.iloc[100:200], X.iloc[300:400]]).to_numpy()\n",
    "y_train = pd.concat([y.iloc[0:100], y.iloc[200:300]]).to_numpy()\n",
    "y_test = pd.concat([y.iloc[100:200], y.iloc[300:400]]).to_numpy()\n",
    "\n",
    "# Sanity checks.\n",
    "print(X_train.shape)  # should be (200, 26)\n",
    "print(X_test.shape) # should be (200, 26)\n",
    "print(y_train.shape) # should be (200,)\n",
    "print(y_test.shape) # should be (200,)\n",
    "print(np.unique(y_train, return_counts=True)) # count the number of 0's and 1's, should be 100 of each\n",
    "print(np.unique(y_test, return_counts=True)) # count the number of 0's and 1's, should be 100 of each"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0e1002-79eb-4c98-b20c-6f648c90ff2d",
   "metadata": {},
   "source": [
    "## Single Layer Neural Network Refresher\n",
    "\n",
    "Recall that in a single layer network, we have the following variables.  Some variables have their superscripts\n",
    "dropped because in a single layer network, there's only one copy of them.\n",
    "\n",
    "- $x$: values of our features that are in the input to the network.  Here, this\n",
    "  is a vector of 26 letter frequencies.  In the code, this is called `x`.\n",
    "- $in^{[1]}$: A copy of $x$, but with a 1 appended at the front.  In code, this is called `in` or `in1`.\n",
    "- $W$ or $W^{[1]}$: Weight matrix. Dimensions are 1 row by 27 columns, because we have 27 features and 1 output\n",
    "  in our network.  In code, this is `W`.\n",
    "- $z$ or $z^{[1]}$: Computed as the matrix product of $W$ and the inputs $in^{[1]}$.  Because $W$ is only one row, this is \n",
    "  essentially the dot product of $W$ (treated as a vector) and $in^{[1]}$.  In code, this is `z` or `z1`.\n",
    "- $a$ or $a^{[1]}$: Computed by the sigmoid function (activation function) applied to $z^{[1]}$.\n",
    "  In code, this is `a` or `a1`.\n",
    "  \n",
    "Recall that a neural network's output is the result of the last activation function, which here is $a$ or $a^{[1]}$.\n",
    "\n",
    "The loss function for a neural network doing classification is the cross-entropy loss function, which is:\n",
    "\n",
    "$L(\\hat{y}, y) = \\dfrac{\\hat{y} - y}{\\hat{y}  (1 - \\hat{y})}$.\n",
    "\n",
    "Because the predicted value of a neural net, $\\hat{y}$ is the same as the output variable $a$ or $a^{[1]}$ here,\n",
    "we will often see:\n",
    "\n",
    "$L(a, y) = \\dfrac{a - y}{a  (1 - a)}$.\n",
    "\n",
    "The cost function for a neural net is just the average loss over an entire set of training examples:\n",
    "\n",
    "$J(W) = \\displaystyle \\dfrac{1}{m}\\sum_{i=1}^{m} L(\\hat{y}^{(i)}, y^{(i)})$,\n",
    "\n",
    "where $y^{(i)}$ is the $i$'th training example *correct* output (0 or 1) and $\\hat{y}^{(i)} = a^{(i)}$ is the \n",
    "corresponding *predicted* output (a number between 0 and 1) for that same $i$'th training example.\n",
    "\n",
    "The forward propagation code for this neural net is already written for you.  Note that it returns\n",
    "three variables, `in`, `z1`, and `a1`.\n",
    "  \n",
    "You must write the following functions:\n",
    "\n",
    "- `compute_cost`: Compute the cost (average loss) over the training set.\n",
    "- `backward_prop`: Run backpropagation.\n",
    "\n",
    "The values you are computing for backpropagation are the entries in the matrix $\\dfrac{\\partial L}{\\partial w_j}$.\n",
    "We are using the notation $w_j$ to stand for the $j$'th entry in the weight matrix $W$.  Because $W$ has only one row,\n",
    "$w_j$ is stored in the variable `W[0][j]` and the corresponding partial derivative will be in `dL_dW[0][j]`.\n",
    "\n",
    "Formula: $\\dfrac{\\partial L}{\\partial w_j} = \\dfrac{\\partial L}{\\partial a} \\cdot \\dfrac{\\partial a}{\\partial z}\n",
    "  \\cdot \\dfrac{\\partial z}{\\partial w_j}$\n",
    "  \n",
    "where\n",
    "\n",
    "$\\dfrac{\\partial L}{\\partial a} = \\dfrac{a-y}{a(1-a)}$\n",
    "\n",
    "$\\dfrac{\\partial a}{\\partial z} = \\sigma(z)(1-\\sigma(z))$\n",
    "\n",
    "and\n",
    "\n",
    "$\\dfrac{\\partial z}{\\partial w_j} = in_j$\n",
    "\n",
    "So you will return a single row, 27 column matrix with entries using the formula above (three multiplications).\n",
    "You can do this with a for loop.\n",
    "Note that the first two terms are the same for all entries in the matrix, and so don't need to be computed inside the loop.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "90ae7557-0abb-42cb-8c28-7f9684107123",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single layer neural network\n",
    "\n",
    "INPUT_SIZE = 27\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1/(1 + np.exp(-z))\n",
    "\n",
    "def compute_z(W, inputs):\n",
    "    '''\n",
    "    Compute the z vector for a neural network.\n",
    "    inputs: inputs to the neural network, vector of variable size.\n",
    "    W: weight matrix for the inputs, matrix of variable size.\n",
    "    '''\n",
    "    return W @ inputs\n",
    "\n",
    "def compute_activation(z_vector):\n",
    "    return sigmoid(z_vector)\n",
    "\n",
    "def deriv_activation(z):\n",
    "    return sigmoid(z) * (1 - sigmoid(z))\n",
    "\n",
    "def deriv_cross_entropy_loss(a, y):\n",
    "    return (a - y)/(a * (1 - a))\n",
    "\n",
    "def compute_cross_entropy_loss(a, y):\n",
    "    if y == 0:\n",
    "        return -np.log(1-a)\n",
    "    else:\n",
    "        return -np.log(a)\n",
    "\n",
    "def augment_vector(v):\n",
    "    return np.insert(v, 0, 1)\n",
    "\n",
    "def forward_prop(W, x):\n",
    "    '''\n",
    "    Run the forward propagation algorithm.\n",
    "    x: input to the neural network, vector of size INPUT_SIZE - 1\n",
    "    W: weight matrix, matrix of size (1, INPUT_SIZE)\n",
    "    returns: 2 scalars, z1 and a1\n",
    "    '''\n",
    "    DEBUG = False\n",
    "    \n",
    "    # we assume x DOES NOT have a 1 in front.\n",
    "    assert len(x) == INPUT_SIZE - 1\n",
    "    \n",
    "    # check dimensions of W\n",
    "    assert W.shape == (1, INPUT_SIZE)\n",
    "    \n",
    "    # set inputs\n",
    "    in1 = augment_vector(x)\n",
    "    if DEBUG: print(\"Input to this layer is\", in1)\n",
    "    \n",
    "    # make sure dimensions match inputs and W\n",
    "    assert len(in1) == W.shape[1]\n",
    "\n",
    "    # get W matrix\n",
    "    if DEBUG: print(\"Using W of shape\", W.shape)\n",
    "\n",
    "    # compute z1\n",
    "    z1 = compute_z(W, in1)\n",
    "    if DEBUG: print(\"z is\", z1)\n",
    "\n",
    "    # compute a1\n",
    "    a1 = compute_activation(z1)\n",
    "    if DEBUG: print(\"a1 is\", a1)\n",
    "    \n",
    "    #print(\"x\", x)\n",
    "    #print()\n",
    "    \n",
    "    return in1, z1, a1\n",
    "    \n",
    "def compute_output(x, W):\n",
    "    '''\n",
    "    Returns the output of the neural network (probability of x being in the 1 class).\n",
    "    x: input to the neural network, vector of size INPUT_SIZE - 1\n",
    "    W: weight matrix, matrix of size (1, INPUT_SIZE)\n",
    "    '''\n",
    "    \n",
    "    # we assume x DOES NOT have a 1 in front.\n",
    "    assert len(x) == INPUT_SIZE - 1\n",
    "    \n",
    "    # check dimensions of W\n",
    "    assert W.shape == (1, INPUT_SIZE)\n",
    "    \n",
    "    in1, z1, a1 = forward_prop(W, x)\n",
    "    return a1\n",
    "\n",
    "def make_prediction(x, W):\n",
    "    '''\n",
    "    Returns the classification of x.\n",
    "    x: input to the neural network, vector of size INPUT_SIZE - 1\n",
    "    W: weight matrix, matrix of size (1, INPUT_SIZE)\n",
    "    '''\n",
    "    \n",
    "    # we assume x DOES NOT have a 1 in front.\n",
    "    assert len(x) == INPUT_SIZE - 1\n",
    "    \n",
    "    # check dimensions of W\n",
    "    assert W.shape == (1, INPUT_SIZE)\n",
    "    \n",
    "    in1, z1, a1 = forward_prop(W, x)\n",
    "    if a1 >= 0.5:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def compute_cost(X_data, y_data, W):\n",
    "    '''\n",
    "    Compute the cost (average loss) over the training examples in X_data, y_data.\n",
    "    W: weight matrix, matrix of size (1, INPUT_SIZE)\n",
    "    returns: a scalar with the cost\n",
    "    '''\n",
    "    \n",
    "    # check dimensions of W\n",
    "    assert W.shape == (1, INPUT_SIZE)\n",
    "    \n",
    "    # we assume x DOES NOT have a 1 in front.\n",
    "    #assert len(x) == INPUT_SIZE - 1\n",
    "    #print(W)\n",
    "    \n",
    "    m = X_data.shape[0] # number of data points\n",
    "    #print(m)\n",
    "    cost = 0\n",
    "    #print(\"X_data: \", X_data)\n",
    "    ##print(\"y_data: \", y_data)\n",
    "    #print(\"W: \", W)\n",
    "    \n",
    "    for i in range(m):\n",
    "        #z = compute_z(W, X_data[i])\n",
    "        #a = compute_activation(z)\n",
    "        y = y_data[i]\n",
    "        y_hat = make_prediction(X_data[i], W)\n",
    "        cost += deriv_cross_entropy_loss(y_hat, y)\n",
    "        #print(\"x_i\", x_i)\n",
    "        #print(\"X_data[i]\", X_data[i])\n",
    "    print(cost)   \n",
    "    total_cost = (1 / (2 * m)) * cost\n",
    "    #print(\"x_i\", x_i)\n",
    "    #print(\"X_data[i]\", x_data[i])\n",
    "\n",
    "    return total_cost\n",
    "    \n",
    "\n",
    "def backward_prop(W, y, in1, z1, a1):\n",
    "    '''\n",
    "    Run backward propagation.\n",
    "    W: weight matrix of size (1, INPUT_SIZE)\n",
    "    y: scalar of correct target output, 0 or 1\n",
    "    in1: inputs to the neural network, length INPUT_SIZE\n",
    "    z1: z1 scalar from forward prop of NN\n",
    "    a1: activation vector from forward prop of NN\n",
    "    Returns: partial derivatives of loss function with\n",
    "       respect to each entry in weight matrix W (same dimensions as W)\n",
    "    '''\n",
    "    \n",
    "    # make sure dimensions match inputs and W\n",
    "    assert len(in1) == W.shape[1]\n",
    "    \n",
    "    # make derivative matrix of same size as W, filled with zeros\n",
    "    dL_dW = np.full_like(W, 0)\n",
    "    \n",
    "    # compute dL/dw\n",
    "    dL_da = deriv_cross_entropy_loss(a1, y)\n",
    "    da_dz = deriv_activation(z1)\n",
    "    dz_dw0 = in1\n",
    "    # multiply\n",
    "    dL_dW = dL_da * da_dz * dz_dw0\n",
    "    \n",
    "    \n",
    "    return dL_dW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "d502f34a-8180-494e-88fa-ba76d62bc1a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.         0.11111111 0.         0.         0.08333333 0.08333333\n",
      " 0.         0.         0.05555556 0.         0.         0.02777778\n",
      " 0.         0.05555556 0.08333333 0.13888889 0.         0.\n",
      " 0.08333333 0.         0.08333333 0.02777778 0.         0.05555556\n",
      " 0.         0.11111111 0.        ] \n",
      " [2.] \n",
      " [0.88079708]\n"
     ]
    }
   ],
   "source": [
    "# Sanity checks for forward_prop\n",
    "\n",
    "W_sanity = np.ones((1, INPUT_SIZE))\n",
    "in1, z1, a1 = forward_prop(W_sanity, X_train[0])\n",
    "print(in1, \"\\n\", z1, \"\\n\", a1)\n",
    "\n",
    "# output:\n",
    "#[1.         0.11111111 0.         0.         0.08333333 0.08333333\n",
    "# 0.         0.         0.05555556 0.         0.         0.02777778\n",
    "# 0.         0.05555556 0.08333333 0.13888889 0.         0.\n",
    "# 0.08333333 0.         0.08333333 0.02777778 0.         0.05555556\n",
    "# 0.         0.11111111 0.        ] \n",
    "# [2.] \n",
    "# [0.88079708]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "8250a99f-c586-4e6e-a562-4b6728857800",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nikki\\AppData\\Local\\Temp\\ipykernel_5300\\301934505.py:23: RuntimeWarning: divide by zero encountered in long_scalars\n",
      "  return (a - y)/(a * (1 - a))\n",
      "C:\\Users\\nikki\\AppData\\Local\\Temp\\ipykernel_5300\\301934505.py:23: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  return (a - y)/(a * (1 - a))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sanity checks for compute_cost\n",
    "\n",
    "compute_cost(X_train, y_train, W_sanity)  # should be array([1.12692801])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b18092cc-5c8e-4ebc-84b7-03cef5ef7f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.88079708 0.09786634 0.         0.         0.07339976 0.07339976\n",
      " 0.         0.         0.04893317 0.         0.         0.02446659\n",
      " 0.         0.04893317 0.07339976 0.12233293 0.         0.\n",
      " 0.07339976 0.         0.07339976 0.02446659 0.         0.04893317\n",
      " 0.         0.09786634 0.        ]\n"
     ]
    }
   ],
   "source": [
    "# Sanity checks for backprop\n",
    "\n",
    "print(backward_prop(W_sanity, y_train[0], in1, z1, a1))\n",
    "\n",
    "#[[0.88079708 0.09786634 0.         0.         0.07339976 0.07339976\n",
    "#  0.         0.         0.04893317 0.         0.         0.02446659\n",
    "#  0.         0.04893317 0.07339976 0.12233293 0.         0.\n",
    "#  0.07339976 0.         0.07339976 0.02446659 0.         0.04893317\n",
    "#  0.         0.09786634 0.        ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "92bafc3b-775e-48f6-b503-86d567305319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  0\n",
      "W = [[-7.7502355   0.08198839  0.87579969  0.76986291  0.62311651 -0.14944646\n",
      "   0.9067675   0.86015029  0.67561784  0.47358234  0.96546849  0.94096487\n",
      "   0.62154879  0.68899697  0.44856255  0.23718925  0.82862809  0.94201437\n",
      "   0.46792104  0.40149879  0.30379843  0.66418988  0.91647297  0.87953911\n",
      "   0.99471734  0.84992327  0.9808913 ]]\n",
      "Cost is [1.84292689]\n",
      "Iteration:  1\n",
      "W = [[-7.7502355   0.08198839  0.87579969  0.76986291  0.62311651 -0.14944646\n",
      "   0.9067675   0.86015029  0.67561784  0.47358234  0.96546849  0.94096487\n",
      "   0.62154879  0.68899697  0.44856255  0.23718925  0.82862809  0.94201437\n",
      "   0.46792104  0.40149879  0.30379843  0.66418988  0.91647297  0.87953911\n",
      "   0.99471734  0.84992327  0.9808913 ]]\n",
      "Cost is [1.84292689]\n",
      "Iteration:  2\n",
      "W = [[-7.7502355   0.08198839  0.87579969  0.76986291  0.62311651 -0.14944646\n",
      "   0.9067675   0.86015029  0.67561784  0.47358234  0.96546849  0.94096487\n",
      "   0.62154879  0.68899697  0.44856255  0.23718925  0.82862809  0.94201437\n",
      "   0.46792104  0.40149879  0.30379843  0.66418988  0.91647297  0.87953911\n",
      "   0.99471734  0.84992327  0.9808913 ]]\n",
      "Cost is [1.84292689]\n",
      "Iteration:  3\n",
      "W = [[-7.7502355   0.08198839  0.87579969  0.76986291  0.62311651 -0.14944646\n",
      "   0.9067675   0.86015029  0.67561784  0.47358234  0.96546849  0.94096487\n",
      "   0.62154879  0.68899697  0.44856255  0.23718925  0.82862809  0.94201437\n",
      "   0.46792104  0.40149879  0.30379843  0.66418988  0.91647297  0.87953911\n",
      "   0.99471734  0.84992327  0.9808913 ]]\n",
      "Cost is [1.84292689]\n",
      "Iteration:  4\n",
      "W = [[-7.7502355   0.08198839  0.87579969  0.76986291  0.62311651 -0.14944646\n",
      "   0.9067675   0.86015029  0.67561784  0.47358234  0.96546849  0.94096487\n",
      "   0.62154879  0.68899697  0.44856255  0.23718925  0.82862809  0.94201437\n",
      "   0.46792104  0.40149879  0.30379843  0.66418988  0.91647297  0.87953911\n",
      "   0.99471734  0.84992327  0.9808913 ]]\n",
      "Cost is [1.84292689]\n",
      "Iteration:  5\n",
      "W = [[-7.7502355   0.08198839  0.87579969  0.76986291  0.62311651 -0.14944646\n",
      "   0.9067675   0.86015029  0.67561784  0.47358234  0.96546849  0.94096487\n",
      "   0.62154879  0.68899697  0.44856255  0.23718925  0.82862809  0.94201437\n",
      "   0.46792104  0.40149879  0.30379843  0.66418988  0.91647297  0.87953911\n",
      "   0.99471734  0.84992327  0.9808913 ]]\n",
      "Cost is [1.84292689]\n",
      "Iteration:  6\n",
      "W = [[-7.7502355   0.08198839  0.87579969  0.76986291  0.62311651 -0.14944646\n",
      "   0.9067675   0.86015029  0.67561784  0.47358234  0.96546849  0.94096487\n",
      "   0.62154879  0.68899697  0.44856255  0.23718925  0.82862809  0.94201437\n",
      "   0.46792104  0.40149879  0.30379843  0.66418988  0.91647297  0.87953911\n",
      "   0.99471734  0.84992327  0.9808913 ]]\n",
      "Cost is [1.84292689]\n",
      "Iteration:  7\n",
      "W = [[-7.7502355   0.08198839  0.87579969  0.76986291  0.62311651 -0.14944646\n",
      "   0.9067675   0.86015029  0.67561784  0.47358234  0.96546849  0.94096487\n",
      "   0.62154879  0.68899697  0.44856255  0.23718925  0.82862809  0.94201437\n",
      "   0.46792104  0.40149879  0.30379843  0.66418988  0.91647297  0.87953911\n",
      "   0.99471734  0.84992327  0.9808913 ]]\n",
      "Cost is [1.84292689]\n",
      "Iteration:  8\n",
      "W = [[-7.7502355   0.08198839  0.87579969  0.76986291  0.62311651 -0.14944646\n",
      "   0.9067675   0.86015029  0.67561784  0.47358234  0.96546849  0.94096487\n",
      "   0.62154879  0.68899697  0.44856255  0.23718925  0.82862809  0.94201437\n",
      "   0.46792104  0.40149879  0.30379843  0.66418988  0.91647297  0.87953911\n",
      "   0.99471734  0.84992327  0.9808913 ]]\n",
      "Cost is [1.84292689]\n",
      "Iteration:  9\n",
      "W = [[-7.7502355   0.08198839  0.87579969  0.76986291  0.62311651 -0.14944646\n",
      "   0.9067675   0.86015029  0.67561784  0.47358234  0.96546849  0.94096487\n",
      "   0.62154879  0.68899697  0.44856255  0.23718925  0.82862809  0.94201437\n",
      "   0.46792104  0.40149879  0.30379843  0.66418988  0.91647297  0.87953911\n",
      "   0.99471734  0.84992327  0.9808913 ]]\n",
      "Cost is [1.84292689]\n",
      "Iteration:  10\n",
      "W = [[-7.7502355   0.08198839  0.87579969  0.76986291  0.62311651 -0.14944646\n",
      "   0.9067675   0.86015029  0.67561784  0.47358234  0.96546849  0.94096487\n",
      "   0.62154879  0.68899697  0.44856255  0.23718925  0.82862809  0.94201437\n",
      "   0.46792104  0.40149879  0.30379843  0.66418988  0.91647297  0.87953911\n",
      "   0.99471734  0.84992327  0.9808913 ]]\n",
      "Cost is [1.84292689]\n",
      "Iteration:  11\n",
      "W = [[-7.7502355   0.08198839  0.87579969  0.76986291  0.62311651 -0.14944646\n",
      "   0.9067675   0.86015029  0.67561784  0.47358234  0.96546849  0.94096487\n",
      "   0.62154879  0.68899697  0.44856255  0.23718925  0.82862809  0.94201437\n",
      "   0.46792104  0.40149879  0.30379843  0.66418988  0.91647297  0.87953911\n",
      "   0.99471734  0.84992327  0.9808913 ]]\n",
      "Cost is [1.84292689]\n",
      "Iteration:  12\n",
      "W = [[-7.7502355   0.08198839  0.87579969  0.76986291  0.62311651 -0.14944646\n",
      "   0.9067675   0.86015029  0.67561784  0.47358234  0.96546849  0.94096487\n",
      "   0.62154879  0.68899697  0.44856255  0.23718925  0.82862809  0.94201437\n",
      "   0.46792104  0.40149879  0.30379843  0.66418988  0.91647297  0.87953911\n",
      "   0.99471734  0.84992327  0.9808913 ]]\n",
      "Cost is [1.84292689]\n",
      "Iteration:  13\n",
      "W = [[-7.7502355   0.08198839  0.87579969  0.76986291  0.62311651 -0.14944646\n",
      "   0.9067675   0.86015029  0.67561784  0.47358234  0.96546849  0.94096487\n",
      "   0.62154879  0.68899697  0.44856255  0.23718925  0.82862809  0.94201437\n",
      "   0.46792104  0.40149879  0.30379843  0.66418988  0.91647297  0.87953911\n",
      "   0.99471734  0.84992327  0.9808913 ]]\n",
      "Cost is [1.84292689]\n",
      "Iteration:  14\n",
      "W = [[-7.7502355   0.08198839  0.87579969  0.76986291  0.62311651 -0.14944646\n",
      "   0.9067675   0.86015029  0.67561784  0.47358234  0.96546849  0.94096487\n",
      "   0.62154879  0.68899697  0.44856255  0.23718925  0.82862809  0.94201437\n",
      "   0.46792104  0.40149879  0.30379843  0.66418988  0.91647297  0.87953911\n",
      "   0.99471734  0.84992327  0.9808913 ]]\n",
      "Cost is [1.84292689]\n",
      "Iteration:  15\n",
      "W = [[-7.7502355   0.08198839  0.87579969  0.76986291  0.62311651 -0.14944646\n",
      "   0.9067675   0.86015029  0.67561784  0.47358234  0.96546849  0.94096487\n",
      "   0.62154879  0.68899697  0.44856255  0.23718925  0.82862809  0.94201437\n",
      "   0.46792104  0.40149879  0.30379843  0.66418988  0.91647297  0.87953911\n",
      "   0.99471734  0.84992327  0.9808913 ]]\n",
      "Cost is [1.84292689]\n",
      "Iteration:  16\n",
      "W = [[-7.7502355   0.08198839  0.87579969  0.76986291  0.62311651 -0.14944646\n",
      "   0.9067675   0.86015029  0.67561784  0.47358234  0.96546849  0.94096487\n",
      "   0.62154879  0.68899697  0.44856255  0.23718925  0.82862809  0.94201437\n",
      "   0.46792104  0.40149879  0.30379843  0.66418988  0.91647297  0.87953911\n",
      "   0.99471734  0.84992327  0.9808913 ]]\n",
      "Cost is [1.84292689]\n",
      "Iteration:  17\n",
      "W = [[-7.7502355   0.08198839  0.87579969  0.76986291  0.62311651 -0.14944646\n",
      "   0.9067675   0.86015029  0.67561784  0.47358234  0.96546849  0.94096487\n",
      "   0.62154879  0.68899697  0.44856255  0.23718925  0.82862809  0.94201437\n",
      "   0.46792104  0.40149879  0.30379843  0.66418988  0.91647297  0.87953911\n",
      "   0.99471734  0.84992327  0.9808913 ]]\n",
      "Cost is [1.84292689]\n",
      "Iteration:  18\n",
      "W = [[-7.7502355   0.08198839  0.87579969  0.76986291  0.62311651 -0.14944646\n",
      "   0.9067675   0.86015029  0.67561784  0.47358234  0.96546849  0.94096487\n",
      "   0.62154879  0.68899697  0.44856255  0.23718925  0.82862809  0.94201437\n",
      "   0.46792104  0.40149879  0.30379843  0.66418988  0.91647297  0.87953911\n",
      "   0.99471734  0.84992327  0.9808913 ]]\n",
      "Cost is [1.84292689]\n",
      "Iteration:  19\n",
      "W = [[-7.7502355   0.08198839  0.87579969  0.76986291  0.62311651 -0.14944646\n",
      "   0.9067675   0.86015029  0.67561784  0.47358234  0.96546849  0.94096487\n",
      "   0.62154879  0.68899697  0.44856255  0.23718925  0.82862809  0.94201437\n",
      "   0.46792104  0.40149879  0.30379843  0.66418988  0.91647297  0.87953911\n",
      "   0.99471734  0.84992327  0.9808913 ]]\n",
      "Cost is [1.84292689]\n",
      "Iteration:  20\n",
      "W = [[-7.7502355   0.08198839  0.87579969  0.76986291  0.62311651 -0.14944646\n",
      "   0.9067675   0.86015029  0.67561784  0.47358234  0.96546849  0.94096487\n",
      "   0.62154879  0.68899697  0.44856255  0.23718925  0.82862809  0.94201437\n",
      "   0.46792104  0.40149879  0.30379843  0.66418988  0.91647297  0.87953911\n",
      "   0.99471734  0.84992327  0.9808913 ]]\n",
      "Cost is [1.84292689]\n",
      "Iteration:  21\n",
      "W = [[-7.7502355   0.08198839  0.87579969  0.76986291  0.62311651 -0.14944646\n",
      "   0.9067675   0.86015029  0.67561784  0.47358234  0.96546849  0.94096487\n",
      "   0.62154879  0.68899697  0.44856255  0.23718925  0.82862809  0.94201437\n",
      "   0.46792104  0.40149879  0.30379843  0.66418988  0.91647297  0.87953911\n",
      "   0.99471734  0.84992327  0.9808913 ]]\n",
      "Cost is [1.84292689]\n",
      "Iteration:  22\n",
      "W = [[-7.7502355   0.08198839  0.87579969  0.76986291  0.62311651 -0.14944646\n",
      "   0.9067675   0.86015029  0.67561784  0.47358234  0.96546849  0.94096487\n",
      "   0.62154879  0.68899697  0.44856255  0.23718925  0.82862809  0.94201437\n",
      "   0.46792104  0.40149879  0.30379843  0.66418988  0.91647297  0.87953911\n",
      "   0.99471734  0.84992327  0.9808913 ]]\n",
      "Cost is [1.84292689]\n",
      "Iteration:  23\n",
      "W = [[-7.7502355   0.08198839  0.87579969  0.76986291  0.62311651 -0.14944646\n",
      "   0.9067675   0.86015029  0.67561784  0.47358234  0.96546849  0.94096487\n",
      "   0.62154879  0.68899697  0.44856255  0.23718925  0.82862809  0.94201437\n",
      "   0.46792104  0.40149879  0.30379843  0.66418988  0.91647297  0.87953911\n",
      "   0.99471734  0.84992327  0.9808913 ]]\n",
      "Cost is [1.84292689]\n",
      "Iteration:  24\n",
      "W = [[-7.7502355   0.08198839  0.87579969  0.76986291  0.62311651 -0.14944646\n",
      "   0.9067675   0.86015029  0.67561784  0.47358234  0.96546849  0.94096487\n",
      "   0.62154879  0.68899697  0.44856255  0.23718925  0.82862809  0.94201437\n",
      "   0.46792104  0.40149879  0.30379843  0.66418988  0.91647297  0.87953911\n",
      "   0.99471734  0.84992327  0.9808913 ]]\n",
      "Cost is [1.84292689]\n",
      "Iteration:  25\n",
      "W = [[-7.7502355   0.08198839  0.87579969  0.76986291  0.62311651 -0.14944646\n",
      "   0.9067675   0.86015029  0.67561784  0.47358234  0.96546849  0.94096487\n",
      "   0.62154879  0.68899697  0.44856255  0.23718925  0.82862809  0.94201437\n",
      "   0.46792104  0.40149879  0.30379843  0.66418988  0.91647297  0.87953911\n",
      "   0.99471734  0.84992327  0.9808913 ]]\n",
      "Cost is [1.84292689]\n",
      "Iteration:  26\n",
      "W = [[-7.7502355   0.08198839  0.87579969  0.76986291  0.62311651 -0.14944646\n",
      "   0.9067675   0.86015029  0.67561784  0.47358234  0.96546849  0.94096487\n",
      "   0.62154879  0.68899697  0.44856255  0.23718925  0.82862809  0.94201437\n",
      "   0.46792104  0.40149879  0.30379843  0.66418988  0.91647297  0.87953911\n",
      "   0.99471734  0.84992327  0.9808913 ]]\n",
      "Cost is [1.84292689]\n",
      "Iteration:  27\n",
      "W = [[-7.7502355   0.08198839  0.87579969  0.76986291  0.62311651 -0.14944646\n",
      "   0.9067675   0.86015029  0.67561784  0.47358234  0.96546849  0.94096487\n",
      "   0.62154879  0.68899697  0.44856255  0.23718925  0.82862809  0.94201437\n",
      "   0.46792104  0.40149879  0.30379843  0.66418988  0.91647297  0.87953911\n",
      "   0.99471734  0.84992327  0.9808913 ]]\n",
      "Cost is [1.84292689]\n",
      "Iteration:  28\n",
      "W = [[-7.7502355   0.08198839  0.87579969  0.76986291  0.62311651 -0.14944646\n",
      "   0.9067675   0.86015029  0.67561784  0.47358234  0.96546849  0.94096487\n",
      "   0.62154879  0.68899697  0.44856255  0.23718925  0.82862809  0.94201437\n",
      "   0.46792104  0.40149879  0.30379843  0.66418988  0.91647297  0.87953911\n",
      "   0.99471734  0.84992327  0.9808913 ]]\n",
      "Cost is [1.84292689]\n",
      "Iteration:  29\n",
      "W = [[-7.7502355   0.08198839  0.87579969  0.76986291  0.62311651 -0.14944646\n",
      "   0.9067675   0.86015029  0.67561784  0.47358234  0.96546849  0.94096487\n",
      "   0.62154879  0.68899697  0.44856255  0.23718925  0.82862809  0.94201437\n",
      "   0.46792104  0.40149879  0.30379843  0.66418988  0.91647297  0.87953911\n",
      "   0.99471734  0.84992327  0.9808913 ]]\n",
      "Cost is [1.84292689]\n",
      "Iteration:  30\n",
      "W = [[-7.7502355   0.08198839  0.87579969  0.76986291  0.62311651 -0.14944646\n",
      "   0.9067675   0.86015029  0.67561784  0.47358234  0.96546849  0.94096487\n",
      "   0.62154879  0.68899697  0.44856255  0.23718925  0.82862809  0.94201437\n",
      "   0.46792104  0.40149879  0.30379843  0.66418988  0.91647297  0.87953911\n",
      "   0.99471734  0.84992327  0.9808913 ]]\n",
      "Cost is [1.84292689]\n",
      "Iteration:  31\n",
      "W = [[-7.7502355   0.08198839  0.87579969  0.76986291  0.62311651 -0.14944646\n",
      "   0.9067675   0.86015029  0.67561784  0.47358234  0.96546849  0.94096487\n",
      "   0.62154879  0.68899697  0.44856255  0.23718925  0.82862809  0.94201437\n",
      "   0.46792104  0.40149879  0.30379843  0.66418988  0.91647297  0.87953911\n",
      "   0.99471734  0.84992327  0.9808913 ]]\n",
      "Cost is [1.84292689]\n",
      "Iteration:  32\n",
      "W = [[-7.7502355   0.08198839  0.87579969  0.76986291  0.62311651 -0.14944646\n",
      "   0.9067675   0.86015029  0.67561784  0.47358234  0.96546849  0.94096487\n",
      "   0.62154879  0.68899697  0.44856255  0.23718925  0.82862809  0.94201437\n",
      "   0.46792104  0.40149879  0.30379843  0.66418988  0.91647297  0.87953911\n",
      "   0.99471734  0.84992327  0.9808913 ]]\n",
      "Cost is [1.84292689]\n",
      "Iteration:  33\n",
      "W = [[-7.7502355   0.08198839  0.87579969  0.76986291  0.62311651 -0.14944646\n",
      "   0.9067675   0.86015029  0.67561784  0.47358234  0.96546849  0.94096487\n",
      "   0.62154879  0.68899697  0.44856255  0.23718925  0.82862809  0.94201437\n",
      "   0.46792104  0.40149879  0.30379843  0.66418988  0.91647297  0.87953911\n",
      "   0.99471734  0.84992327  0.9808913 ]]\n",
      "Cost is [1.84292689]\n",
      "Iteration:  34\n",
      "W = [[-7.7502355   0.08198839  0.87579969  0.76986291  0.62311651 -0.14944646\n",
      "   0.9067675   0.86015029  0.67561784  0.47358234  0.96546849  0.94096487\n",
      "   0.62154879  0.68899697  0.44856255  0.23718925  0.82862809  0.94201437\n",
      "   0.46792104  0.40149879  0.30379843  0.66418988  0.91647297  0.87953911\n",
      "   0.99471734  0.84992327  0.9808913 ]]\n",
      "Cost is [1.84292689]\n",
      "Iteration:  35\n",
      "W = [[-7.7502355   0.08198839  0.87579969  0.76986291  0.62311651 -0.14944646\n",
      "   0.9067675   0.86015029  0.67561784  0.47358234  0.96546849  0.94096487\n",
      "   0.62154879  0.68899697  0.44856255  0.23718925  0.82862809  0.94201437\n",
      "   0.46792104  0.40149879  0.30379843  0.66418988  0.91647297  0.87953911\n",
      "   0.99471734  0.84992327  0.9808913 ]]\n",
      "Cost is [1.84292689]\n",
      "Iteration:  36\n",
      "W = [[-7.7502355   0.08198839  0.87579969  0.76986291  0.62311651 -0.14944646\n",
      "   0.9067675   0.86015029  0.67561784  0.47358234  0.96546849  0.94096487\n",
      "   0.62154879  0.68899697  0.44856255  0.23718925  0.82862809  0.94201437\n",
      "   0.46792104  0.40149879  0.30379843  0.66418988  0.91647297  0.87953911\n",
      "   0.99471734  0.84992327  0.9808913 ]]\n",
      "Cost is [1.84292689]\n",
      "Iteration:  37\n",
      "W = [[-7.7502355   0.08198839  0.87579969  0.76986291  0.62311651 -0.14944646\n",
      "   0.9067675   0.86015029  0.67561784  0.47358234  0.96546849  0.94096487\n",
      "   0.62154879  0.68899697  0.44856255  0.23718925  0.82862809  0.94201437\n",
      "   0.46792104  0.40149879  0.30379843  0.66418988  0.91647297  0.87953911\n",
      "   0.99471734  0.84992327  0.9808913 ]]\n",
      "Cost is [1.84292689]\n",
      "Iteration:  38\n",
      "W = [[-7.7502355   0.08198839  0.87579969  0.76986291  0.62311651 -0.14944646\n",
      "   0.9067675   0.86015029  0.67561784  0.47358234  0.96546849  0.94096487\n",
      "   0.62154879  0.68899697  0.44856255  0.23718925  0.82862809  0.94201437\n",
      "   0.46792104  0.40149879  0.30379843  0.66418988  0.91647297  0.87953911\n",
      "   0.99471734  0.84992327  0.9808913 ]]\n",
      "Cost is [1.84292689]\n",
      "Iteration:  39\n",
      "W = [[-7.7502355   0.08198839  0.87579969  0.76986291  0.62311651 -0.14944646\n",
      "   0.9067675   0.86015029  0.67561784  0.47358234  0.96546849  0.94096487\n",
      "   0.62154879  0.68899697  0.44856255  0.23718925  0.82862809  0.94201437\n",
      "   0.46792104  0.40149879  0.30379843  0.66418988  0.91647297  0.87953911\n",
      "   0.99471734  0.84992327  0.9808913 ]]\n",
      "Cost is [1.84292689]\n",
      "Iteration:  40\n",
      "W = [[-7.7502355   0.08198839  0.87579969  0.76986291  0.62311651 -0.14944646\n",
      "   0.9067675   0.86015029  0.67561784  0.47358234  0.96546849  0.94096487\n",
      "   0.62154879  0.68899697  0.44856255  0.23718925  0.82862809  0.94201437\n",
      "   0.46792104  0.40149879  0.30379843  0.66418988  0.91647297  0.87953911\n",
      "   0.99471734  0.84992327  0.9808913 ]]\n",
      "Cost is [1.84292689]\n",
      "Iteration:  41\n",
      "W = [[-7.7502355   0.08198839  0.87579969  0.76986291  0.62311651 -0.14944646\n",
      "   0.9067675   0.86015029  0.67561784  0.47358234  0.96546849  0.94096487\n",
      "   0.62154879  0.68899697  0.44856255  0.23718925  0.82862809  0.94201437\n",
      "   0.46792104  0.40149879  0.30379843  0.66418988  0.91647297  0.87953911\n",
      "   0.99471734  0.84992327  0.9808913 ]]\n",
      "Cost is [1.84292689]\n",
      "Iteration:  42\n",
      "W = [[-7.7502355   0.08198839  0.87579969  0.76986291  0.62311651 -0.14944646\n",
      "   0.9067675   0.86015029  0.67561784  0.47358234  0.96546849  0.94096487\n",
      "   0.62154879  0.68899697  0.44856255  0.23718925  0.82862809  0.94201437\n",
      "   0.46792104  0.40149879  0.30379843  0.66418988  0.91647297  0.87953911\n",
      "   0.99471734  0.84992327  0.9808913 ]]\n",
      "Cost is [1.84292689]\n",
      "Iteration:  43\n",
      "W = [[-7.7502355   0.08198839  0.87579969  0.76986291  0.62311651 -0.14944646\n",
      "   0.9067675   0.86015029  0.67561784  0.47358234  0.96546849  0.94096487\n",
      "   0.62154879  0.68899697  0.44856255  0.23718925  0.82862809  0.94201437\n",
      "   0.46792104  0.40149879  0.30379843  0.66418988  0.91647297  0.87953911\n",
      "   0.99471734  0.84992327  0.9808913 ]]\n",
      "Cost is [1.84292689]\n",
      "Iteration:  44\n",
      "W = [[-7.7502355   0.08198839  0.87579969  0.76986291  0.62311651 -0.14944646\n",
      "   0.9067675   0.86015029  0.67561784  0.47358234  0.96546849  0.94096487\n",
      "   0.62154879  0.68899697  0.44856255  0.23718925  0.82862809  0.94201437\n",
      "   0.46792104  0.40149879  0.30379843  0.66418988  0.91647297  0.87953911\n",
      "   0.99471734  0.84992327  0.9808913 ]]\n",
      "Cost is [1.84292689]\n",
      "Iteration:  45\n",
      "W = [[-7.7502355   0.08198839  0.87579969  0.76986291  0.62311651 -0.14944646\n",
      "   0.9067675   0.86015029  0.67561784  0.47358234  0.96546849  0.94096487\n",
      "   0.62154879  0.68899697  0.44856255  0.23718925  0.82862809  0.94201437\n",
      "   0.46792104  0.40149879  0.30379843  0.66418988  0.91647297  0.87953911\n",
      "   0.99471734  0.84992327  0.9808913 ]]\n",
      "Cost is [1.84292689]\n",
      "Iteration:  46\n",
      "W = [[-7.7502355   0.08198839  0.87579969  0.76986291  0.62311651 -0.14944646\n",
      "   0.9067675   0.86015029  0.67561784  0.47358234  0.96546849  0.94096487\n",
      "   0.62154879  0.68899697  0.44856255  0.23718925  0.82862809  0.94201437\n",
      "   0.46792104  0.40149879  0.30379843  0.66418988  0.91647297  0.87953911\n",
      "   0.99471734  0.84992327  0.9808913 ]]\n",
      "Cost is [1.84292689]\n",
      "Iteration:  47\n",
      "W = [[-7.7502355   0.08198839  0.87579969  0.76986291  0.62311651 -0.14944646\n",
      "   0.9067675   0.86015029  0.67561784  0.47358234  0.96546849  0.94096487\n",
      "   0.62154879  0.68899697  0.44856255  0.23718925  0.82862809  0.94201437\n",
      "   0.46792104  0.40149879  0.30379843  0.66418988  0.91647297  0.87953911\n",
      "   0.99471734  0.84992327  0.9808913 ]]\n",
      "Cost is [1.84292689]\n",
      "Iteration:  48\n",
      "W = [[-7.7502355   0.08198839  0.87579969  0.76986291  0.62311651 -0.14944646\n",
      "   0.9067675   0.86015029  0.67561784  0.47358234  0.96546849  0.94096487\n",
      "   0.62154879  0.68899697  0.44856255  0.23718925  0.82862809  0.94201437\n",
      "   0.46792104  0.40149879  0.30379843  0.66418988  0.91647297  0.87953911\n",
      "   0.99471734  0.84992327  0.9808913 ]]\n",
      "Cost is [1.84292689]\n",
      "Iteration:  49\n",
      "W = [[-7.7502355   0.08198839  0.87579969  0.76986291  0.62311651 -0.14944646\n",
      "   0.9067675   0.86015029  0.67561784  0.47358234  0.96546849  0.94096487\n",
      "   0.62154879  0.68899697  0.44856255  0.23718925  0.82862809  0.94201437\n",
      "   0.46792104  0.40149879  0.30379843  0.66418988  0.91647297  0.87953911\n",
      "   0.99471734  0.84992327  0.9808913 ]]\n",
      "Cost is [1.84292689]\n",
      "Iteration:  50\n",
      "W = [[-7.7502355   0.08198839  0.87579969  0.76986291  0.62311651 -0.14944646\n",
      "   0.9067675   0.86015029  0.67561784  0.47358234  0.96546849  0.94096487\n",
      "   0.62154879  0.68899697  0.44856255  0.23718925  0.82862809  0.94201437\n",
      "   0.46792104  0.40149879  0.30379843  0.66418988  0.91647297  0.87953911\n",
      "   0.99471734  0.84992327  0.9808913 ]]\n",
      "Cost is [1.84292689]\n",
      "Iteration:  51\n",
      "W = [[-7.7502355   0.08198839  0.87579969  0.76986291  0.62311651 -0.14944646\n",
      "   0.9067675   0.86015029  0.67561784  0.47358234  0.96546849  0.94096487\n",
      "   0.62154879  0.68899697  0.44856255  0.23718925  0.82862809  0.94201437\n",
      "   0.46792104  0.40149879  0.30379843  0.66418988  0.91647297  0.87953911\n",
      "   0.99471734  0.84992327  0.9808913 ]]\n",
      "Cost is [1.84292689]\n",
      "Iteration:  52\n",
      "W = [[-7.7502355   0.08198839  0.87579969  0.76986291  0.62311651 -0.14944646\n",
      "   0.9067675   0.86015029  0.67561784  0.47358234  0.96546849  0.94096487\n",
      "   0.62154879  0.68899697  0.44856255  0.23718925  0.82862809  0.94201437\n",
      "   0.46792104  0.40149879  0.30379843  0.66418988  0.91647297  0.87953911\n",
      "   0.99471734  0.84992327  0.9808913 ]]\n",
      "Cost is [1.84292689]\n",
      "Iteration:  53\n",
      "W = [[-7.7502355   0.08198839  0.87579969  0.76986291  0.62311651 -0.14944646\n",
      "   0.9067675   0.86015029  0.67561784  0.47358234  0.96546849  0.94096487\n",
      "   0.62154879  0.68899697  0.44856255  0.23718925  0.82862809  0.94201437\n",
      "   0.46792104  0.40149879  0.30379843  0.66418988  0.91647297  0.87953911\n",
      "   0.99471734  0.84992327  0.9808913 ]]\n",
      "Cost is [1.84292689]\n",
      "Iteration:  54\n",
      "W = [[-7.7502355   0.08198839  0.87579969  0.76986291  0.62311651 -0.14944646\n",
      "   0.9067675   0.86015029  0.67561784  0.47358234  0.96546849  0.94096487\n",
      "   0.62154879  0.68899697  0.44856255  0.23718925  0.82862809  0.94201437\n",
      "   0.46792104  0.40149879  0.30379843  0.66418988  0.91647297  0.87953911\n",
      "   0.99471734  0.84992327  0.9808913 ]]\n",
      "Cost is [1.84292689]\n",
      "Iteration:  55\n",
      "W = [[-7.7502355   0.08198839  0.87579969  0.76986291  0.62311651 -0.14944646\n",
      "   0.9067675   0.86015029  0.67561784  0.47358234  0.96546849  0.94096487\n",
      "   0.62154879  0.68899697  0.44856255  0.23718925  0.82862809  0.94201437\n",
      "   0.46792104  0.40149879  0.30379843  0.66418988  0.91647297  0.87953911\n",
      "   0.99471734  0.84992327  0.9808913 ]]\n",
      "Cost is [1.84292689]\n",
      "Iteration:  56\n",
      "W = [[-7.7502355   0.08198839  0.87579969  0.76986291  0.62311651 -0.14944646\n",
      "   0.9067675   0.86015029  0.67561784  0.47358234  0.96546849  0.94096487\n",
      "   0.62154879  0.68899697  0.44856255  0.23718925  0.82862809  0.94201437\n",
      "   0.46792104  0.40149879  0.30379843  0.66418988  0.91647297  0.87953911\n",
      "   0.99471734  0.84992327  0.9808913 ]]\n",
      "Cost is [1.84292689]\n",
      "Iteration:  57\n",
      "W = [[-7.7502355   0.08198839  0.87579969  0.76986291  0.62311651 -0.14944646\n",
      "   0.9067675   0.86015029  0.67561784  0.47358234  0.96546849  0.94096487\n",
      "   0.62154879  0.68899697  0.44856255  0.23718925  0.82862809  0.94201437\n",
      "   0.46792104  0.40149879  0.30379843  0.66418988  0.91647297  0.87953911\n",
      "   0.99471734  0.84992327  0.9808913 ]]\n",
      "Cost is [1.84292689]\n",
      "Iteration:  58\n",
      "W = [[-7.7502355   0.08198839  0.87579969  0.76986291  0.62311651 -0.14944646\n",
      "   0.9067675   0.86015029  0.67561784  0.47358234  0.96546849  0.94096487\n",
      "   0.62154879  0.68899697  0.44856255  0.23718925  0.82862809  0.94201437\n",
      "   0.46792104  0.40149879  0.30379843  0.66418988  0.91647297  0.87953911\n",
      "   0.99471734  0.84992327  0.9808913 ]]\n",
      "Cost is [1.84292689]\n",
      "Iteration:  59\n",
      "W = [[-7.7502355   0.08198839  0.87579969  0.76986291  0.62311651 -0.14944646\n",
      "   0.9067675   0.86015029  0.67561784  0.47358234  0.96546849  0.94096487\n",
      "   0.62154879  0.68899697  0.44856255  0.23718925  0.82862809  0.94201437\n",
      "   0.46792104  0.40149879  0.30379843  0.66418988  0.91647297  0.87953911\n",
      "   0.99471734  0.84992327  0.9808913 ]]\n",
      "Cost is [1.84292689]\n",
      "Iteration:  60\n",
      "W = [[-7.7502355   0.08198839  0.87579969  0.76986291  0.62311651 -0.14944646\n",
      "   0.9067675   0.86015029  0.67561784  0.47358234  0.96546849  0.94096487\n",
      "   0.62154879  0.68899697  0.44856255  0.23718925  0.82862809  0.94201437\n",
      "   0.46792104  0.40149879  0.30379843  0.66418988  0.91647297  0.87953911\n",
      "   0.99471734  0.84992327  0.9808913 ]]\n",
      "Cost is [1.84292689]\n",
      "Iteration:  61\n",
      "W = [[-7.7502355   0.08198839  0.87579969  0.76986291  0.62311651 -0.14944646\n",
      "   0.9067675   0.86015029  0.67561784  0.47358234  0.96546849  0.94096487\n",
      "   0.62154879  0.68899697  0.44856255  0.23718925  0.82862809  0.94201437\n",
      "   0.46792104  0.40149879  0.30379843  0.66418988  0.91647297  0.87953911\n",
      "   0.99471734  0.84992327  0.9808913 ]]\n",
      "Cost is [1.84292689]\n",
      "Iteration:  62\n",
      "W = [[-7.7502355   0.08198839  0.87579969  0.76986291  0.62311651 -0.14944646\n",
      "   0.9067675   0.86015029  0.67561784  0.47358234  0.96546849  0.94096487\n",
      "   0.62154879  0.68899697  0.44856255  0.23718925  0.82862809  0.94201437\n",
      "   0.46792104  0.40149879  0.30379843  0.66418988  0.91647297  0.87953911\n",
      "   0.99471734  0.84992327  0.9808913 ]]\n",
      "Cost is [1.84292689]\n",
      "Iteration:  63\n",
      "W = [[-7.7502355   0.08198839  0.87579969  0.76986291  0.62311651 -0.14944646\n",
      "   0.9067675   0.86015029  0.67561784  0.47358234  0.96546849  0.94096487\n",
      "   0.62154879  0.68899697  0.44856255  0.23718925  0.82862809  0.94201437\n",
      "   0.46792104  0.40149879  0.30379843  0.66418988  0.91647297  0.87953911\n",
      "   0.99471734  0.84992327  0.9808913 ]]\n",
      "Cost is [1.84292689]\n",
      "Iteration:  64\n",
      "W = [[-7.7502355   0.08198839  0.87579969  0.76986291  0.62311651 -0.14944646\n",
      "   0.9067675   0.86015029  0.67561784  0.47358234  0.96546849  0.94096487\n",
      "   0.62154879  0.68899697  0.44856255  0.23718925  0.82862809  0.94201437\n",
      "   0.46792104  0.40149879  0.30379843  0.66418988  0.91647297  0.87953911\n",
      "   0.99471734  0.84992327  0.9808913 ]]\n",
      "Cost is [1.84292689]\n",
      "Iteration:  65\n",
      "W = [[-7.7502355   0.08198839  0.87579969  0.76986291  0.62311651 -0.14944646\n",
      "   0.9067675   0.86015029  0.67561784  0.47358234  0.96546849  0.94096487\n",
      "   0.62154879  0.68899697  0.44856255  0.23718925  0.82862809  0.94201437\n",
      "   0.46792104  0.40149879  0.30379843  0.66418988  0.91647297  0.87953911\n",
      "   0.99471734  0.84992327  0.9808913 ]]\n",
      "Cost is [1.84292689]\n",
      "Iteration:  66\n",
      "W = [[-7.7502355   0.08198839  0.87579969  0.76986291  0.62311651 -0.14944646\n",
      "   0.9067675   0.86015029  0.67561784  0.47358234  0.96546849  0.94096487\n",
      "   0.62154879  0.68899697  0.44856255  0.23718925  0.82862809  0.94201437\n",
      "   0.46792104  0.40149879  0.30379843  0.66418988  0.91647297  0.87953911\n",
      "   0.99471734  0.84992327  0.9808913 ]]\n",
      "Cost is [1.84292689]\n",
      "Iteration:  67\n",
      "W = [[-7.7502355   0.08198839  0.87579969  0.76986291  0.62311651 -0.14944646\n",
      "   0.9067675   0.86015029  0.67561784  0.47358234  0.96546849  0.94096487\n",
      "   0.62154879  0.68899697  0.44856255  0.23718925  0.82862809  0.94201437\n",
      "   0.46792104  0.40149879  0.30379843  0.66418988  0.91647297  0.87953911\n",
      "   0.99471734  0.84992327  0.9808913 ]]\n",
      "Cost is [1.84292689]\n",
      "Iteration:  68\n",
      "W = [[-7.7502355   0.08198839  0.87579969  0.76986291  0.62311651 -0.14944646\n",
      "   0.9067675   0.86015029  0.67561784  0.47358234  0.96546849  0.94096487\n",
      "   0.62154879  0.68899697  0.44856255  0.23718925  0.82862809  0.94201437\n",
      "   0.46792104  0.40149879  0.30379843  0.66418988  0.91647297  0.87953911\n",
      "   0.99471734  0.84992327  0.9808913 ]]\n",
      "Cost is [1.84292689]\n",
      "Iteration:  69\n",
      "W = [[-7.7502355   0.08198839  0.87579969  0.76986291  0.62311651 -0.14944646\n",
      "   0.9067675   0.86015029  0.67561784  0.47358234  0.96546849  0.94096487\n",
      "   0.62154879  0.68899697  0.44856255  0.23718925  0.82862809  0.94201437\n",
      "   0.46792104  0.40149879  0.30379843  0.66418988  0.91647297  0.87953911\n",
      "   0.99471734  0.84992327  0.9808913 ]]\n",
      "Cost is [1.84292689]\n",
      "Iteration:  70\n",
      "W = [[-7.7502355   0.08198839  0.87579969  0.76986291  0.62311651 -0.14944646\n",
      "   0.9067675   0.86015029  0.67561784  0.47358234  0.96546849  0.94096487\n",
      "   0.62154879  0.68899697  0.44856255  0.23718925  0.82862809  0.94201437\n",
      "   0.46792104  0.40149879  0.30379843  0.66418988  0.91647297  0.87953911\n",
      "   0.99471734  0.84992327  0.9808913 ]]\n",
      "Cost is [1.84292689]\n",
      "Iteration:  71\n",
      "W = [[-7.7502355   0.08198839  0.87579969  0.76986291  0.62311651 -0.14944646\n",
      "   0.9067675   0.86015029  0.67561784  0.47358234  0.96546849  0.94096487\n",
      "   0.62154879  0.68899697  0.44856255  0.23718925  0.82862809  0.94201437\n",
      "   0.46792104  0.40149879  0.30379843  0.66418988  0.91647297  0.87953911\n",
      "   0.99471734  0.84992327  0.9808913 ]]\n",
      "Cost is [1.84292689]\n",
      "Iteration:  72\n",
      "W = [[-7.7502355   0.08198839  0.87579969  0.76986291  0.62311651 -0.14944646\n",
      "   0.9067675   0.86015029  0.67561784  0.47358234  0.96546849  0.94096487\n",
      "   0.62154879  0.68899697  0.44856255  0.23718925  0.82862809  0.94201437\n",
      "   0.46792104  0.40149879  0.30379843  0.66418988  0.91647297  0.87953911\n",
      "   0.99471734  0.84992327  0.9808913 ]]\n",
      "Cost is [1.84292689]\n",
      "Iteration:  73\n",
      "W = [[-7.7502355   0.08198839  0.87579969  0.76986291  0.62311651 -0.14944646\n",
      "   0.9067675   0.86015029  0.67561784  0.47358234  0.96546849  0.94096487\n",
      "   0.62154879  0.68899697  0.44856255  0.23718925  0.82862809  0.94201437\n",
      "   0.46792104  0.40149879  0.30379843  0.66418988  0.91647297  0.87953911\n",
      "   0.99471734  0.84992327  0.9808913 ]]\n",
      "Cost is [1.84292689]\n",
      "Iteration:  74\n",
      "W = [[-7.7502355   0.08198839  0.87579969  0.76986291  0.62311651 -0.14944646\n",
      "   0.9067675   0.86015029  0.67561784  0.47358234  0.96546849  0.94096487\n",
      "   0.62154879  0.68899697  0.44856255  0.23718925  0.82862809  0.94201437\n",
      "   0.46792104  0.40149879  0.30379843  0.66418988  0.91647297  0.87953911\n",
      "   0.99471734  0.84992327  0.9808913 ]]\n",
      "Cost is [1.84292689]\n",
      "Iteration:  75\n",
      "W = [[-7.7502355   0.08198839  0.87579969  0.76986291  0.62311651 -0.14944646\n",
      "   0.9067675   0.86015029  0.67561784  0.47358234  0.96546849  0.94096487\n",
      "   0.62154879  0.68899697  0.44856255  0.23718925  0.82862809  0.94201437\n",
      "   0.46792104  0.40149879  0.30379843  0.66418988  0.91647297  0.87953911\n",
      "   0.99471734  0.84992327  0.9808913 ]]\n",
      "Cost is [1.84292689]\n",
      "Iteration:  76\n",
      "W = [[-7.7502355   0.08198839  0.87579969  0.76986291  0.62311651 -0.14944646\n",
      "   0.9067675   0.86015029  0.67561784  0.47358234  0.96546849  0.94096487\n",
      "   0.62154879  0.68899697  0.44856255  0.23718925  0.82862809  0.94201437\n",
      "   0.46792104  0.40149879  0.30379843  0.66418988  0.91647297  0.87953911\n",
      "   0.99471734  0.84992327  0.9808913 ]]\n",
      "Cost is [1.84292689]\n",
      "Iteration:  77\n",
      "W = [[-7.7502355   0.08198839  0.87579969  0.76986291  0.62311651 -0.14944646\n",
      "   0.9067675   0.86015029  0.67561784  0.47358234  0.96546849  0.94096487\n",
      "   0.62154879  0.68899697  0.44856255  0.23718925  0.82862809  0.94201437\n",
      "   0.46792104  0.40149879  0.30379843  0.66418988  0.91647297  0.87953911\n",
      "   0.99471734  0.84992327  0.9808913 ]]\n",
      "Cost is [1.84292689]\n",
      "Iteration:  78\n",
      "W = [[-7.7502355   0.08198839  0.87579969  0.76986291  0.62311651 -0.14944646\n",
      "   0.9067675   0.86015029  0.67561784  0.47358234  0.96546849  0.94096487\n",
      "   0.62154879  0.68899697  0.44856255  0.23718925  0.82862809  0.94201437\n",
      "   0.46792104  0.40149879  0.30379843  0.66418988  0.91647297  0.87953911\n",
      "   0.99471734  0.84992327  0.9808913 ]]\n",
      "Cost is [1.84292689]\n",
      "Iteration:  79\n",
      "W = [[-7.7502355   0.08198839  0.87579969  0.76986291  0.62311651 -0.14944646\n",
      "   0.9067675   0.86015029  0.67561784  0.47358234  0.96546849  0.94096487\n",
      "   0.62154879  0.68899697  0.44856255  0.23718925  0.82862809  0.94201437\n",
      "   0.46792104  0.40149879  0.30379843  0.66418988  0.91647297  0.87953911\n",
      "   0.99471734  0.84992327  0.9808913 ]]\n",
      "Cost is [1.84292689]\n",
      "Iteration:  80\n",
      "W = [[-7.7502355   0.08198839  0.87579969  0.76986291  0.62311651 -0.14944646\n",
      "   0.9067675   0.86015029  0.67561784  0.47358234  0.96546849  0.94096487\n",
      "   0.62154879  0.68899697  0.44856255  0.23718925  0.82862809  0.94201437\n",
      "   0.46792104  0.40149879  0.30379843  0.66418988  0.91647297  0.87953911\n",
      "   0.99471734  0.84992327  0.9808913 ]]\n",
      "Cost is [1.84292689]\n",
      "Iteration:  81\n",
      "W = [[-7.7502355   0.08198839  0.87579969  0.76986291  0.62311651 -0.14944646\n",
      "   0.9067675   0.86015029  0.67561784  0.47358234  0.96546849  0.94096487\n",
      "   0.62154879  0.68899697  0.44856255  0.23718925  0.82862809  0.94201437\n",
      "   0.46792104  0.40149879  0.30379843  0.66418988  0.91647297  0.87953911\n",
      "   0.99471734  0.84992327  0.9808913 ]]\n",
      "Cost is [1.84292689]\n",
      "Iteration:  82\n",
      "W = [[-7.7502355   0.08198839  0.87579969  0.76986291  0.62311651 -0.14944646\n",
      "   0.9067675   0.86015029  0.67561784  0.47358234  0.96546849  0.94096487\n",
      "   0.62154879  0.68899697  0.44856255  0.23718925  0.82862809  0.94201437\n",
      "   0.46792104  0.40149879  0.30379843  0.66418988  0.91647297  0.87953911\n",
      "   0.99471734  0.84992327  0.9808913 ]]\n",
      "Cost is [1.84292689]\n",
      "Iteration:  83\n",
      "W = [[-7.7502355   0.08198839  0.87579969  0.76986291  0.62311651 -0.14944646\n",
      "   0.9067675   0.86015029  0.67561784  0.47358234  0.96546849  0.94096487\n",
      "   0.62154879  0.68899697  0.44856255  0.23718925  0.82862809  0.94201437\n",
      "   0.46792104  0.40149879  0.30379843  0.66418988  0.91647297  0.87953911\n",
      "   0.99471734  0.84992327  0.9808913 ]]\n",
      "Cost is [1.84292689]\n",
      "Iteration:  84\n",
      "W = [[-7.7502355   0.08198839  0.87579969  0.76986291  0.62311651 -0.14944646\n",
      "   0.9067675   0.86015029  0.67561784  0.47358234  0.96546849  0.94096487\n",
      "   0.62154879  0.68899697  0.44856255  0.23718925  0.82862809  0.94201437\n",
      "   0.46792104  0.40149879  0.30379843  0.66418988  0.91647297  0.87953911\n",
      "   0.99471734  0.84992327  0.9808913 ]]\n",
      "Cost is [1.84292689]\n",
      "Iteration:  85\n",
      "W = [[-7.7502355   0.08198839  0.87579969  0.76986291  0.62311651 -0.14944646\n",
      "   0.9067675   0.86015029  0.67561784  0.47358234  0.96546849  0.94096487\n",
      "   0.62154879  0.68899697  0.44856255  0.23718925  0.82862809  0.94201437\n",
      "   0.46792104  0.40149879  0.30379843  0.66418988  0.91647297  0.87953911\n",
      "   0.99471734  0.84992327  0.9808913 ]]\n",
      "Cost is [1.84292689]\n",
      "Iteration:  86\n",
      "W = [[-7.7502355   0.08198839  0.87579969  0.76986291  0.62311651 -0.14944646\n",
      "   0.9067675   0.86015029  0.67561784  0.47358234  0.96546849  0.94096487\n",
      "   0.62154879  0.68899697  0.44856255  0.23718925  0.82862809  0.94201437\n",
      "   0.46792104  0.40149879  0.30379843  0.66418988  0.91647297  0.87953911\n",
      "   0.99471734  0.84992327  0.9808913 ]]\n",
      "Cost is [1.84292689]\n",
      "Iteration:  87\n",
      "W = [[-7.7502355   0.08198839  0.87579969  0.76986291  0.62311651 -0.14944646\n",
      "   0.9067675   0.86015029  0.67561784  0.47358234  0.96546849  0.94096487\n",
      "   0.62154879  0.68899697  0.44856255  0.23718925  0.82862809  0.94201437\n",
      "   0.46792104  0.40149879  0.30379843  0.66418988  0.91647297  0.87953911\n",
      "   0.99471734  0.84992327  0.9808913 ]]\n",
      "Cost is [1.84292689]\n",
      "Iteration:  88\n",
      "W = [[-7.7502355   0.08198839  0.87579969  0.76986291  0.62311651 -0.14944646\n",
      "   0.9067675   0.86015029  0.67561784  0.47358234  0.96546849  0.94096487\n",
      "   0.62154879  0.68899697  0.44856255  0.23718925  0.82862809  0.94201437\n",
      "   0.46792104  0.40149879  0.30379843  0.66418988  0.91647297  0.87953911\n",
      "   0.99471734  0.84992327  0.9808913 ]]\n",
      "Cost is [1.84292689]\n",
      "Iteration:  89\n",
      "W = [[-7.7502355   0.08198839  0.87579969  0.76986291  0.62311651 -0.14944646\n",
      "   0.9067675   0.86015029  0.67561784  0.47358234  0.96546849  0.94096487\n",
      "   0.62154879  0.68899697  0.44856255  0.23718925  0.82862809  0.94201437\n",
      "   0.46792104  0.40149879  0.30379843  0.66418988  0.91647297  0.87953911\n",
      "   0.99471734  0.84992327  0.9808913 ]]\n",
      "Cost is [1.84292689]\n",
      "Iteration:  90\n",
      "W = [[-7.7502355   0.08198839  0.87579969  0.76986291  0.62311651 -0.14944646\n",
      "   0.9067675   0.86015029  0.67561784  0.47358234  0.96546849  0.94096487\n",
      "   0.62154879  0.68899697  0.44856255  0.23718925  0.82862809  0.94201437\n",
      "   0.46792104  0.40149879  0.30379843  0.66418988  0.91647297  0.87953911\n",
      "   0.99471734  0.84992327  0.9808913 ]]\n",
      "Cost is [1.84292689]\n",
      "Iteration:  91\n",
      "W = [[-7.7502355   0.08198839  0.87579969  0.76986291  0.62311651 -0.14944646\n",
      "   0.9067675   0.86015029  0.67561784  0.47358234  0.96546849  0.94096487\n",
      "   0.62154879  0.68899697  0.44856255  0.23718925  0.82862809  0.94201437\n",
      "   0.46792104  0.40149879  0.30379843  0.66418988  0.91647297  0.87953911\n",
      "   0.99471734  0.84992327  0.9808913 ]]\n",
      "Cost is [1.84292689]\n",
      "Iteration:  92\n",
      "W = [[-7.7502355   0.08198839  0.87579969  0.76986291  0.62311651 -0.14944646\n",
      "   0.9067675   0.86015029  0.67561784  0.47358234  0.96546849  0.94096487\n",
      "   0.62154879  0.68899697  0.44856255  0.23718925  0.82862809  0.94201437\n",
      "   0.46792104  0.40149879  0.30379843  0.66418988  0.91647297  0.87953911\n",
      "   0.99471734  0.84992327  0.9808913 ]]\n",
      "Cost is [1.84292689]\n",
      "Iteration:  93\n",
      "W = [[-7.7502355   0.08198839  0.87579969  0.76986291  0.62311651 -0.14944646\n",
      "   0.9067675   0.86015029  0.67561784  0.47358234  0.96546849  0.94096487\n",
      "   0.62154879  0.68899697  0.44856255  0.23718925  0.82862809  0.94201437\n",
      "   0.46792104  0.40149879  0.30379843  0.66418988  0.91647297  0.87953911\n",
      "   0.99471734  0.84992327  0.9808913 ]]\n",
      "Cost is [1.84292689]\n",
      "Iteration:  94\n",
      "W = [[-7.7502355   0.08198839  0.87579969  0.76986291  0.62311651 -0.14944646\n",
      "   0.9067675   0.86015029  0.67561784  0.47358234  0.96546849  0.94096487\n",
      "   0.62154879  0.68899697  0.44856255  0.23718925  0.82862809  0.94201437\n",
      "   0.46792104  0.40149879  0.30379843  0.66418988  0.91647297  0.87953911\n",
      "   0.99471734  0.84992327  0.9808913 ]]\n",
      "Cost is [1.84292689]\n",
      "Iteration:  95\n",
      "W = [[-7.7502355   0.08198839  0.87579969  0.76986291  0.62311651 -0.14944646\n",
      "   0.9067675   0.86015029  0.67561784  0.47358234  0.96546849  0.94096487\n",
      "   0.62154879  0.68899697  0.44856255  0.23718925  0.82862809  0.94201437\n",
      "   0.46792104  0.40149879  0.30379843  0.66418988  0.91647297  0.87953911\n",
      "   0.99471734  0.84992327  0.9808913 ]]\n",
      "Cost is [1.84292689]\n",
      "Iteration:  96\n",
      "W = [[-7.7502355   0.08198839  0.87579969  0.76986291  0.62311651 -0.14944646\n",
      "   0.9067675   0.86015029  0.67561784  0.47358234  0.96546849  0.94096487\n",
      "   0.62154879  0.68899697  0.44856255  0.23718925  0.82862809  0.94201437\n",
      "   0.46792104  0.40149879  0.30379843  0.66418988  0.91647297  0.87953911\n",
      "   0.99471734  0.84992327  0.9808913 ]]\n",
      "Cost is [1.84292689]\n",
      "Iteration:  97\n",
      "W = [[-7.7502355   0.08198839  0.87579969  0.76986291  0.62311651 -0.14944646\n",
      "   0.9067675   0.86015029  0.67561784  0.47358234  0.96546849  0.94096487\n",
      "   0.62154879  0.68899697  0.44856255  0.23718925  0.82862809  0.94201437\n",
      "   0.46792104  0.40149879  0.30379843  0.66418988  0.91647297  0.87953911\n",
      "   0.99471734  0.84992327  0.9808913 ]]\n",
      "Cost is [1.84292689]\n",
      "Iteration:  98\n",
      "W = [[-7.7502355   0.08198839  0.87579969  0.76986291  0.62311651 -0.14944646\n",
      "   0.9067675   0.86015029  0.67561784  0.47358234  0.96546849  0.94096487\n",
      "   0.62154879  0.68899697  0.44856255  0.23718925  0.82862809  0.94201437\n",
      "   0.46792104  0.40149879  0.30379843  0.66418988  0.91647297  0.87953911\n",
      "   0.99471734  0.84992327  0.9808913 ]]\n",
      "Cost is [1.84292689]\n",
      "Iteration:  99\n",
      "W = [[-7.7502355   0.08198839  0.87579969  0.76986291  0.62311651 -0.14944646\n",
      "   0.9067675   0.86015029  0.67561784  0.47358234  0.96546849  0.94096487\n",
      "   0.62154879  0.68899697  0.44856255  0.23718925  0.82862809  0.94201437\n",
      "   0.46792104  0.40149879  0.30379843  0.66418988  0.91647297  0.87953911\n",
      "   0.99471734  0.84992327  0.9808913 ]]\n",
      "Cost is [1.84292689]\n",
      "Final W: [[-7.75667727  0.08131993  0.87570782  0.76969322  0.62283943 -0.15027975\n",
      "   0.90669694  0.86004552  0.67537491  0.47319015  0.96544298  0.94091964\n",
      "   0.62127175  0.68876743  0.44815826  0.23662347  0.8285019   0.94197234\n",
      "   0.46753121  0.40106014  0.30328495  0.6639415   0.91641135  0.87944754\n",
      "   0.99471347  0.84980961  0.98087702]]\n"
     ]
    }
   ],
   "source": [
    "# Write code for gradient descent here.\n",
    "\n",
    "# This is the same as every other gradient descent algorithm you've\n",
    "# written.  Use a loop to run each iteration of gradient descent.\n",
    "# Each iteration will make a pass over the training set X_train, y_train,\n",
    "# running forward prop & backward prop to get the output of the network\n",
    "# and the gradient matrix dL_dW.  We need to compute the AVERAGE of all\n",
    "# these dL_dW matrices (we compute one for each training example), so you\n",
    "# will need to add up the matrices as you calculate them into a \"sum\" or\n",
    "# \"total\" matrix variable.  Then once the loop over the training set is done,\n",
    "# you compute:\n",
    "# W -= alpha * (1/m) * (sum of all the dL_dW matrices).\n",
    "\n",
    "# Note that you can comment out the W = np.ones((1, INPUT_SIZE)) line \n",
    "# and the J_sequence = [] line\n",
    "# if you run gradient descent but then want to run the cell again to \n",
    "# run additional training iterations.  Commenting out those initialization lines\n",
    "# just lets you pick up where you left off when you ran the cell before.\n",
    "\n",
    "W = np.ones((1, INPUT_SIZE))  \n",
    "ALPHA = .10 # change this\n",
    "\n",
    "J_sequence = []\n",
    "total = 0\n",
    "m = X_train.shape[0]\n",
    "\n",
    "#print(\"x train is\", X_train)\n",
    "#print(\"y train is\", y_train)\n",
    "\n",
    "for ctr in range(0, 100):\n",
    "    print(\"Iteration: \", ctr)\n",
    "    print(\"W =\", W)\n",
    "    print(\"Cost is\", compute_cost(X_train, y_train, W))\n",
    "    \n",
    "    for i in range(X_train.shape[0]):   # m\n",
    "        in1, z1, a1 = forward_prop(W, X_train[i])\n",
    "        dL_dW = backward_prop(W, y_train[0], in1, z1, a1)\n",
    "        #print(\"Gradients\", dL_dW)\n",
    "        total += dL_dW\n",
    "        \n",
    "        \n",
    "    # \"batch\" gradient descent:\n",
    "    #D = [0,0]\n",
    "    #for i in range(X_train.shape[0]):   # m\n",
    "    #    z1, a1 = forward_prop(W, X_train[i])\n",
    "    #    dL_dw0, dL_dw1 = backward_prop(W, X_train[i], y_train[i], z1, a1)\n",
    "    #    #print(\"Gradients\", dL_dw0, dL_dw1)\n",
    "    #    #W[0][0] -= ALPHA * dL_dw0\n",
    "    #    #W[0][1] -= ALPHA * dL_dw1\n",
    "    #    D[0] += dL_dw0\n",
    "    #   D[1] += dL_dw1   \n",
    "    #W[0][0] -= ALPHA * D[0]\n",
    "    #W[0][1] -= ALPHA * D[1]\n",
    "                              \n",
    "    J_sequence.append(compute_cost(X_train, y_train, W))\n",
    "    \n",
    "W -= ALPHA * (1/m) * total\n",
    "    \n",
    "    \n",
    "print(\"Final W:\", W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "18d536d2-b691-459a-a42f-200ca575475e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's plot the cost as a function of number of iterations of the\n",
    "# gradient descent algorithm.\n",
    "\n",
    "plt.scatter(range(0, len(J_sequence)), J_sequence)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "2529afca-0a10-41ee-aaed-79079e2df21f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Test/train accuracy\n",
    "\n",
    "def compute_accuracy(X, y):\n",
    "    # YOUR CODE HERE - compute accuracy for this data set of examples (X) and their\n",
    "    # corresponding outputs (y).  X is a matrix and y is a vector, but X has the same\n",
    "    # number of rows as y has entries.  Hint:\n",
    "    pass\n",
    "\n",
    "# Try to get training accuracy and test accuracy to at least\n",
    "# 95% and 90% respectively.  (for Spanish/English)\n",
    "\n",
    "print(compute_accuracy(X_train, y_train))\n",
    "print(compute_accuracy(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615d9397-df8a-459e-8639-52a58e508e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Real life demo!\n",
    "\n",
    "def classify_language(sentence):\n",
    "    stripped_sentence = unidecode(\"\".join(c for c in sentence.lower() if c.isalpha()))\n",
    "    #print(\"Removed punctuation/accents/spaces:\", stripped_sentence)\n",
    "    d = {}\n",
    "    for letter in ALL_LETTERS:\n",
    "        d['freq_' + letter] = [stripped_sentence.lower().count(letter)/len(stripped_sentence)]\n",
    "    x_demo = pd.DataFrame(d).to_numpy()[0]\n",
    "    answer = make_prediction(x_demo, W)\n",
    "    if answer == 1:\n",
    "        return LANG1\n",
    "    else:\n",
    "        return LANG0\n",
    "    \n",
    "# Demo first five examples of each language:\n",
    "for i in range(0, 5):\n",
    "    sentence = all_data[all_data['lang'] == LANG0]['text'].iloc[i]\n",
    "    print(\"\\\"\" + sentence + \"\\\" classified as \" + classify_language(sentence))\n",
    "    sentence = all_data[all_data['lang'] == LANG1]['text'].iloc[i]\n",
    "    print(\"\\\"\" + sentence + \"\\\" classified as \" + classify_language(sentence))\n",
    "    \n",
    "print()\n",
    "sentence = input(\"Type in a sentence in \" + LANG0 + \" or \" + LANG1)\n",
    "print(\"You typed\", sentence)\n",
    "\n",
    "print(\"Predicted language:\", classify_language(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323f76d4-d051-4ec0-8105-cd8c0a17baad",
   "metadata": {},
   "source": [
    "## Part B: 2 Layer Neural Network Refresher\n",
    "\n",
    "Recall that in a 2 layer network (1 hidden layer), we have the following variables.  \n",
    "\n",
    "- `HIDDEN_LAYER_SIZE`: number of nodes in the hidden layer.  This can be set to essentially any integer greater than zero.\n",
    "  Larger numbers will generally increase performance, but will take longer to train.\n",
    "- $x$: values of our features that are in the input to the network.  Here, this\n",
    "  is a vector of 26 letter frequencies.  In the code, this is called `x`.\n",
    "- $in^{[1]}$: A copy of $x$, but with a 1 appended at the front.  In code, this is called `in1`.\n",
    "- $W^{[1]}$: Weight matrix. Dimensions are HIDDEN_LAYER_SIZE rows by 27 columns, because the first layer has 27 input \n",
    "  features and HIDDEN_LAYER_SIZE outputs.  In code, this is `W1`.\n",
    "- $z^{[1]}$: Computed as the matrix product of $W1$ and the inputs $in^{[1]}$.  This will be a vector with the\n",
    "  same number of entries as HIDDEN_LAYER_SIZE, because the first layer is now computing this many outputs (rather than\n",
    "  just 1).  In code, this is `z1`.\n",
    "- $a$ or $a^{[1]}$: Computed by the sigmoid function (activation function) applied to $z^{[1]}$.  Same size as $a^{[1]}$\n",
    "  In code, this is `a` or `a1`.\n",
    "- $in^{[2]}$: A copy of $a^{[1]}$, but with a 1 appended at the front.  In code, this is called `in2`.\n",
    "- $W^{[2]}$: Weight matrix. Dimensions are 1 rows by HIDDEN_LAYER_SIZE+1 columns, because the second layer has \n",
    "  HIDDEN_LAYER_SIZE inputs (one each from layer 1, plus the bias input) and 1 output.  In code, this is `W2`.\n",
    "- $z^{[2]}$: Computed as the matrix product of $W2$ and the inputs $in^{[2]}$.  Because $W2$ is only one row, this is \n",
    "  essentially the dot product of $W2$ (treated as a vector) and $in^{[2]}$.  In code, this is `z2`.\n",
    "- $a^{[2]}$: Computed by the sigmoid function (activation function) applied to $z^{[2]}$.  Final output of the network.\n",
    "  In code, this is `a2`.\n",
    "\n",
    "Recall that a 2-layer neural network's output is the result of the last activation function, which here is $a^{[2]}$.\n",
    "  \n",
    "You must write the following functions:\n",
    "\n",
    "- `forward_prop`: Run forward propagation.  You can use the single-layer NN code as a guide.  Returns six \n",
    "  variables: in1, a1, z1, in2, z2, a2.\n",
    "- `make_prediction`: Use the single-layer code as a guide.\n",
    "- `compute_cost`: Compute the cost (average loss) over the training set.\n",
    "- `backward_prop`: Run backpropagation.\n",
    "\n",
    "The values you are computing for backpropagation are the entries in the TWO matrices $\\dfrac{\\partial L}{\\partial w^{[2]}_j}$\n",
    "and $\\dfrac{\\partial L}{\\partial w^{[1]}_{k,j}}$.\n",
    "\n",
    "We are using the notation $w^{[2]}_j$ to stand for the $j$'th entry in the weight matrix $W^{[2]}$.  Because $W^{[2]}$ has only one row,\n",
    "$w^{[2]}_j$ is stored in the variable `W2[0][j]` and the corresponding partial derivative will be in `dL_dW2[0][j]`.\n",
    "\n",
    "We are using the notation $w^{[1]}_{k,j}$ to stand for the $k$'th row and $j$'th column in the weight matrix $W^{[1]}$.  \n",
    "Unlike $W^{[2]}$, $W^{[1]}$ has multiple rows, so \n",
    "$w^{[1]}_{k,j}$ is stored in the variable `W1[k][j]` and the corresponding partial derivative will\n",
    "be in `dL_dW1[k][j]`.\n",
    "\n",
    "Formulas: \n",
    "\n",
    "Derivatives of output layer going backwards to hidden layer (do this first).  These entries will go in `dL_dW2`.\n",
    "\n",
    "$\\dfrac{\\partial L}{\\partial w^{[2]}_j} = \\dfrac{\\partial L}{\\partial a^{[2]}} \\cdot \\dfrac{\\partial a^{[2]}}{\\partial z^{[2]}}\n",
    "  \\cdot \\dfrac{\\partial z^{[2]}}{\\partial w^{[2]}_j}$\n",
    "  \n",
    "this is all basically the same as the single layer network, with one change:\n",
    "\n",
    "$\\dfrac{\\partial z^{[2]}}{\\partial w^{[2]}_j} = in^{[2]}_j$  (we just added the [2] subscript)\n",
    "\n",
    "So you will return a single row, HIDDEN_LAYER_SIZE column matrix with entries using the formula above (three multiplications).\n",
    "You can do this with a for loop where you loop over the `in2` variable.\n",
    "Note that the first two terms are the same for all entries in the matrix, and so don't need to be computed inside the loop.\n",
    "\n",
    "The derivatives for going from the hidden layer to the input layer are slightly more complicated.\n",
    "\n",
    "$\\dfrac{\\partial L}{\\partial w^{[1]}_{k,j}} = \\dfrac{\\partial L}{\\partial a^{[2]}} \\cdot \\dfrac{\\partial a^{[2]}}{\\partial z^{[2]}}\n",
    "  \\cdot \\dfrac{\\partial z^{[2]}}{\\partial a^{[1]}_k} \\cdot \\dfrac{\\partial a^{[1]}_k}{\\partial z^{[1]}_k}\n",
    "  \\cdot \\dfrac{\\partial z^{[1]}_k}{\\partial w^{[1]}_{k,j}}$. \n",
    "  \n",
    "The first two terms are the same as the first two terms above.\n",
    "\n",
    "The third, fourth, and fifth terms are defined as:\n",
    "\n",
    "$\\dfrac{\\partial z^{[2]}}{\\partial a^{[1]}_k} = w^{[2]}_{k+1}$ = `W2[0][k+1]` (remember W2 is only one row) (the k+1 comes\n",
    "from the fact that when we moved from $a^{[1]}$ to $in^{[2]}$, we added a bias input as $in^{[2]}_0$, so all the subscripts \n",
    "for $in^{[2]}$ are shifted up by 1.\n",
    "\n",
    "$\\dfrac{\\partial a^{[1]}_k}{\\partial z^{[1]}_k} = \\sigma(z^{[1]}_k)(1-\\sigma(z^{[1]}_k)$.\n",
    "\n",
    "$\\dfrac{\\partial z^{[1]}_k}{\\partial w^{[1]}_{k,j}} = in^{[1]}_j$\n",
    "\n",
    "Computing this whole thing:\n",
    "\n",
    "Use nested loops, one for $j$ and one for $k$.  Which one is inner and which one is outer doesn't matter too much.\n",
    "The $k$ variable counts *rows* of `W1`/`dL_dW1` and $j$ counts the columns.  So you can calculate the upper bounds\n",
    "of these nested loops in a few ways, based on the dimensions of `W1` or the lengths of `in1` and `in2`:\n",
    "\n",
    "  In other words, remember the dimensions of `W1` (and therefore `dL_dW1`) are HIDDEN_LAYER_SIZE rows by 27 columns,\n",
    "  So $k$ (row counter) should range from 0 to `HIDDEN_LAYER_SIZE` which should also be `len(in2)-1`.\n",
    "  And $j$ (column counter) should range from 0 to `len(in1)` which should be 27.\n",
    "  \n",
    "Inside the nested loop, you should compute the last three terms of the five above.  You can re-use the first\n",
    "two terms from the `dL_dW2` computation, assuming you saved them in variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150065bb-9f1e-41bf-9b54-c63ea09b60d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 layer neural network (one hidden layer)\n",
    "\n",
    "INPUT_SIZE = 27\n",
    "HIDDEN_LAYER_SIZE = 5\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1/(1 + np.exp(-z))\n",
    "\n",
    "def compute_z(W, inputs):\n",
    "    '''\n",
    "    Compute the z vector for a neural network.\n",
    "    inputs: inputs to the neural network, vector of variable size.\n",
    "    W: weight matrix for the inputs, matrix of variable size.\n",
    "    '''\n",
    "    return W @ inputs\n",
    "\n",
    "def compute_activation(z_vector):\n",
    "    return sigmoid(z_vector)\n",
    "\n",
    "def deriv_activation(z):\n",
    "    return sigmoid(z) * (1 - sigmoid(z))\n",
    "\n",
    "def deriv_cross_entropy_loss(a, y):\n",
    "    return (a - y)/(a * (1 - a))\n",
    "\n",
    "def compute_cross_entropy_loss(a, y):\n",
    "    if y == 0:\n",
    "        return -np.log(1-a)\n",
    "    else:\n",
    "        return -np.log(a)\n",
    "\n",
    "def augment_vector(v):\n",
    "    return np.insert(v, 0, 1)\n",
    "\n",
    "def forward_prop(W1, W2, x):\n",
    "    '''\n",
    "    Run the forward propagation algorithm.\n",
    "    x: input to the neural network, vector of size INPUT_SIZE - 1\n",
    "    W1: weight matrix, matrix of size (HIDDEN_LAYER_SIZE, INPUT_SIZE)\n",
    "    W2: weight matrix, matrix of size (1, HIDDEN_LAYER_SIZE + 1)\n",
    "    returns: in1, z1, a1, in2, z2, a2\n",
    "    '''\n",
    "    DEBUG = False\n",
    "    \n",
    "    # we assume x DOES NOT have a 1 in front.\n",
    "    assert len(x) == INPUT_SIZE - 1\n",
    "    \n",
    "    # check dimensions of W1\n",
    "    assert W1.shape == (HIDDEN_LAYER_SIZE, INPUT_SIZE)\n",
    "    \n",
    "    # check dimensions of W2\n",
    "    assert W2.shape == (1, HIDDEN_LAYER_SIZE + 1)\n",
    "    \n",
    "    # YOUR CODE HERE: use single layer NN code as a guide.\n",
    "    \n",
    "    return in1, z1, a1, in2, z2, a2\n",
    "    \n",
    "def compute_output(x, W1, W2):\n",
    "    '''\n",
    "    Returns the output of the neural network (probability of x being in the 1 class).\n",
    "    x: input to the neural network, vector of size INPUT_SIZE\n",
    "    W1: weight matrix, matrix of size (HIDDEN_LAYER_SIZE, INPUT_SIZE)\n",
    "    W2: weight matrix, matrix of size (1, HIDDEN_LAYER_SIZE + 1)\n",
    "    '''\n",
    "    \n",
    "    # we assume x DOES NOT have a 1 in front.\n",
    "    assert len(x) == INPUT_SIZE - 1\n",
    "    \n",
    "    # check dimensions of W1\n",
    "    assert W1.shape == (HIDDEN_LAYER_SIZE, INPUT_SIZE)\n",
    "    \n",
    "    # check dimensions of W2\n",
    "    assert W2.shape == (1, HIDDEN_LAYER_SIZE + 1)\n",
    "    \n",
    "    in1, z1, a1, in2, z2, a2 = forward_prop(W1, W2, x)\n",
    "    return a2\n",
    "    \n",
    "def make_prediction(x, W1, W2):\n",
    "    '''\n",
    "    Returns the classification of x.\n",
    "    x: input to the neural network, vector of size INPUT_SIZE\n",
    "    W1: weight matrix, matrix of size (HIDDEN_LAYER_SIZE, INPUT_SIZE)\n",
    "    W2: weight matrix, matrix of size (1, HIDDEN_LAYER_SIZE + 1)\n",
    "    '''\n",
    "    \n",
    "    # we assume x DOES NOT have a 1 in front.\n",
    "    assert len(x) == INPUT_SIZE - 1\n",
    "    \n",
    "    # check dimensions of W1\n",
    "    assert W1.shape == (HIDDEN_LAYER_SIZE, INPUT_SIZE)\n",
    "    \n",
    "    # check dimensions of W2\n",
    "    assert W2.shape == (1, HIDDEN_LAYER_SIZE + 1)\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "\n",
    "def compute_cost(X_data, y_data, W1, W2):\n",
    "    '''\n",
    "    Compute the cost (average loss) over the training examples in X_data, y_data.\n",
    "    W1: weight matrix, matrix of size (HIDDEN_LAYER_SIZE, INPUT_SIZE)\n",
    "    W2: weight matrix, matrix of size (1, HIDDEN_LAYER_SIZE + 1)\n",
    "    '''\n",
    "    \n",
    "    # check dimensions of W1\n",
    "    assert W1.shape == (HIDDEN_LAYER_SIZE, INPUT_SIZE)\n",
    "    \n",
    "    # check dimensions of W2\n",
    "    assert W2.shape == (1, HIDDEN_LAYER_SIZE + 1)\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "\n",
    "def backward_prop(W1, W2, y, in1, z1, a1, in2, z2, a2):\n",
    "    '''\n",
    "    Run backward propagation.\n",
    "    W1: weight matrix, matrix of size (HIDDEN_LAYER_SIZE, INPUT_SIZE)\n",
    "    W2: weight matrix, matrix of size (1, HIDDEN_LAYER_SIZE + 1)\n",
    "    y: scalar of correct target output, 0 or 1\n",
    "    in1: inputs to the neural network, length INPUT_SIZE\n",
    "    z1: z1 scalar from forward prop of NN\n",
    "    a1: activation vector from forward prop of NN\n",
    "    in2: inputs to the 2nd layer of NN\n",
    "    z2: z2 scalar from forward prop of NN\n",
    "    a2: activation vector from forward prop of NN\n",
    "    Returns: partial derivatives of loss function with\n",
    "        respect to each entry in weight matrix W1 and W2 (same dimensions as W1 and W2)\n",
    "    '''\n",
    "    \n",
    "    # check dimensions of W1\n",
    "    assert W1.shape == (HIDDEN_LAYER_SIZE, INPUT_SIZE)\n",
    "    \n",
    "    # check dimensions of W2\n",
    "    assert W2.shape == (1, HIDDEN_LAYER_SIZE + 1)\n",
    "    \n",
    "    # make derivative matrix of same size as W1, filled with zeros\n",
    "    dL_dW1 = np.full_like(W1, 0)\n",
    "    \n",
    "    # make derivative matrix of same size as W2, filled with zeros\n",
    "    dL_dW2 = np.full_like(W2, 0)\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    return dL_dW1, dL_dW2\n",
    "     \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104007c1-b8f7-440b-a190-95b177986d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity checks for forward_prop (for a hidden layer size of 5)\n",
    "\n",
    "W1_sanity = np.ones((HIDDEN_LAYER_SIZE, INPUT_SIZE))\n",
    "W2_sanity = np.ones((1, HIDDEN_LAYER_SIZE+1))\n",
    "in1, z1, a1, in2, z2, a2 = forward_prop(W1_sanity, W2_sanity, X_train[0])\n",
    "print(in1, \"\\n\", z1, \"\\n\", a1)\n",
    "print(in2, \"\\n\", z2, \"\\n\", a2)\n",
    "\n",
    "# output:\n",
    "#[1.         0.11111111 0.         0.         0.08333333 0.08333333\n",
    "# 0.         0.         0.05555556 0.         0.         0.02777778\n",
    "# 0.         0.05555556 0.08333333 0.13888889 0.         0.\n",
    "# 0.08333333 0.         0.08333333 0.02777778 0.         0.05555556\n",
    "# 0.         0.11111111 0.        ] \n",
    "# [2. 2. 2. 2. 2.] \n",
    "# [0.88079708 0.88079708 0.88079708 0.88079708 0.88079708]\n",
    "#[1.         0.88079708 0.88079708 0.88079708 0.88079708 0.88079708] \n",
    "# [5.40398539] \n",
    "# [0.99552153]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d322e3-1258-491a-a9d8-7803d5af3cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity checks for compute_cost (hidden layer size = 5)\n",
    "\n",
    "compute_cost(X_train, y_train, W1_sanity, W2_sanity)  # should be array([2.70648122])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70884ee9-5408-445b-96c2-7a556dc0b8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity checks for backprop (hidden layer size = 5)\n",
    "\n",
    "print(backward_prop(W1_sanity, W2_sanity, y_train[0], in1, z1, a1, in2, z2, a2))\n",
    "\n",
    "#(array([[0.10452337, 0.01161371, 0.        , 0.        , 0.00871028,\n",
    "#        0.00871028, 0.        , 0.        , 0.00580685, 0.        ,\n",
    "#        0.        , 0.00290343, 0.        , 0.00580685, 0.00871028,\n",
    "#        0.01451714, 0.        , 0.        , 0.00871028, 0.        ,\n",
    "#        0.00871028, 0.00290343, 0.        , 0.00580685, 0.        ,\n",
    "#        0.01161371, 0.        ],\n",
    "#       [0.10452337, 0.01161371, 0.        , 0.        , 0.00871028,\n",
    "#        0.00871028, 0.        , 0.        , 0.00580685, 0.        ,\n",
    "#        0.        , 0.00290343, 0.        , 0.00580685, 0.00871028,\n",
    "#        0.01451714, 0.        , 0.        , 0.00871028, 0.        ,\n",
    "#        0.00871028, 0.00290343, 0.        , 0.00580685, 0.        ,\n",
    "#        0.01161371, 0.        ],\n",
    "#       [0.10452337, 0.01161371, 0.        , 0.        , 0.00871028,\n",
    "#        0.00871028, 0.        , 0.        , 0.00580685, 0.        ,\n",
    "#        0.        , 0.00290343, 0.        , 0.00580685, 0.00871028,\n",
    "#        0.01451714, 0.        , 0.        , 0.00871028, 0.        ,\n",
    "#        0.00871028, 0.00290343, 0.        , 0.00580685, 0.        ,\n",
    "#        0.01161371, 0.        ],\n",
    "#       [0.10452337, 0.01161371, 0.        , 0.        , 0.00871028,\n",
    "#        0.00871028, 0.        , 0.        , 0.00580685, 0.        ,\n",
    "#        0.        , 0.00290343, 0.        , 0.00580685, 0.00871028,\n",
    "#        0.01451714, 0.        , 0.        , 0.00871028, 0.        ,\n",
    "#        0.00871028, 0.00290343, 0.        , 0.00580685, 0.        ,\n",
    "#        0.01161371, 0.        ],\n",
    "#       [0.10452337, 0.01161371, 0.        , 0.        , 0.00871028,\n",
    "#        0.00871028, 0.        , 0.        , 0.00580685, 0.        ,\n",
    "#        0.        , 0.00290343, 0.        , 0.00580685, 0.00871028,\n",
    "#        0.01451714, 0.        , 0.        , 0.00871028, 0.        ,\n",
    "#        0.00871028, 0.00290343, 0.        , 0.00580685, 0.        ,\n",
    "#        0.01161371, 0.        ]]), array([[0.99552153, 0.87685246, 0.87685246, 0.87685246, 0.87685246,\n",
    "#        0.87685246]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d26477e-ef28-4227-9c4d-16e95ae8ec4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write code for gradient descent here.\n",
    "\n",
    "# This is the same as earlier in this project, except you have\n",
    "# two matrix variables, W1 and W2 now, and their corresponding\n",
    "# gradients (derivative variables).\n",
    "\n",
    "# Note that you can comment out the two W1/W2 initialization lines \n",
    "# and the J_sequence initialization line\n",
    "# if you run gradient descent but then want to run the cell again to \n",
    "# run additional training iterations.  Commenting out the W initialization line\n",
    "# just lets you pick up where you left off when you ran the cell before.\n",
    "\n",
    "# THIS WILL TAKE A LONG TIME TO CONVERGE.  WHERE LONG TIME = POSSIBLY 5-10 MINUTES.\n",
    "# I recommend finding a reasonable alpha by starting with a small number of iterations.\n",
    "# Once you have a good alpha, then you can start running more and more iterations\n",
    "# as to not have to sit around for minutes at a time waiting while this runs.\n",
    "# You can also use the commenting-out-the-initialization idea above to see\n",
    "# if you've converged, and if not, just pick up where you left off.\n",
    "\n",
    "W1 = np.random.normal(size=(HIDDEN_LAYER_SIZE, INPUT_SIZE))\n",
    "W2 = np.random.normal(size=(1, HIDDEN_LAYER_SIZE+1))\n",
    "ALPHA = None\n",
    "\n",
    "J_sequence = []\n",
    "    \n",
    "print(\"Final W1 and W2:\", W1, W2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf48547-ae0e-4277-b0c5-b54d8a44946e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot the cost as a function of number of iterations of the\n",
    "# gradient descent algorithm.\n",
    "\n",
    "plt.scatter(range(0, len(J_sequence)), J_sequence)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182a20c3-1abf-4225-a7af-7952058d9320",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test/train accuracy\n",
    "\n",
    "def compute_accuracy(X, y):\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "# Try to get training accuracy and test accuracy to at least\n",
    "# 99% and 91% respectively.  (for spanish/english)\n",
    "\n",
    "print(compute_accuracy(X_train, y_train))\n",
    "print(compute_accuracy(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645de7fd-d976-4d92-a7ec-260867e1a9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Real life demo!\n",
    "\n",
    "def classify_language(sentence):\n",
    "    stripped_sentence = unidecode(\"\".join(c for c in sentence.lower() if c.isalpha()))\n",
    "    #print(\"Removed punctuation/accents/spaces:\", stripped_sentence)\n",
    "    d = {}\n",
    "    for letter in ALL_LETTERS:\n",
    "        d['freq_' + letter] = [stripped_sentence.lower().count(letter)/len(stripped_sentence)]\n",
    "    x_demo = pd.DataFrame(d).to_numpy()[0]\n",
    "    answer = make_prediction(x_demo, W1, W2)\n",
    "    if answer == 1:\n",
    "        return LANG1\n",
    "    else:\n",
    "        return LANG0\n",
    "    \n",
    "# Demo first five examples of each language:\n",
    "for i in range(0, 5):\n",
    "    sentence = all_data[all_data['lang'] == LANG0]['text'].iloc[i]\n",
    "    print(\"\\\"\" + sentence + \"\\\" classified as \" + classify_language(sentence))\n",
    "    sentence = all_data[all_data['lang'] == LANG1]['text'].iloc[i]\n",
    "    print(\"\\\"\" + sentence + \"\\\" classified as \" + classify_language(sentence))\n",
    "    \n",
    "print()\n",
    "sentence = input(\"Type in a sentence in \" + LANG0 + \" or \" + LANG1)\n",
    "print(\"You typed\", sentence)\n",
    "\n",
    "print(\"Predicted language:\", classify_language(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a322ab-f889-4272-90e7-38b2291b1d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FINAL QUESTIONS\n",
    "# answer this with respect to training on Spanish/English\n",
    "\n",
    "# Train/Test accuracy for single-layer network:  (copy from above)\n",
    "# ANSWER:\n",
    "\n",
    "# Train/Test accuracy for two-layer network:  (copy from above)\n",
    "# ANSWER:\n",
    "\n",
    "# Extra credit for testing other languages.  For each other language pair\n",
    "# you test, report training and testing accuracy.  (Up to 3 bonus points).\n",
    "# ANSWERS: (tell me what language pairs you tried)\n",
    "\n",
    "# For up to 2 more bonus points, speculate on why some language pairs are \"harder\"\n",
    "# to learn than others.  Harder = lower testing/training accuracies meaning\n",
    "# they are harder to distinguish from each other.  Refer to the specific\n",
    "# language pairs you ran and the accuracies you came up with to support your\n",
    "# arguments.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
