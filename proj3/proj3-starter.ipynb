{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e56e29b-6e7e-4781-b7fb-906d3e768580",
   "metadata": {},
   "source": [
    "# Project 3: Language detection\n",
    "\n",
    "In this project, you will create a 2-layer neural network by hand to detect\n",
    "what language a sentence is written in.  I have provided you a data set\n",
    "containing sentences in English, Spanish, French, German, Italian, and Portuguese,\n",
    "and your neural network will use the frequency of the letters in the sentence\n",
    "to detect the language.  \n",
    "\n",
    "To make life simple, your network will only process two languages at a time,\n",
    "for example, English-vs-Spanish; we do this so we can create a network that\n",
    "does binary classification rather than multi-category classification.\n",
    "\n",
    "## Features\n",
    "\n",
    "The features we will use in this project are the relative frequencies of the\n",
    "26 letters of the English alphabet in each sentence.  Each of the six languages\n",
    "in our data set uses letters differently; for instance, see here:\n",
    "\n",
    "http://letterfrequency.org/letter-frequency-by-language/\n",
    "\n",
    "For example, while both Spanish and English have \"e\" as their most common letter,\n",
    "the relative popularities of other letters differ in small or large ways.  Take a\n",
    "look at the letter \"t\": in English, it's the 2nd-most-common letter, but in Spanish,\n",
    "it barely makes the top third.  \"h\" is another letter that is used more frequently in English\n",
    "than in Spanish.  The letters \"w\" and \"k\" show the opposite pattern: these letters appear\n",
    "very rarely in Spanish but are common in English.\n",
    "\n",
    "**Caveat**: While all six languages use essentially the same letters, there are extra\n",
    "letters (plus accents) that we will have to handle.  Though linguistically questionable,\n",
    "we will use a Python library to convert any letter not in the English alphabet to the closest\n",
    "English letter.  For instance, in Spanish, \"á\" will be converted to \"a,\" \"ñ\" will be converted\n",
    "to \"n,\" etc.\n",
    "\n",
    "## Data\n",
    "\n",
    "The dataset contains a selection of 200 sentences in each of the six languages (1200 sentences in all).\n",
    "Each sentence is between 20 and 200 characters long.  **There is much more data available, but running this\n",
    "on individual laptops limits our ability to process more data.  Running your code on larger training sets\n",
    "will get you bonus points.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd3f631-4098-4914-ba1f-23148980f2c8",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Let's examine our data set a little."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2226b780-a240-45eb-972b-38c6ce45a6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "\n",
    "!pip install unidecode\n",
    "!pip install scikit-learn\n",
    "from unidecode import unidecode\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d40713-5354-47e9-8b85-5dd050ab26ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at the data\n",
    "\n",
    "all_data = pd.read_csv(\"six-languages.csv\")\n",
    "all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0f1879-3471-4820-a93d-6b7b18b42ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a few sentences from each language (at random).\n",
    "# The languages are denoted by three-letter abbreviations:\n",
    "\n",
    "# deu = German\n",
    "# eng = English\n",
    "# fra = French\n",
    "# ita = Italian\n",
    "# por = Portuguese\n",
    "# spa = Spanish\n",
    "\n",
    "LANGUAGES = ['deu', 'eng', 'fra', 'ita', 'por', 'spa']\n",
    "\n",
    "for lang in LANGUAGES:\n",
    "    display(all_data[all_data['lang'] == lang].sample(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc4344b-c485-497b-9179-fe86f3fdba90",
   "metadata": {},
   "source": [
    "## Data cleaning\n",
    "\n",
    "Here, we will remove punctuation, spaces, convert everything to lowercase, and\n",
    "convert all characters to the 26-letter English alphabet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b2e796-e158-4038-a873-98e215bf46ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all punctuation from the sentences, and convert all characters\n",
    "# to the 26-letter English alphabet.\n",
    "# We do this by first removing all characters from each sentence that \n",
    "\n",
    "ALL_LETTERS = \"abcdefghijklmnopqrstuvwxyz\"\n",
    "\n",
    "# Illustrate with some data.\n",
    "# Print one example from each language before modifying.\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "print(\"Before modifying:\")\n",
    "sample = pd.DataFrame()\n",
    "for lang in LANGUAGES:\n",
    "    sample = pd.concat([sample, all_data[all_data['lang'] == lang].sample(1, random_state=10)])\n",
    "display(sample)\n",
    "    \n",
    "all_data_freq = all_data.copy()\n",
    "\n",
    "# Remove punctuation & spaces, convert to lowercase, convert to 26-letter English alphabet.\n",
    "\n",
    "all_data_freq['text_alpha'] = all_data_freq['text'].map(lambda str: unidecode(\"\".join(c for c in str.lower() if c.isalpha())))\n",
    "print(\"\\nAfter converting:\")\n",
    "sample = pd.DataFrame()\n",
    "for lang in LANGUAGES:\n",
    "    sample = pd.concat([sample, all_data_freq[all_data_freq['lang'] == lang].sample(1, random_state=10)])\n",
    "display(sample)\n",
    "    \n",
    "# Find letter frequencies.\n",
    "\n",
    "for letter in ALL_LETTERS:\n",
    "    all_data_freq['freq_' + letter] = all_data_freq['text_alpha'].map(lambda str: str.lower().count(letter)/len(str))\n",
    "    \n",
    "print(\"With frequencies:\")\n",
    "sample = pd.DataFrame()\n",
    "for lang in LANGUAGES:\n",
    "    sample = pd.concat([sample, all_data_freq[all_data_freq['lang'] == lang].sample(1, random_state=10)])\n",
    "display(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c5f522-f152-4af6-b72f-eaa33585f9bc",
   "metadata": {},
   "source": [
    "## Part A: Single layer neural network\n",
    "\n",
    "Here, you will write code to create a single layer neural network that can distinguish between two\n",
    "languages of your choice (from the six options)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21d9138-84a9-4cce-b97a-5badf3acfaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Part A.\n",
    "\n",
    "# Choose your languages.  Edit the variables below to set one language\n",
    "# to be the \"positive class\" (LANG1) and one to be the \"negative class\" (LANG0).\n",
    "# Use the three-letter abbreviation: deu, eng, fra, ita, por, spa.\n",
    "\n",
    "LANG1 = 'spa'  # positive category\n",
    "LANG0 = 'eng'  # negative category\n",
    "\n",
    "# SANITY CHECKS BELOW ARE GIVEN FOR SPANISH/ENGLISH, but after you make sure those\n",
    "# two languages are working, you can play around with others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2522912c-11b8-4866-a703-0db890bb2647",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I have written some code to produce a data set from the one above with only\n",
    "# our two chosen languages.  It also removes the full sentences (which we don't need),\n",
    "# since we're only training on the \"freq_\" columns.\n",
    "\n",
    "def make_dataset(complete_data, pos_cat, neg_cat):\n",
    "    data = complete_data.copy()\n",
    "    data = data.drop(columns=['text', 'text_alpha'])\n",
    "    data = data[data['lang'].isin([pos_cat, neg_cat])]\n",
    "    data['lang'] = (data['lang'] == pos_cat).astype(int)\n",
    "    return data\n",
    "\n",
    "data = make_dataset(all_data_freq, LANG1, LANG0)\n",
    "display(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab60c839-e90b-47a3-b200-14953a849918",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create testing and training sets.\n",
    "\n",
    "# In the real world, we would shuffle the data to make a random training/testing split,\n",
    "# but here, since we have 200 examples, we're going to use the first half of each\n",
    "# language for training and the last half for testing.\n",
    "\n",
    "# In our data set above, the first half of the examples are one class and the last half\n",
    "# are the other class (although depending on the languages you chose, 0 might be first and 1\n",
    "# second, or the other way around, though it doesn't matter).  \n",
    "\n",
    "# So create a training set of rows 0-99 and 200-299,\n",
    "# and a testing set of rows 100-199 and 300-399\n",
    "# \n",
    "\n",
    "# Make training set:\n",
    "X = data.copy().drop(columns=['lang'])  # get rid of the lang column\n",
    "y = data.copy()['lang']  # keep only the lang column\n",
    "                      \n",
    "X_train = pd.concat([X.iloc[0:100], X.iloc[200:300]]).to_numpy()\n",
    "X_test = pd.concat([X.iloc[100:200], X.iloc[300:400]]).to_numpy()\n",
    "y_train = pd.concat([y.iloc[0:100], y.iloc[200:300]]).to_numpy()\n",
    "y_test = pd.concat([y.iloc[100:200], y.iloc[300:400]]).to_numpy()\n",
    "\n",
    "# Sanity checks.\n",
    "print(X_train.shape)  # should be (200, 26)\n",
    "print(X_test.shape) # should be (200, 26)\n",
    "print(y_train.shape) # should be (200,)\n",
    "print(y_test.shape) # should be (200,)\n",
    "print(np.unique(y_train, return_counts=True)) # count the number of 0's and 1's, should be 100 of each\n",
    "print(np.unique(y_test, return_counts=True)) # count the number of 0's and 1's, should be 100 of each"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0e1002-79eb-4c98-b20c-6f648c90ff2d",
   "metadata": {},
   "source": [
    "## Single Layer Neural Network Refresher\n",
    "\n",
    "Recall that in a single layer network, we have the following variables.  Some variables have their superscripts\n",
    "dropped because in a single layer network, there's only one copy of them.\n",
    "\n",
    "- $x$: values of our features that are in the input to the network.  Here, this\n",
    "  is a vector of 26 letter frequencies.  In the code, this is called `x`.\n",
    "- $in^{[1]}$: A copy of $x$, but with a 1 appended at the front.  In code, this is called `in` or `in1`.\n",
    "- $W$ or $W^{[1]}$: Weight matrix. Dimensions are 1 row by 27 columns, because we have 27 features and 1 output\n",
    "  in our network.  In code, this is `W`.\n",
    "- $z$ or $z^{[1]}$: Computed as the matrix product of $W$ and the inputs $in^{[1]}$.  Because $W$ is only one row, this is \n",
    "  essentially the dot product of $W$ (treated as a vector) and $in^{[1]}$.  In code, this is `z` or `z1`.\n",
    "- $a$ or $a^{[1]}$: Computed by the sigmoid function (activation function) applied to $z^{[1]}$.\n",
    "  In code, this is `a` or `a1`.\n",
    "  \n",
    "Recall that a neural network's output is the result of the last activation function, which here is $a$ or $a^{[1]}$.\n",
    "\n",
    "The loss function for a neural network doing classification is the cross-entropy loss function, which is:\n",
    "\n",
    "$L(\\hat{y}, y) = \\dfrac{\\hat{y} - y}{\\hat{y}  (1 - \\hat{y})}$.\n",
    "\n",
    "Because the predicted value of a neural net, $\\hat{y}$ is the same as the output variable $a$ or $a^{[1]}$ here,\n",
    "we will often see:\n",
    "\n",
    "$L(a, y) = \\dfrac{a - y}{a  (1 - a)}$.\n",
    "\n",
    "The cost function for a neural net is just the average loss over an entire set of training examples:\n",
    "\n",
    "$J(W) = \\displaystyle \\dfrac{1}{m}\\sum_{i=1}^{m} L(\\hat{y}^{(i)}, y^{(i)})$,\n",
    "\n",
    "where $y^{(i)}$ is the $i$'th training example *correct* output (0 or 1) and $\\hat{y}^{(i)} = a^{(i)}$ is the \n",
    "corresponding *predicted* output (a number between 0 and 1) for that same $i$'th training example.\n",
    "\n",
    "The forward propagation code for this neural net is already written for you.  Note that it returns\n",
    "three variables, `in`, `z1`, and `a1`.\n",
    "  \n",
    "You must write the following functions:\n",
    "\n",
    "- `compute_cost`: Compute the cost (average loss) over the training set.\n",
    "- `backward_prop`: Run backpropagation.\n",
    "\n",
    "The values you are computing for backpropagation are the entries in the matrix $\\dfrac{\\partial L}{\\partial w_j}$.\n",
    "We are using the notation $w_j$ to stand for the $j$'th entry in the weight matrix $W$.  Because $W$ has only one row,\n",
    "$w_j$ is stored in the variable `W[0][j]` and the corresponding partial derivative will be in `dL_dW[0][j]`.\n",
    "\n",
    "Formula: $\\dfrac{\\partial L}{\\partial w_j} = \\dfrac{\\partial L}{\\partial a} \\cdot \\dfrac{\\partial a}{\\partial z}\n",
    "  \\cdot \\dfrac{\\partial z}{\\partial w_j}$\n",
    "  \n",
    "where\n",
    "\n",
    "$\\dfrac{\\partial L}{\\partial a} = \\dfrac{a-y}{a(1-a)}$\n",
    "\n",
    "$\\dfrac{\\partial a}{\\partial z} = \\sigma(z)(1-\\sigma(z))$\n",
    "\n",
    "and\n",
    "\n",
    "$\\dfrac{\\partial z}{\\partial w_j} = in_j$\n",
    "\n",
    "So you will return a single row, 27 column matrix with entries using the formula above (three multiplications).\n",
    "You can do this with a for loop.\n",
    "Note that the first two terms are the same for all entries in the matrix, and so don't need to be computed inside the loop.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ae7557-0abb-42cb-8c28-7f9684107123",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single layer neural network\n",
    "\n",
    "INPUT_SIZE = 27\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1/(1 + np.exp(-z))\n",
    "\n",
    "def compute_z(W, inputs):\n",
    "    '''\n",
    "    Compute the z vector for a neural network.\n",
    "    inputs: inputs to the neural network, vector of variable size.\n",
    "    W: weight matrix for the inputs, matrix of variable size.\n",
    "    '''\n",
    "    return W @ inputs\n",
    "\n",
    "def compute_activation(z_vector):\n",
    "    return sigmoid(z_vector)\n",
    "\n",
    "def deriv_activation(z):\n",
    "    return sigmoid(z) * (1 - sigmoid(z))\n",
    "\n",
    "def deriv_cross_entropy_loss(a, y):\n",
    "    return (a - y)/(a * (1 - a))\n",
    "\n",
    "def compute_cross_entropy_loss(a, y):\n",
    "    if y == 0:\n",
    "        return -np.log(1-a)\n",
    "    else:\n",
    "        return -np.log(a)\n",
    "\n",
    "def augment_vector(v):\n",
    "    return np.insert(v, 0, 1)\n",
    "\n",
    "def forward_prop(W, x):\n",
    "    '''\n",
    "    Run the forward propagation algorithm.\n",
    "    x: input to the neural network, vector of size INPUT_SIZE - 1\n",
    "    W: weight matrix, matrix of size (1, INPUT_SIZE)\n",
    "    returns: 2 scalars, z1 and a1\n",
    "    '''\n",
    "    DEBUG = False\n",
    "    \n",
    "    # we assume x DOES NOT have a 1 in front.\n",
    "    assert len(x) == INPUT_SIZE - 1\n",
    "    \n",
    "    # check dimensions of W\n",
    "    assert W.shape == (1, INPUT_SIZE)\n",
    "    \n",
    "    # set inputs\n",
    "    in1 = augment_vector(x)\n",
    "    if DEBUG: print(\"Input to this layer is\", in1)\n",
    "    \n",
    "    # make sure dimensions match inputs and W\n",
    "    assert len(in1) == W.shape[1]\n",
    "\n",
    "    # get W matrix\n",
    "    if DEBUG: print(\"Using W of shape\", W.shape)\n",
    "\n",
    "    # compute z1\n",
    "    z1 = compute_z(W, in1)\n",
    "    if DEBUG: print(\"z is\", z1)\n",
    "\n",
    "    # compute a1\n",
    "    a1 = compute_activation(z1)\n",
    "    if DEBUG: print(\"a1 is\", a1)\n",
    "    \n",
    "    return in1, z1, a1\n",
    "    \n",
    "def compute_output(x, W):\n",
    "    '''\n",
    "    Returns the output of the neural network (probability of x being in the 1 class).\n",
    "    x: input to the neural network, vector of size INPUT_SIZE - 1\n",
    "    W: weight matrix, matrix of size (1, INPUT_SIZE)\n",
    "    '''\n",
    "    \n",
    "    # we assume x DOES NOT have a 1 in front.\n",
    "    assert len(x) == INPUT_SIZE - 1\n",
    "    \n",
    "    # check dimensions of W\n",
    "    assert W.shape == (1, INPUT_SIZE)\n",
    "    \n",
    "    in1, z1, a1 = forward_prop(W, x)\n",
    "    return a1\n",
    "\n",
    "def make_prediction(x, W):\n",
    "    '''\n",
    "    Returns the classification of x.\n",
    "    x: input to the neural network, vector of size INPUT_SIZE - 1\n",
    "    W: weight matrix, matrix of size (1, INPUT_SIZE)\n",
    "    '''\n",
    "    \n",
    "    # we assume x DOES NOT have a 1 in front.\n",
    "    assert len(x) == INPUT_SIZE - 1\n",
    "    \n",
    "    # check dimensions of W\n",
    "    assert W.shape == (1, INPUT_SIZE)\n",
    "    \n",
    "    in1, z1, a1 = forward_prop(W, x)\n",
    "    if a1 >= 0.5:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def compute_cost(X_data, y_data, W):\n",
    "    '''\n",
    "    Compute the cost (average loss) over the training examples in X_data, y_data.\n",
    "    W: weight matrix, matrix of size (1, INPUT_SIZE)\n",
    "    returns: a scalar with the cost\n",
    "    '''\n",
    "    \n",
    "    # check dimensions of W\n",
    "    assert W.shape == (1, INPUT_SIZE)\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "\n",
    "def backward_prop(W, y, in1, z1, a1):\n",
    "    '''\n",
    "    Run backward propagation.\n",
    "    W: weight matrix of size (1, INPUT_SIZE)\n",
    "    y: scalar of correct target output, 0 or 1\n",
    "    in1: inputs to the neural network, length INPUT_SIZE\n",
    "    z1: z1 scalar from forward prop of NN\n",
    "    a1: activation vector from forward prop of NN\n",
    "    Returns: partial derivatives of loss function with\n",
    "       respect to each entry in weight matrix W (same dimensions as W)\n",
    "    '''\n",
    "    \n",
    "    # make sure dimensions match inputs and W\n",
    "    assert len(in1) == W.shape[1]\n",
    "    \n",
    "    # make derivative matrix of same size as W, filled with zeros\n",
    "    dL_dW = np.full_like(W, 0)\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    return dL_dW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d502f34a-8180-494e-88fa-ba76d62bc1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity checks for forward_prop\n",
    "\n",
    "W_sanity = np.ones((1, INPUT_SIZE))\n",
    "in1, z1, a1 = forward_prop(W_sanity, X_train[0])\n",
    "print(in1, \"\\n\", z1, \"\\n\", a1)\n",
    "\n",
    "# output:\n",
    "#[1.         0.11111111 0.         0.         0.08333333 0.08333333\n",
    "# 0.         0.         0.05555556 0.         0.         0.02777778\n",
    "# 0.         0.05555556 0.08333333 0.13888889 0.         0.\n",
    "# 0.08333333 0.         0.08333333 0.02777778 0.         0.05555556\n",
    "# 0.         0.11111111 0.        ] \n",
    "# [2.] \n",
    "# [0.88079708]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8250a99f-c586-4e6e-a562-4b6728857800",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity checks for compute_cost\n",
    "\n",
    "compute_cost(X_train, y_train, W_sanity)  # should be array([1.12692801])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18092cc-5c8e-4ebc-84b7-03cef5ef7f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity checks for backprop\n",
    "\n",
    "print(backward_prop(W_sanity, y_train[0], in1, z1, a1))\n",
    "\n",
    "#[[0.88079708 0.09786634 0.         0.         0.07339976 0.07339976\n",
    "#  0.         0.         0.04893317 0.         0.         0.02446659\n",
    "#  0.         0.04893317 0.07339976 0.12233293 0.         0.\n",
    "#  0.07339976 0.         0.07339976 0.02446659 0.         0.04893317\n",
    "#  0.         0.09786634 0.        ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92bafc3b-775e-48f6-b503-86d567305319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write code for gradient descent here.\n",
    "\n",
    "# This is the same as every other gradient descent algorithm you've\n",
    "# written.  Use a loop to run each iteration of gradient descent.\n",
    "# Each iteration will make a pass over the training set X_train, y_train,\n",
    "# running forward prop & backward prop to get the output of the network\n",
    "# and the gradient matrix dL_dW.  We need to compute the AVERAGE of all\n",
    "# these dL_dW matrices (we compute one for each training example), so you\n",
    "# will need to add up the matrices as you calculate them into a \"sum\" or\n",
    "# \"total\" matrix variable.  Then once the loop over the training set is done,\n",
    "# you compute:\n",
    "# W -= alpha * (1/m) * (sum of all the dL_dW matrices).\n",
    "\n",
    "# Note that you can comment out the W = np.ones((1, INPUT_SIZE)) line \n",
    "# and the J_sequence = [] line\n",
    "# if you run gradient descent but then want to run the cell again to \n",
    "# run additional training iterations.  Commenting out those initialization lines\n",
    "# just lets you pick up where you left off when you ran the cell before.\n",
    "\n",
    "W = np.ones((1, INPUT_SIZE))  \n",
    "ALPHA = None # change this\n",
    "\n",
    "J_sequence = []\n",
    "\n",
    "# Your code here\n",
    "    \n",
    "print(\"Final W:\", W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d536d2-b691-459a-a42f-200ca575475e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot the cost as a function of number of iterations of the\n",
    "# gradient descent algorithm.\n",
    "\n",
    "plt.scatter(range(0, len(J_sequence)), J_sequence)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2529afca-0a10-41ee-aaed-79079e2df21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test/train accuracy\n",
    "\n",
    "def compute_accuracy(X, y):\n",
    "    # YOUR CODE HERE - compute accuracy for this data set of examples (X) and their\n",
    "    # corresponding outputs (y).  X is a matrix and y is a vector, but X has the same\n",
    "    # number of rows as y has entries.  Hint:\n",
    "    pass\n",
    "\n",
    "# Try to get training accuracy and test accuracy to at least\n",
    "# 95% and 90% respectively.  (for Spanish/English)\n",
    "\n",
    "print(compute_accuracy(X_train, y_train))\n",
    "print(compute_accuracy(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615d9397-df8a-459e-8639-52a58e508e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Real life demo!\n",
    "\n",
    "def classify_language(sentence):\n",
    "    stripped_sentence = unidecode(\"\".join(c for c in sentence.lower() if c.isalpha()))\n",
    "    #print(\"Removed punctuation/accents/spaces:\", stripped_sentence)\n",
    "    d = {}\n",
    "    for letter in ALL_LETTERS:\n",
    "        d['freq_' + letter] = [stripped_sentence.lower().count(letter)/len(stripped_sentence)]\n",
    "    x_demo = pd.DataFrame(d).to_numpy()[0]\n",
    "    answer = make_prediction(x_demo, W)\n",
    "    if answer == 1:\n",
    "        return LANG1\n",
    "    else:\n",
    "        return LANG0\n",
    "    \n",
    "# Demo first five examples of each language:\n",
    "for i in range(0, 5):\n",
    "    sentence = all_data[all_data['lang'] == LANG0]['text'].iloc[i]\n",
    "    print(\"\\\"\" + sentence + \"\\\" classified as \" + classify_language(sentence))\n",
    "    sentence = all_data[all_data['lang'] == LANG1]['text'].iloc[i]\n",
    "    print(\"\\\"\" + sentence + \"\\\" classified as \" + classify_language(sentence))\n",
    "    \n",
    "print()\n",
    "sentence = input(\"Type in a sentence in \" + LANG0 + \" or \" + LANG1)\n",
    "print(\"You typed\", sentence)\n",
    "\n",
    "print(\"Predicted language:\", classify_language(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323f76d4-d051-4ec0-8105-cd8c0a17baad",
   "metadata": {},
   "source": [
    "## Part B: 2 Layer Neural Network Refresher\n",
    "\n",
    "Recall that in a 2 layer network (1 hidden layer), we have the following variables.  \n",
    "\n",
    "- `HIDDEN_LAYER_SIZE`: number of nodes in the hidden layer.  This can be set to essentially any integer greater than zero.\n",
    "  Larger numbers will generally increase performance, but will take longer to train.\n",
    "- $x$: values of our features that are in the input to the network.  Here, this\n",
    "  is a vector of 26 letter frequencies.  In the code, this is called `x`.\n",
    "- $in^{[1]}$: A copy of $x$, but with a 1 appended at the front.  In code, this is called `in1`.\n",
    "- $W^{[1]}$: Weight matrix. Dimensions are HIDDEN_LAYER_SIZE rows by 27 columns, because the first layer has 27 input \n",
    "  features and HIDDEN_LAYER_SIZE outputs.  In code, this is `W1`.\n",
    "- $z^{[1]}$: Computed as the matrix product of $W1$ and the inputs $in^{[1]}$.  This will be a vector with the\n",
    "  same number of entries as HIDDEN_LAYER_SIZE, because the first layer is now computing this many outputs (rather than\n",
    "  just 1).  In code, this is `z1`.\n",
    "- $a$ or $a^{[1]}$: Computed by the sigmoid function (activation function) applied to $z^{[1]}$.  Same size as $a^{[1]}$\n",
    "  In code, this is `a` or `a1`.\n",
    "- $in^{[2]}$: A copy of $a^{[1]}$, but with a 1 appended at the front.  In code, this is called `in2`.\n",
    "- $W^{[2]}$: Weight matrix. Dimensions are 1 rows by HIDDEN_LAYER_SIZE+1 columns, because the second layer has \n",
    "  HIDDEN_LAYER_SIZE inputs (one each from layer 1, plus the bias input) and 1 output.  In code, this is `W2`.\n",
    "- $z^{[2]}$: Computed as the matrix product of $W2$ and the inputs $in^{[2]}$.  Because $W2$ is only one row, this is \n",
    "  essentially the dot product of $W2$ (treated as a vector) and $in^{[2]}$.  In code, this is `z2`.\n",
    "- $a^{[2]}$: Computed by the sigmoid function (activation function) applied to $z^{[2]}$.  Final output of the network.\n",
    "  In code, this is `a2`.\n",
    "\n",
    "Recall that a 2-layer neural network's output is the result of the last activation function, which here is $a^{[2]}$.\n",
    "  \n",
    "You must write the following functions:\n",
    "\n",
    "- `forward_prop`: Run forward propagation.  You can use the single-layer NN code as a guide.  Returns six \n",
    "  variables: in1, a1, z1, in2, z2, a2.\n",
    "- `make_prediction`: Use the single-layer code as a guide.\n",
    "- `compute_cost`: Compute the cost (average loss) over the training set.\n",
    "- `backward_prop`: Run backpropagation.\n",
    "\n",
    "The values you are computing for backpropagation are the entries in the TWO matrices $\\dfrac{\\partial L}{\\partial w^{[2]}_j}$\n",
    "and $\\dfrac{\\partial L}{\\partial w^{[1]}_{k,j}}$.\n",
    "\n",
    "We are using the notation $w^{[2]}_j$ to stand for the $j$'th entry in the weight matrix $W^{[2]}$.  Because $W^{[2]}$ has only one row,\n",
    "$w^{[2]}_j$ is stored in the variable `W2[0][j]` and the corresponding partial derivative will be in `dL_dW2[0][j]`.\n",
    "\n",
    "We are using the notation $w^{[1]}_{k,j}$ to stand for the $k$'th row and $j$'th column in the weight matrix $W^{[1]}$.  \n",
    "Unlike $W^{[2]}$, $W^{[1]}$ has multiple rows, so \n",
    "$w^{[1]}_{k,j}$ is stored in the variable `W1[k][j]` and the corresponding partial derivative will\n",
    "be in `dL_dW1[k][j]`.\n",
    "\n",
    "Formulas: \n",
    "\n",
    "Derivatives of output layer going backwards to hidden layer (do this first).  These entries will go in `dL_dW2`.\n",
    "\n",
    "$\\dfrac{\\partial L}{\\partial w^{[2]}_j} = \\dfrac{\\partial L}{\\partial a^{[2]}} \\cdot \\dfrac{\\partial a^{[2]}}{\\partial z^{[2]}}\n",
    "  \\cdot \\dfrac{\\partial z^{[2]}}{\\partial w^{[2]}_j}$\n",
    "  \n",
    "this is all basically the same as the single layer network, with one change:\n",
    "\n",
    "$\\dfrac{\\partial z^{[2]}}{\\partial w^{[2]}_j} = in^{[2]}_j$  (we just added the [2] subscript)\n",
    "\n",
    "So you will return a single row, HIDDEN_LAYER_SIZE column matrix with entries using the formula above (three multiplications).\n",
    "You can do this with a for loop where you loop over the `in2` variable.\n",
    "Note that the first two terms are the same for all entries in the matrix, and so don't need to be computed inside the loop.\n",
    "\n",
    "The derivatives for going from the hidden layer to the input layer are slightly more complicated.\n",
    "\n",
    "$\\dfrac{\\partial L}{\\partial w^{[1]}_{k,j}} = \\dfrac{\\partial L}{\\partial a^{[2]}} \\cdot \\dfrac{\\partial a^{[2]}}{\\partial z^{[2]}}\n",
    "  \\cdot \\dfrac{\\partial z^{[2]}}{\\partial a^{[1]}_k} \\cdot \\dfrac{\\partial a^{[1]}_k}{\\partial z^{[1]}_k}\n",
    "  \\cdot \\dfrac{\\partial z^{[1]}_k}{\\partial w^{[1]}_{k,j}}$. \n",
    "  \n",
    "The first two terms are the same as the first two terms above.\n",
    "\n",
    "The third, fourth, and fifth terms are defined as:\n",
    "\n",
    "$\\dfrac{\\partial z^{[2]}}{\\partial a^{[1]}_k} = w^{[2]}_{k+1}$ = `W2[0][k+1]` (remember W2 is only one row) (the k+1 comes\n",
    "from the fact that when we moved from $a^{[1]}$ to $in^{[2]}$, we added a bias input as $in^{[2]}_0$, so all the subscripts \n",
    "for $in^{[2]}$ are shifted up by 1.\n",
    "\n",
    "$\\dfrac{\\partial a^{[1]}_k}{\\partial z^{[1]}_k} = \\sigma(z^{[1]}_k)(1-\\sigma(z^{[1]}_k)$.\n",
    "\n",
    "$\\dfrac{\\partial z^{[1]}_k}{\\partial w^{[1]}_{k,j}} = in^{[1]}_j$\n",
    "\n",
    "Computing this whole thing:\n",
    "\n",
    "Use nested loops, one for $j$ and one for $k$.  Which one is inner and which one is outer doesn't matter too much.\n",
    "The $k$ variable counts *rows* of `W1`/`dL_dW1` and $j$ counts the columns.  So you can calculate the upper bounds\n",
    "of these nested loops in a few ways, based on the dimensions of `W1` or the lengths of `in1` and `in2`:\n",
    "\n",
    "  In other words, remember the dimensions of `W1` (and therefore `dL_dW1`) are HIDDEN_LAYER_SIZE rows by 27 columns,\n",
    "  So $k$ (row counter) should range from 0 to `HIDDEN_LAYER_SIZE` which should also be `len(in2)-1`.\n",
    "  And $j$ (column counter) should range from 0 to `len(in1)` which should be 27.\n",
    "  \n",
    "Inside the nested loop, you should compute the last three terms of the five above.  You can re-use the first\n",
    "two terms from the `dL_dW2` computation, assuming you saved them in variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150065bb-9f1e-41bf-9b54-c63ea09b60d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 layer neural network (one hidden layer)\n",
    "\n",
    "INPUT_SIZE = 27\n",
    "HIDDEN_LAYER_SIZE = 5\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1/(1 + np.exp(-z))\n",
    "\n",
    "def compute_z(W, inputs):\n",
    "    '''\n",
    "    Compute the z vector for a neural network.\n",
    "    inputs: inputs to the neural network, vector of variable size.\n",
    "    W: weight matrix for the inputs, matrix of variable size.\n",
    "    '''\n",
    "    return W @ inputs\n",
    "\n",
    "def compute_activation(z_vector):\n",
    "    return sigmoid(z_vector)\n",
    "\n",
    "def deriv_activation(z):\n",
    "    return sigmoid(z) * (1 - sigmoid(z))\n",
    "\n",
    "def deriv_cross_entropy_loss(a, y):\n",
    "    return (a - y)/(a * (1 - a))\n",
    "\n",
    "def compute_cross_entropy_loss(a, y):\n",
    "    if y == 0:\n",
    "        return -np.log(1-a)\n",
    "    else:\n",
    "        return -np.log(a)\n",
    "\n",
    "def augment_vector(v):\n",
    "    return np.insert(v, 0, 1)\n",
    "\n",
    "def forward_prop(W1, W2, x):\n",
    "    '''\n",
    "    Run the forward propagation algorithm.\n",
    "    x: input to the neural network, vector of size INPUT_SIZE - 1\n",
    "    W1: weight matrix, matrix of size (HIDDEN_LAYER_SIZE, INPUT_SIZE)\n",
    "    W2: weight matrix, matrix of size (1, HIDDEN_LAYER_SIZE + 1)\n",
    "    returns: in1, z1, a1, in2, z2, a2\n",
    "    '''\n",
    "    DEBUG = False\n",
    "    \n",
    "    # we assume x DOES NOT have a 1 in front.\n",
    "    assert len(x) == INPUT_SIZE - 1\n",
    "    \n",
    "    # check dimensions of W1\n",
    "    assert W1.shape == (HIDDEN_LAYER_SIZE, INPUT_SIZE)\n",
    "    \n",
    "    # check dimensions of W2\n",
    "    assert W2.shape == (1, HIDDEN_LAYER_SIZE + 1)\n",
    "    \n",
    "    # YOUR CODE HERE: use single layer NN code as a guide.\n",
    "    \n",
    "    return in1, z1, a1, in2, z2, a2\n",
    "    \n",
    "def compute_output(x, W1, W2):\n",
    "    '''\n",
    "    Returns the output of the neural network (probability of x being in the 1 class).\n",
    "    x: input to the neural network, vector of size INPUT_SIZE\n",
    "    W1: weight matrix, matrix of size (HIDDEN_LAYER_SIZE, INPUT_SIZE)\n",
    "    W2: weight matrix, matrix of size (1, HIDDEN_LAYER_SIZE + 1)\n",
    "    '''\n",
    "    \n",
    "    # we assume x DOES NOT have a 1 in front.\n",
    "    assert len(x) == INPUT_SIZE - 1\n",
    "    \n",
    "    # check dimensions of W1\n",
    "    assert W1.shape == (HIDDEN_LAYER_SIZE, INPUT_SIZE)\n",
    "    \n",
    "    # check dimensions of W2\n",
    "    assert W2.shape == (1, HIDDEN_LAYER_SIZE + 1)\n",
    "    \n",
    "    in1, z1, a1, in2, z2, a2 = forward_prop(W1, W2, x)\n",
    "    return a2\n",
    "    \n",
    "def make_prediction(x, W1, W2):\n",
    "    '''\n",
    "    Returns the classification of x.\n",
    "    x: input to the neural network, vector of size INPUT_SIZE\n",
    "    W1: weight matrix, matrix of size (HIDDEN_LAYER_SIZE, INPUT_SIZE)\n",
    "    W2: weight matrix, matrix of size (1, HIDDEN_LAYER_SIZE + 1)\n",
    "    '''\n",
    "    \n",
    "    # we assume x DOES NOT have a 1 in front.\n",
    "    assert len(x) == INPUT_SIZE - 1\n",
    "    \n",
    "    # check dimensions of W1\n",
    "    assert W1.shape == (HIDDEN_LAYER_SIZE, INPUT_SIZE)\n",
    "    \n",
    "    # check dimensions of W2\n",
    "    assert W2.shape == (1, HIDDEN_LAYER_SIZE + 1)\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "\n",
    "def compute_cost(X_data, y_data, W1, W2):\n",
    "    '''\n",
    "    Compute the cost (average loss) over the training examples in X_data, y_data.\n",
    "    W1: weight matrix, matrix of size (HIDDEN_LAYER_SIZE, INPUT_SIZE)\n",
    "    W2: weight matrix, matrix of size (1, HIDDEN_LAYER_SIZE + 1)\n",
    "    '''\n",
    "    \n",
    "    # check dimensions of W1\n",
    "    assert W1.shape == (HIDDEN_LAYER_SIZE, INPUT_SIZE)\n",
    "    \n",
    "    # check dimensions of W2\n",
    "    assert W2.shape == (1, HIDDEN_LAYER_SIZE + 1)\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "\n",
    "def backward_prop(W1, W2, y, in1, z1, a1, in2, z2, a2):\n",
    "    '''\n",
    "    Run backward propagation.\n",
    "    W1: weight matrix, matrix of size (HIDDEN_LAYER_SIZE, INPUT_SIZE)\n",
    "    W2: weight matrix, matrix of size (1, HIDDEN_LAYER_SIZE + 1)\n",
    "    y: scalar of correct target output, 0 or 1\n",
    "    in1: inputs to the neural network, length INPUT_SIZE\n",
    "    z1: z1 scalar from forward prop of NN\n",
    "    a1: activation vector from forward prop of NN\n",
    "    in2: inputs to the 2nd layer of NN\n",
    "    z2: z2 scalar from forward prop of NN\n",
    "    a2: activation vector from forward prop of NN\n",
    "    Returns: partial derivatives of loss function with\n",
    "        respect to each entry in weight matrix W1 and W2 (same dimensions as W1 and W2)\n",
    "    '''\n",
    "    \n",
    "    # check dimensions of W1\n",
    "    assert W1.shape == (HIDDEN_LAYER_SIZE, INPUT_SIZE)\n",
    "    \n",
    "    # check dimensions of W2\n",
    "    assert W2.shape == (1, HIDDEN_LAYER_SIZE + 1)\n",
    "    \n",
    "    # make derivative matrix of same size as W1, filled with zeros\n",
    "    dL_dW1 = np.full_like(W1, 0)\n",
    "    \n",
    "    # make derivative matrix of same size as W2, filled with zeros\n",
    "    dL_dW2 = np.full_like(W2, 0)\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    return dL_dW1, dL_dW2\n",
    "     \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104007c1-b8f7-440b-a190-95b177986d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity checks for forward_prop (for a hidden layer size of 5)\n",
    "\n",
    "W1_sanity = np.ones((HIDDEN_LAYER_SIZE, INPUT_SIZE))\n",
    "W2_sanity = np.ones((1, HIDDEN_LAYER_SIZE+1))\n",
    "in1, z1, a1, in2, z2, a2 = forward_prop(W1_sanity, W2_sanity, X_train[0])\n",
    "print(in1, \"\\n\", z1, \"\\n\", a1)\n",
    "print(in2, \"\\n\", z2, \"\\n\", a2)\n",
    "\n",
    "# output:\n",
    "#[1.         0.11111111 0.         0.         0.08333333 0.08333333\n",
    "# 0.         0.         0.05555556 0.         0.         0.02777778\n",
    "# 0.         0.05555556 0.08333333 0.13888889 0.         0.\n",
    "# 0.08333333 0.         0.08333333 0.02777778 0.         0.05555556\n",
    "# 0.         0.11111111 0.        ] \n",
    "# [2. 2. 2. 2. 2.] \n",
    "# [0.88079708 0.88079708 0.88079708 0.88079708 0.88079708]\n",
    "#[1.         0.88079708 0.88079708 0.88079708 0.88079708 0.88079708] \n",
    "# [5.40398539] \n",
    "# [0.99552153]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d322e3-1258-491a-a9d8-7803d5af3cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity checks for compute_cost (hidden layer size = 5)\n",
    "\n",
    "compute_cost(X_train, y_train, W1_sanity, W2_sanity)  # should be array([2.70648122])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70884ee9-5408-445b-96c2-7a556dc0b8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity checks for backprop (hidden layer size = 5)\n",
    "\n",
    "print(backward_prop(W1_sanity, W2_sanity, y_train[0], in1, z1, a1, in2, z2, a2))\n",
    "\n",
    "#(array([[0.10452337, 0.01161371, 0.        , 0.        , 0.00871028,\n",
    "#        0.00871028, 0.        , 0.        , 0.00580685, 0.        ,\n",
    "#        0.        , 0.00290343, 0.        , 0.00580685, 0.00871028,\n",
    "#        0.01451714, 0.        , 0.        , 0.00871028, 0.        ,\n",
    "#        0.00871028, 0.00290343, 0.        , 0.00580685, 0.        ,\n",
    "#        0.01161371, 0.        ],\n",
    "#       [0.10452337, 0.01161371, 0.        , 0.        , 0.00871028,\n",
    "#        0.00871028, 0.        , 0.        , 0.00580685, 0.        ,\n",
    "#        0.        , 0.00290343, 0.        , 0.00580685, 0.00871028,\n",
    "#        0.01451714, 0.        , 0.        , 0.00871028, 0.        ,\n",
    "#        0.00871028, 0.00290343, 0.        , 0.00580685, 0.        ,\n",
    "#        0.01161371, 0.        ],\n",
    "#       [0.10452337, 0.01161371, 0.        , 0.        , 0.00871028,\n",
    "#        0.00871028, 0.        , 0.        , 0.00580685, 0.        ,\n",
    "#        0.        , 0.00290343, 0.        , 0.00580685, 0.00871028,\n",
    "#        0.01451714, 0.        , 0.        , 0.00871028, 0.        ,\n",
    "#        0.00871028, 0.00290343, 0.        , 0.00580685, 0.        ,\n",
    "#        0.01161371, 0.        ],\n",
    "#       [0.10452337, 0.01161371, 0.        , 0.        , 0.00871028,\n",
    "#        0.00871028, 0.        , 0.        , 0.00580685, 0.        ,\n",
    "#        0.        , 0.00290343, 0.        , 0.00580685, 0.00871028,\n",
    "#        0.01451714, 0.        , 0.        , 0.00871028, 0.        ,\n",
    "#        0.00871028, 0.00290343, 0.        , 0.00580685, 0.        ,\n",
    "#        0.01161371, 0.        ],\n",
    "#       [0.10452337, 0.01161371, 0.        , 0.        , 0.00871028,\n",
    "#        0.00871028, 0.        , 0.        , 0.00580685, 0.        ,\n",
    "#        0.        , 0.00290343, 0.        , 0.00580685, 0.00871028,\n",
    "#        0.01451714, 0.        , 0.        , 0.00871028, 0.        ,\n",
    "#        0.00871028, 0.00290343, 0.        , 0.00580685, 0.        ,\n",
    "#        0.01161371, 0.        ]]), array([[0.99552153, 0.87685246, 0.87685246, 0.87685246, 0.87685246,\n",
    "#        0.87685246]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d26477e-ef28-4227-9c4d-16e95ae8ec4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write code for gradient descent here.\n",
    "\n",
    "# This is the same as earlier in this project, except you have\n",
    "# two matrix variables, W1 and W2 now, and their corresponding\n",
    "# gradients (derivative variables).\n",
    "\n",
    "# Note that you can comment out the two W1/W2 initialization lines \n",
    "# and the J_sequence initialization line\n",
    "# if you run gradient descent but then want to run the cell again to \n",
    "# run additional training iterations.  Commenting out the W initialization line\n",
    "# just lets you pick up where you left off when you ran the cell before.\n",
    "\n",
    "# THIS WILL TAKE A LONG TIME TO CONVERGE.  WHERE LONG TIME = POSSIBLY 5-10 MINUTES.\n",
    "# I recommend finding a reasonable alpha by starting with a small number of iterations.\n",
    "# Once you have a good alpha, then you can start running more and more iterations\n",
    "# as to not have to sit around for minutes at a time waiting while this runs.\n",
    "# You can also use the commenting-out-the-initialization idea above to see\n",
    "# if you've converged, and if not, just pick up where you left off.\n",
    "\n",
    "W1 = np.random.normal(size=(HIDDEN_LAYER_SIZE, INPUT_SIZE))\n",
    "W2 = np.random.normal(size=(1, HIDDEN_LAYER_SIZE+1))\n",
    "ALPHA = None\n",
    "\n",
    "J_sequence = []\n",
    "    \n",
    "print(\"Final W1 and W2:\", W1, W2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf48547-ae0e-4277-b0c5-b54d8a44946e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot the cost as a function of number of iterations of the\n",
    "# gradient descent algorithm.\n",
    "\n",
    "plt.scatter(range(0, len(J_sequence)), J_sequence)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182a20c3-1abf-4225-a7af-7952058d9320",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test/train accuracy\n",
    "\n",
    "def compute_accuracy(X, y):\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "# Try to get training accuracy and test accuracy to at least\n",
    "# 99% and 91% respectively.  (for spanish/english)\n",
    "\n",
    "print(compute_accuracy(X_train, y_train))\n",
    "print(compute_accuracy(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645de7fd-d976-4d92-a7ec-260867e1a9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Real life demo!\n",
    "\n",
    "def classify_language(sentence):\n",
    "    stripped_sentence = unidecode(\"\".join(c for c in sentence.lower() if c.isalpha()))\n",
    "    #print(\"Removed punctuation/accents/spaces:\", stripped_sentence)\n",
    "    d = {}\n",
    "    for letter in ALL_LETTERS:\n",
    "        d['freq_' + letter] = [stripped_sentence.lower().count(letter)/len(stripped_sentence)]\n",
    "    x_demo = pd.DataFrame(d).to_numpy()[0]\n",
    "    answer = make_prediction(x_demo, W1, W2)\n",
    "    if answer == 1:\n",
    "        return LANG1\n",
    "    else:\n",
    "        return LANG0\n",
    "    \n",
    "# Demo first five examples of each language:\n",
    "for i in range(0, 5):\n",
    "    sentence = all_data[all_data['lang'] == LANG0]['text'].iloc[i]\n",
    "    print(\"\\\"\" + sentence + \"\\\" classified as \" + classify_language(sentence))\n",
    "    sentence = all_data[all_data['lang'] == LANG1]['text'].iloc[i]\n",
    "    print(\"\\\"\" + sentence + \"\\\" classified as \" + classify_language(sentence))\n",
    "    \n",
    "print()\n",
    "sentence = input(\"Type in a sentence in \" + LANG0 + \" or \" + LANG1)\n",
    "print(\"You typed\", sentence)\n",
    "\n",
    "print(\"Predicted language:\", classify_language(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a322ab-f889-4272-90e7-38b2291b1d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FINAL QUESTIONS\n",
    "# answer this with respect to training on Spanish/English\n",
    "\n",
    "# Train/Test accuracy for single-layer network:  (copy from above)\n",
    "# ANSWER:\n",
    "\n",
    "# Train/Test accuracy for two-layer network:  (copy from above)\n",
    "# ANSWER:\n",
    "\n",
    "# Extra credit for testing other languages.  For each other language pair\n",
    "# you test, report training and testing accuracy.  (Up to 3 bonus points).\n",
    "# ANSWERS: (tell me what language pairs you tried)\n",
    "\n",
    "# For up to 2 more bonus points, speculate on why some language pairs are \"harder\"\n",
    "# to learn than others.  Harder = lower testing/training accuracies meaning\n",
    "# they are harder to distinguish from each other.  Refer to the specific\n",
    "# language pairs you ran and the accuracies you came up with to support your\n",
    "# arguments.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
