{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abfb2a24-05ab-4b1d-8fc2-7448458663bd",
   "metadata": {},
   "source": [
    "# Project 2.5\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Project 2.5 picks up where Project 2 left off.  Recall that in Project 2, were exploring logistic regression by using an example from \n",
    "the TV show *The Good Place*.  The show is centered around a number of humans who have died and find themselves in the afterlife.  In this conception\n",
    "of the afterlife, humans are sent to \"the Good Place\" or \"the Bad Place\" after death.  All humans are assigned a numerical score based on the morality of their conduct in life, and only those with the very highest scores are sent to the \"Good Place\", where they enjoy eternal happiness; all others experience an eternity of torture in the \"Bad Place.\"\n",
    "\n",
    "In Project 2.5, we expand the concepts of the good place and the bad place to also include the \"medium place,\" and we will attempt to classify which \n",
    "of these three places someone will end up in the TV-show-afterlife.  We will use **multinomial logistic regression** for this.  \n",
    "\n",
    "***You will probably want to refresh your memory by re-reading the logistic-regression-3 notebook, which covers multinomial linear regression.***\n",
    "\n",
    "***We also make one slight change in this project: we use 0-based class indexing, rather than 1-based.  So our 3 classes are 0/1/2, not 1/2/3. We\n",
    "do this so they match up with the array indices in Python.***\n",
    "\n",
    "As before, we have data for 1000 people about how often they:\n",
    "\n",
    "- Let someone merge in front of them in traffic\n",
    "- Didn't tip their server at a restaurant\n",
    "- Held a door open for someone who was walking behind them\n",
    "- Littered\n",
    "\n",
    "These will be our four features for the problem.  Our data set consists of these four features tallied for 1000 different people.\n",
    "\n",
    "To complete this project, you will write Python code in places marked\n",
    "`# YOUR CODE HERE`.  There are also code cells in this notebook you must run\n",
    "to produce various kinds of plots and graphs.  There are also a number of cells\n",
    "marked with `# YOUR ANSWER HERE` where you will answer questions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a37aed2-9e9c-43c6-a7fb-0d93fddff848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FILL THIS IN:\n",
    "\n",
    "# Name:\n",
    "# Honor pledge:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33089f13-fef9-4680-9095-ddf1333cee62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10cb214-0b0d-4fbc-b6e9-38a4b4e066e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "\n",
    "# Write code below to read the CSV file \"data2.csv\" and put it into a\n",
    "# Pandas dataframe called `df`:\n",
    "\n",
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a7ce96-70c8-4ae6-aecf-2d06692fe2e8",
   "metadata": {},
   "source": [
    "## Explanation of the data file\n",
    "\n",
    "Each row of the file represents data about a person.  \n",
    "\n",
    "The first four columns should be self-explanatory: they tell how often a person did a\n",
    "certain activity (explained above).  \n",
    "\n",
    "There is now a new column at the end called `noisygoodmedbed` which tells which of the three places\n",
    "(good/medium/bad) the person ends up in.  The \"noisy\" part is because some of the data has been altered to be a little less \"perfect.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3786e4-a047-4030-bf20-e97b083b15df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first few and last few lines of this data:\n",
    "\n",
    "print(len(df)) # Should be 1000\n",
    "df  # Verify this looks ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a4df79-02ae-47de-aef3-c381bd00a97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select our X and y data\n",
    "\n",
    "# First, we will split the data frame above into a four-column frame\n",
    "# with the input features (X's) and a one-column frame with the target\n",
    "# feature (y), which we will use the noisy column (noisygoodmedbad).\n",
    "\n",
    "# Write code below to create df_X with just the four X feature columns,\n",
    "# and df_y that has just the noisygoodbad column.\n",
    "\n",
    "# Then **normalize** the X values with Z-score normalization as in \n",
    "# project 1.\n",
    "\n",
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db28502-695c-4caa-994c-d5bd1f929bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check for df_X\n",
    "\n",
    "df_X  # Should print a data frame with 1000 rows and 4 columns.\n",
    "# First row should be [ -1.161413 -1.267048 -1.350553 -1.620752]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f31299c-bed6-44be-bd11-bb5eb84ba006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check for df_y\n",
    "\n",
    "df_y  # Should be a column of goods, mediums, and bads, starting with medium, bad, medium, medium, medium,\n",
    "# and ending with medium, bad, medium, good, good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6667ab3c-7106-4b57-b612-4756034136bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we need to switch df_y to not have good/medium/bad strings, but rather 0/1/2's.\n",
    "# Use this piece of code:\n",
    "\n",
    "df_y, y_cats = pd.factorize(pd.Categorical(df_y, categories=['bad', 'medium', 'good'], ordered=True),sort=True)\n",
    "\n",
    "print(\"Categories=\", y_cats)\n",
    "print(\"New df_y=\", df_y)\n",
    "\n",
    "# This changes df_y so 0 represents bad, 1 represents medium, and 2 represents good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e3bd12-3d9e-45b3-9aea-9177b8904507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot vectors\n",
    "\n",
    "# Run this cell to convert df_y to one-hot vectors.\n",
    "\n",
    "df_y_one_hot = pd.get_dummies(df_y).to_numpy()\n",
    "\n",
    "print(df_y[0:10])  # should print [1 0 1 1 1 1 2 2 2 0]\n",
    "print(df_y_one_hot[0:10])  # should print 10 one-hot vectors correspoding to the 10 values above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d6faa7-5308-4884-ac49-ed5af1eb15a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check shapes:\n",
    "\n",
    "print(df_X.shape) # Should be (1000, 4)\n",
    "print(df_y.shape) # Should be (1000,)\n",
    "print(df_y_one_hot.shape) # Should be (1000, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ea4ac9-0c3c-470b-8425-2da67ca44c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing.\n",
    "\n",
    "# We want to write code to split the data frames above into a few\n",
    "# new data frames.  In particular, are going to have a TRAINING SET\n",
    "# and a TESTING SET for this project.  We will use 80% of the data for \n",
    "# training, and the remaining 20% for testing.  \n",
    "\n",
    "# In the real world, we would split the data randomly, but so we all\n",
    "# end up with the same results, we will use the first 80% of the data\n",
    "# for training, and the last 20% for testing (in order of how the rows\n",
    "# show up in the file).  Note that there are 1000 people (rows in \n",
    "# the file), so the first 800 rows will be training, and the last 200\n",
    "# will be testing.\n",
    "\n",
    "# Write code here to create SIX NUMPY ndarrays:\n",
    "\n",
    "# - X_train: first 800 lines of df_X\n",
    "# - X_test: last 200 lines of df_X\n",
    "# - y_train: first 800 lines of df_y\n",
    "# - y_test: last 200 lines of df_y\n",
    "# - y_train: first 800 lines of df_y_one_hot\n",
    "# - y_test: last 200 lines of df_y_one_hot\n",
    "\n",
    "# Then, add a column of ones to the left side of X_train and X_test.\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3f59aa-ba82-4ec5-9234-b390140e4800",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity checks:\n",
    "\n",
    "print(X_train.shape) # Should be (800, 5)\n",
    "print(y_train.shape) # Should be (800,) \n",
    "print(y_train_one_hot.shape) # Should be (800, 3) \n",
    "print(X_test.shape) # Should be (200, 5)\n",
    "print(y_test.shape) # Should be (200,) \n",
    "print(y_test_one_hot.shape) # Should be (200, 3) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129456c3-f616-41bd-a93e-79d585030e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show first few rows of training/testing data:  (will be useful to have these later)\n",
    "\n",
    "print(\"First 10 training examples:\")\n",
    "print(np.hstack([X_train, y_train.reshape(-1, 1)])[0:10])\n",
    "print()\n",
    "print(\"First 10 testing examples:\")\n",
    "print(np.hstack([X_test, y_test.reshape(-1, 1)])[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f575209-8473-461d-99d3-51e482549cc7",
   "metadata": {},
   "source": [
    "## Part A\n",
    "\n",
    "Like in Part A of the previous project, we will rely on an external method to create\n",
    "a logistic regression model for us, then we will see if we can replicate it ourselves.\n",
    "\n",
    "Below is code that uses scikit-learn to do this for us.  Don't worry too much about what it does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102fdfea-65d7-4803-9ea4-c2fa5f56e9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and install scikit-learn if not already done:\n",
    "%pip install scikit-learn\n",
    "\n",
    "# Import the logistic regression functionality from scikit-learn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Create a logistic regression model and train it on our training data:\n",
    "\n",
    "model = LogisticRegression(random_state=0, penalty=None, fit_intercept=False, multi_class='multinomial').fit(X_train, y_train)\n",
    "\n",
    "# If the line above gives an error about penalty=None, try switching that part to penalty='none' instead.\n",
    "\n",
    "# model.coef_ contains the w vector that this logistic regression model was able to find, so\n",
    "# we'll treat it as the \"best\" w and see if we can match it manually.\n",
    "W_direct = model.coef_\n",
    "\n",
    "print(\"W found through scikit-learn:\", W_direct)\n",
    "\n",
    "# Sanity check: W_direct should be a matrix with 3 rows and 5 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc5e005-4294-4c93-a8a7-227c6db31309",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below, explain why W_direct has 3 rows and 5 columns.  What is the significance of the 3 and the 5,\n",
    "# and what do these numbers correspond to in our data set?\n",
    "\n",
    "# YOUR ANSWER HERE:\n",
    "#\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3031723e-d89c-4613-8eb7-a8bd7cd16b64",
   "metadata": {},
   "source": [
    "## Part B\n",
    "\n",
    "In this part you will write code for multinomial logistic regression by hand, including the model,\n",
    "the loss function, the cost function, and gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ea1eb0-3006-4789-a7da-50bd839697f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the softmax function\n",
    "\n",
    "from scipy.special import softmax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d7adf2-20fe-4f18-a59f-863ffaf8f5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "\n",
    "print(softmax([0, -1, .5]))\n",
    "print(sum(softmax([0, -1, .5])))\n",
    "\n",
    "# should print [0.33149896 0.12195165 0.54654939] and then 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4f2961-7789-4ff7-8aec-35e140601a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function called run_model below to run the multinomial logistic\n",
    "# regression model on one feature vector (x_data).\n",
    "# Recall that the model for multinomial logistic regression is f(x) = yhat = softmax(Wx),\n",
    "# where W is our matrix of weights and x is our input vector, and we multiply them together.\n",
    "\n",
    "def run_model(x_data, W):\n",
    "    \"\"\"\n",
    "    x_data: array of features (n+1)\n",
    "    W: matrix of weights (K rows, n+1 cols)\n",
    "    returns: scalar\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d36a583-5503-4b8b-ace8-851d3b0e5f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check: run the model from Part A on the first training example in X_train:\n",
    "\n",
    "run_model(X_train[0], W_direct)  # should be [3.34188245e-08, 7.79571753e-01, 2.20428213e-01]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111adf69-bbe2-4bab-a09e-161deb3aea1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## QUESTIONS:\n",
    "\n",
    "# Which of the three numbers in the sanity check immediately above is the biggest?\n",
    "# Which *INDEX* in the array does this biggest number appear in?\n",
    "\n",
    "# ANSWER:\n",
    "#\n",
    "#\n",
    "\n",
    "# According to y_train[0], does the data in X_train[0] correspond to someone going to\n",
    "# the good place, medium place, or bad place?\n",
    "# Does this match up with what our model (from Part A) is predicting?  Explain how you determined this.\n",
    "\n",
    "# ANSWER:\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5638376f-586d-4fdc-bb96-4a1d4d322993",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function called make_prediction that will\n",
    "# actually predict the class 0, 1, or 2 for a feature vector x_data.\n",
    "# To do this, just call run_model and find the INDEX of the largest number in the array.\n",
    "\n",
    "def make_prediction(x_data, W):\n",
    "    \"\"\"\n",
    "    x_data: array of features (n+1)\n",
    "    W: matrix of weights (K rows, n+1 cols)\n",
    "    returns: class number (0, 1 or 2)\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e4e0e9-0efc-46c1-a012-2da1a7b05ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check: classify the first few testing examples using the model from Part A\n",
    "\n",
    "for i in range(10):\n",
    "    print(\"Predicted class for test example\", i, \"is\", make_prediction(X_test[i], W_direct), end='')\n",
    "    print(\", true class is\", y_test[i])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3309fc50-bf06-4b31-9740-6f13f1b5eb3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## QUESTION:\n",
    "\n",
    "# Given the output immediately above, what is the accuracy of the model in Part A (since we used W_direct\n",
    "# above) just based on these 10 training examples?  (Answer as a percent; in other words\n",
    "# the percentage of those 10 testing examples that were predicted correctly).\n",
    "\n",
    "# ANSWER:\n",
    "#\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56291ae2-12bf-498b-9500-df3f1d66a934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function called compute_accuracy that takes a \n",
    "# set of X values and a set of y values and a parameter vector\n",
    "# w.  This function should predict the class for each example x\n",
    "# in X_data and based on the true y values (y_data), compute\n",
    "# the accuracy on this data set.\n",
    "\n",
    "# To do this, call make_prediction on each row of X_data\n",
    "# and compare the output against the corresponding value in y_data.\n",
    "# Count how many predictions are correct and divide by the total.\n",
    "\n",
    "def compute_accuracy(X_data, y_data, W):\n",
    "    \"\"\"\n",
    "    X_data: matrix of features (flexible rows, n+1 cols)\n",
    "    y_data: vector of true classes (same number of rows as X_data)\n",
    "    W: matrix of weights (K rows by n+1 cols)\n",
    "    returns: percentage of rows in X_data classified correctly\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbcece4e-4d1c-4627-ac5d-bf70f9b9b9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "\n",
    "train_acc_partA = compute_accuracy(X_train, y_train, W_direct)\n",
    "test_acc_partA = compute_accuracy(X_test, y_test, W_direct)\n",
    "\n",
    "print(train_acc_partA)  # should be 0.94\n",
    "print(test_acc_partA)  # should be 0.91"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b15391a-e57f-4a17-ac3f-468e9f524686",
   "metadata": {},
   "outputs": [],
   "source": [
    "## QUESTION:\n",
    "\n",
    "# Which of the two numbers above do we report as the \"true\" accuracy of our model,\n",
    "# and why do we typically not report the other (or not give it as much importance)?\n",
    "\n",
    "# ANSWER:\n",
    "#\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b50a37-19dd-45fd-9943-6b25a9028b09",
   "metadata": {},
   "source": [
    "### Loss function\n",
    "\n",
    "Recall the loss function for multinomial logistic regression is:\n",
    "\n",
    "$$L( \\boldsymbol{\\hat{y}}, \\boldsymbol{y}) = -\\sum_{k=1}^K y_k \\log \\hat{y}_k$$\n",
    "\n",
    "Recall that $K$ is the number of classes (here, $K=3$).  \n",
    "\n",
    "Also, remember that a similar thing happens with this formula as in binary logistic regression, \n",
    "all but one of the terms in the summation above will drop out because $\\boldsymbol{y}$ is a one-hot vector.  So only one of the $y_k$\n",
    "terms above is 1; all the rest are zeros.  Let's call the $y_k$ that **is** 1 $y_c$\n",
    "($c$ standing for \"correct,\" meaning the \"correct class\"):\n",
    "\n",
    "$$L( \\boldsymbol{\\hat{y}}, \\boldsymbol{y}) = -y_c \\log \\hat{y}_c = -\\log \\hat{y}_c$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19dcefe3-8d56-4007-b4cd-deceab70c723",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the compute_loss function below to compute the\n",
    "# loss over *one* training example, given the true y_one_hot value\n",
    "# and the predicted y value (y_hat).  \n",
    "# NOTICE THE ORDER OF THE PARAMETERS (y, yhat) IS REVERSED FROM ABOVE TO BE CONSISTENT WITH THE EARLIER PROJECT.\n",
    "\n",
    "def compute_loss(y_one_hot, y_hat):\n",
    "    \"\"\"\n",
    "    y: one-hot vector (length K)\n",
    "    y_hat: probability vector (length K)\n",
    "    returns: scalar\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    pass \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24237d37-c078-4457-a9d1-d21d355e06df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check:\n",
    "\n",
    "compute_loss(y_train_one_hot[0], run_model(X_train[0], W_direct))  # should be about 0.25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c40992-f4fd-4e96-b844-44caabfe2ef4",
   "metadata": {},
   "source": [
    "### Cost function\n",
    "\n",
    "We define the cost function $J$ in the same way we always have:\n",
    "as the average of the loss function calculated across our entire training set:\n",
    "\n",
    "$$J(\\boldsymbol{W}) = \\frac{1}{m}\\sum_{i=1}^m L \\left( \n",
    "    f(x^{(i)}), \\boldsymbol{y}^{(i)} \\right) = \\frac{1}{m}\\sum_{i=1}^m L \\left( \n",
    "    \\hat{\\boldsymbol{y}}^{(i)}, \\boldsymbol{y}^{(i)} \\right)$$\n",
    "    \n",
    "Where the loss function $L$ is defined as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb17502-fd18-4d30-bc71-ed151edef0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the compute_cost function below to compute the\n",
    "# total cost over the entire data set X_data and y_data_one_hot,\n",
    "# given parameters matrix W.\n",
    "# Call your run_model() and compute_loss() functions \n",
    "# that you defined above.  You should have one loop.\n",
    "# Remember that run_model() gives you yhat in the formula above,\n",
    "# and compute_loss is the \"L\" function in the formula above.\n",
    "# DO NOT CALL MAKE_PREDICTION; it's not needed here.\n",
    "\n",
    "def compute_cost(X_data, y_data_one_hot, W):\n",
    "    \"\"\"\n",
    "    X_data: matrix (m rows, n+1 cols)\n",
    "    y_data_one_hot: matrix (m rows, K cols)\n",
    "    W: matrix of weights (K rows, n+1 cols)\n",
    "    returns: scalar\n",
    "    \"\"\"\n",
    "        \n",
    "    # YOUR CODE HERE\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f974d1-5015-40ea-9ef0-2acc352a00e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check: compute the cost for W_direct from Part A:\n",
    "\n",
    "W_direct_cost = compute_cost(X_train, y_train_one_hot, W_direct)  \n",
    "# This is the minimum cost we can ever get!  Should be about 0.139.\n",
    "print(W_direct_cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86830eb7-1970-4db1-b0ad-5fc79819f659",
   "metadata": {},
   "source": [
    "### Computing the gradient\n",
    "\n",
    "As always, we will use gradient descent to find the best matrix $\\boldsymbol{W}$ of weights.  Recall this matrix has $K$ rows (one per class) and $n+1$ columns\n",
    "(one per feature, plus the bias feature).  We do this by taking the partial derivative of the $J$ function with respect to each\n",
    "entry in $\\boldsymbol{W}$ individually, which looks like this:\n",
    "\n",
    "Our formula we want is \n",
    "$$\\dfrac{\\partial}{\\partial w_{k,j}} J(\\boldsymbol{W}) =  \n",
    "\\dfrac{\\partial}{\\partial w_{k,j}} \\frac{1}{m}\\sum_{i=1}^m L \\left(  \n",
    "    \\hat{\\boldsymbol{y}}^{(i)}, \\boldsymbol{y}^{(i)} \\right) =\n",
    "\\frac{1}{m} \\sum_{i=1}^m \\left( \\boldsymbol{\\hat{y}}_k^{(i)} - \\boldsymbol{y}^{(i)}_k \\right)x^{(i)}_j$$\n",
    "\n",
    "Let's really understand what this equation is saying.\n",
    "\n",
    "The first part, $\\dfrac{\\partial}{\\partial w_{k,j}}$ is the symbol for taking a partial derivative, but the $w_{k,j}$ part is important just to go over\n",
    "what this notation means.  In particular, we use $w_{k,j}$ to mean the $k$'th row and $j$'th column of matrix $W$ (row is always first, column is always second\n",
    "in the subscripts).\n",
    "\n",
    "Remember that $\\boldsymbol{\\hat{y}}_k^{(i)}$ is the $k$'th term of the $\\boldsymbol{\\hat{y}}^{(i)}$ vector, which is the prediction vector for the $i$'th\n",
    "training example.  Don't forget, `run_model()` gives you this vector!  (We always use $i$ to loop over training examples.)\n",
    "\n",
    "Remember that $\\boldsymbol{y}^{(i)}_k$ is the $k$'th term of the $\\boldsymbol{y}^{(i)}$ vector, which is the one-hot \"true\" vector for the $i$'th training\n",
    "example.  \n",
    "\n",
    "Remember that $x^{(i)}_j$ is the $j$'th feature (entry) in the $x^{(i)}$ vector, which is the feature vector for training example $i$.  (We always use $j$\n",
    "to loop over features.)\n",
    "\n",
    "So putting all of those together, just remember that $\\boldsymbol{\\hat{y}}_k^{(i)}$, $\\boldsymbol{y}^{(i)}_k$, and $x^{(i)}_j$ are **numbers**, not \n",
    "**vectors**.  So each of those terms you're adding up are just taking three numbers and doing one subtraction and one multiplication.\n",
    "\n",
    "As always, $m$ = number of training examples, $n$ = number of features, $K$ = number of classes.\n",
    "\n",
    "<hr>\n",
    "\n",
    "So how do you compute this mess?  The first thing to realize is that this gradient you are computing is another matrix of the same size as \n",
    "the $W$ matrix.  Each entry in the matrix is computed from that formula above.  So the easiest way to do this is with three nested loops, one each\n",
    "for $i$ (training example), $j$ (feature), and $k$ (class), though you may not want to nest them in that order.  In fact, depending on how you like to\n",
    "think about this, you may not even need three loops: you can get away with two if you reframe the $\\left( \\boldsymbol{\\hat{y}}_k^{(i)} - \\boldsymbol{y}^{(i)}_k \\right)x^{(i)}_j$ computation as a vector-times-scalar computation: $\\left( \\boldsymbol{\\hat{y}}^{(i)} - \\boldsymbol{y}^{(i)} \\right)x^{(i)}_j$.  In other words,\n",
    "you can drop the $k$ loop and think of computing the **vector** $\\boldsymbol{\\hat{y}}^{(i)} - \\boldsymbol{y}^{(i)}$ and then multiplying that vector\n",
    "by $x^{(i)}_j$, which is a scalar.  This gives you an entire column of the gradient matrix you are creating.\n",
    "\n",
    "But you can do whatever's easier---if 3 loops makes more sense, do that.  If you understand the 2 loop version, use that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78717fc5-6ff0-4b94-bfe8-bb6186cd8ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the compute_gradient function below to compute\n",
    "# the complete gradient for the function J(W).  \n",
    "# You will need to call your run_model() function\n",
    "# that you defined above.  You should have two or three nested loops.\n",
    "\n",
    "def compute_gradient(X_data, y_data_one_hot, W):\n",
    "    \"\"\"\n",
    "    X_data: matrix (m rows, n+1 cols)\n",
    "    y_data_one_hot: matrix (m rows, K cols)\n",
    "    W: matrix of weights (K rows, n+1 cols)\n",
    "    returns: matrix of gradients (K rows, n+1 cols)\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cde577f-bb7d-4eb9-ba17-f191215281cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check:\n",
    "\n",
    "sample_W = np.ones((3, 5))\n",
    "\n",
    "compute_gradient(X_train, y_train_one_hot, sample_W)\n",
    "\n",
    "# Should give you \n",
    "#array([[ 1.08333333e-01,  1.06555131e-01, -1.54107886e-01,\n",
    "#         1.79715313e-01, -1.78142481e-01],\n",
    "#       [-1.12916667e-01,  2.79368446e-02,  2.27254763e-02,\n",
    "#         1.08439589e-04, -6.76200913e-02],\n",
    "#       [ 4.58333333e-03, -1.34491976e-01,  1.31382409e-01,\n",
    "#        -1.79823753e-01,  2.45762572e-01]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715a6edd-9bc0-4208-9412-501f4af920db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write code here to perform gradient descent, using your\n",
    "# functions above.  You should use three new variables in your\n",
    "# code:\n",
    "# - W_manual: which is the vector of weights that gradient\n",
    "#   descent is designed to find:\n",
    "# - W_manual_cost, which is the cost of these weights,\n",
    "# - J_list, which is the list of \n",
    "#   costs determined by compute_cost() [like in the in-class lab we did].\n",
    "\n",
    "# Setup these vars:\n",
    "W_manual = np.zeros((3, 5))  # n+1 weights\n",
    "W_manual_cost = 0\n",
    "J_list = []\n",
    "\n",
    "# YOUR CODE HERE\n",
    "    \n",
    "print(\"Final W:\", W_manual)\n",
    "print(\"Final cost:\", W_manual_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288f50b2-120a-4237-a766-56ba211a725b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the cost as a function of number of iterations of the\n",
    "# gradient descent algorithm.\n",
    "\n",
    "plt.scatter(range(0, len(J_list)), J_list)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d098a867-f278-43cf-b5c7-b8632cd88fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep playing around with gradient descent until you have a good\n",
    "# learning curve in the plot above (something that appears to flatten out).\n",
    "# Then answer the questions below. \n",
    "\n",
    "# What was your initial choice for alpha?  Your final choice?  How did\n",
    "# you arrive at these choices?\n",
    "#\n",
    "# YOUR ANSWER HERE\n",
    "#\n",
    "# How many iterations of gradient descent did you need until convergence?\n",
    "#\n",
    "# YOUR ANSWER HERE\n",
    "#\n",
    "# What was your final vector of weights? (W_manual)\n",
    "#\n",
    "# YOUR ANSWER HERE\n",
    "#\n",
    "# What was your final cost of these weights? (W_manual_cost)\n",
    "#\n",
    "# YOUR ANSWER HERE\n",
    "#\n",
    "# What was your final vector of weights from Part A? (W_direct)\n",
    "#\n",
    "# YOUR ANSWER HERE\n",
    "#\n",
    "# What cost of these weights? (W_direct_cost)\n",
    "#\n",
    "# YOUR ANSWER HERE\n",
    "#\n",
    "# How close are your weights from Part B to the \"correct\" weights from Part A?\n",
    "#\n",
    "# YOUR ANSWER HERE\n",
    "#\n",
    "#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ae6946-9bd2-4326-8e40-a4e525fa5197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write code here to compute the accuracy of the new model (the one you just trained)\n",
    "# on the training and testing data sets.  \n",
    "# Save these values to two variables called train_acc_partB and test_acc_partB.\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "train_acc_partB = compute_accuracy(X_train, y_train, W_manual)\n",
    "test_acc_partB = compute_accuracy(X_test, y_test, W_manual)\n",
    "\n",
    "print(train_acc_partB)  \n",
    "print(test_acc_partB)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2cb4166-ca99-4dac-950f-0167e8be5780",
   "metadata": {},
   "outputs": [],
   "source": [
    "## QUESTION:\n",
    "\n",
    "# How does the accuracy of the model you created by hand in Part B compare to \n",
    "# the accuracy of the model from Part A created by scikit-learn?\n",
    "\n",
    "# ANSWER:\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b589d9-162a-4dbc-a4e0-89c1a76d9fd2",
   "metadata": {},
   "source": [
    "## Part C\n",
    "\n",
    "Now, you will adjust your cost function to add in **regularization**.\n",
    "\n",
    "Recall that in regularization, we would like to ensure that our parameters for our model are small, because this\n",
    "tends to reduce the chance of overfitting.  Here, this means we want our entries in our $W$ matrix to be small.\n",
    "\n",
    "To make this happen, we change our cost function from this:\n",
    "\n",
    "$$J(\\boldsymbol{W}) = \\frac{1}{m}\\sum_{i=1}^m L  \\left( \n",
    "    \\hat{\\boldsymbol{y}}^{(i)}, \\boldsymbol{y}^{(i)} \\right)$$\n",
    "\n",
    "to this:\n",
    "\n",
    "$$J(\\boldsymbol{W}) = \\frac{1}{m}\\sum_{i=1}^m L  \\left( \n",
    "    \\hat{\\boldsymbol{y}}^{(i)}, \\boldsymbol{y}^{(i)} \\right) + \\dfrac{\\lambda}{2m}\n",
    "\\sum_{k=1}^K \\sum_{j=1}^n w_{k,j}^2$$\n",
    "\n",
    "That last term may look a little bit different than in the regularization notebook; it's because now the $W$\n",
    "parameters are in a matrix and we must sum over all the rows and columns of the matrix.  The effect is still the\n",
    "same; we want to add up the squared values of all the parameters (in the lecture notebook, $w$ as a vector, so there\n",
    "was only one summation).\n",
    "\n",
    "Remember that we must pick a parameter $\\lambda$.  This parameter controls the strength of the regularization;\n",
    "higher values of $\\lambda$ will cause stronger regularization (forces the parameters to be smaller).  **For this\n",
    "project, we will use $\\lambda=1$.**\n",
    "\n",
    "Because we're changing the cost function, the gradient function $J$ also changes.  The change looks like this:\n",
    "\n",
    "Old formula:\n",
    "\n",
    "$$\\dfrac{\\partial}{\\partial w_{k,j}} J(\\boldsymbol{W}) =  \n",
    "\\frac{1}{m} \\sum_{i=1}^m \\left( \\boldsymbol{\\hat{y}}_k^{(i)} - \\boldsymbol{y}^{(i)}_k \\right)x^{(i)}_j$$\n",
    "\n",
    "New formula:\n",
    "\n",
    "Our formula we want is \n",
    "$$\\dfrac{\\partial}{\\partial w_{k,j}} J(\\boldsymbol{W}) =  \n",
    "\\frac{1}{m} \\sum_{i=1}^m \\left( \\boldsymbol{\\hat{y}}_k^{(i)} - \\boldsymbol{y}^{(i)}_k \\right)x^{(i)}_j + \\frac{\\lambda}{m}w_{k,j}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80c98b4-0879-4442-84ec-323a876bbd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we will have scikit-learn do the regularization for us!\n",
    "# Run this code:\n",
    "\n",
    "# Download and install scikit-learn if not already done:\n",
    "%pip install scikit-learn\n",
    "\n",
    "# Import the logistic regression functionality from scikit-learn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Create a logistic regression model and train it on our training data:\n",
    "model = LogisticRegression(random_state=0, fit_intercept=False, multi_class='multinomial').fit(X_train, y_train)\n",
    "\n",
    "# model.coef_ contains the w vector that this logistic regression model was able to find, so\n",
    "# we'll treat it as the \"best\" w and see if we can match it manually.\n",
    "W_direct_reg = model.coef_\n",
    "\n",
    "print(\"W found through scikit-learn:\", W_direct_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4399e331-b96f-4d99-9d47-82721b7a9457",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redefine the compute_cost function below to compute the\n",
    "# total cost over the entire data set X_data and y_data_one_hot,\n",
    "# given parameters matrix W.\n",
    "# YOU WILL NOW INCLUDE REGULARIZATION, but this is easy.\n",
    "# We will simply call compute_cost() from above, and then add in the \n",
    "# extra summation over all the entries in W (squared).\n",
    "\n",
    "def compute_cost_regularization(X_data, y_data_one_hot, W):\n",
    "    \"\"\"\n",
    "    X_data: matrix (m rows, n+1 cols)\n",
    "    y_data_one_hot: matrix (m rows, K cols)\n",
    "    W: matrix of weights (K rows, n+1 cols)\n",
    "    returns: scalar\n",
    "    \"\"\"\n",
    "        \n",
    "    # YOUR CODE HERE\n",
    "    pass\n",
    "\n",
    "    # hint: call compute_cost(X_data, y_data_one_hot, W) and then\n",
    "    # add in the regularization term.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438c702f-8200-40e7-8eec-89cd1d04911b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check: compute the cost for W_direct_regularization:\n",
    "\n",
    "W_direct_reg_cost = compute_cost_regularization(X_train, y_train_one_hot, W_direct_reg)  \n",
    "# This is the minimum cost we can ever get!  Should be about 0.24649\n",
    "print(W_direct_reg_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36482507-9b4b-4097-af6a-0a8533dacaba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redefine the compute_gradient function below to compute\n",
    "# the complete gradient for the function J(W).  \n",
    "# YOU WILL NOW INCLUDE REGULARIZATION.\n",
    "# You can do this any way you want, but the easiest way is to base it off your\n",
    "# code for compute_gradient(), and make some small changes to add the regularization term.\n",
    "\n",
    "def compute_gradient_regularization(X_data, y_data_one_hot, W):\n",
    "    \"\"\"\n",
    "    X_data: matrix (m rows, n+1 cols)\n",
    "    y_data_one_hot: matrix (m rows, K cols)\n",
    "    W: matrix of weights (K rows, n+1 cols)\n",
    "    returns: matrix of gradients (K rows, n+1 cols)\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    pass\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd3cfb5-d359-452d-b3df-617712629faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check:\n",
    "\n",
    "sample_W = np.ones((3, 5))\n",
    "\n",
    "compute_gradient_regularization(X_train, y_train_one_hot, sample_W)\n",
    "\n",
    "# Should give you \n",
    "#array([[ 0.10958333,  0.10780513, -0.15285789,  0.18096531, -0.17689248],\n",
    "#       [-0.11166667,  0.02918684,  0.02397548,  0.00135844, -0.06637009],\n",
    "#       [ 0.00583333, -0.13324198,  0.13263241, -0.17857375,  0.24701257]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a06205-fac9-48d8-86d6-78a5182ba4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write code here to perform gradient descent, using your\n",
    "# functions above, including regularization.  You should use three new variables in your\n",
    "# code:\n",
    "# - W_manual_reg: which is the vector of weights that gradient\n",
    "#   descent is designed to find:\n",
    "# - W_manual_reg_cost, which is the cost of these weights,\n",
    "# - J_list, which is the list of \n",
    "#   costs determined by compute_cost() [like in the in-class lab we did].\n",
    "\n",
    "# Setup these vars:\n",
    "W_manual_reg = np.zeros((3, 5))  \n",
    "W_manual_reg_cost = 0\n",
    "J_list = []\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "\n",
    "print(\"Final w:\", W_manual_reg)\n",
    "print(\"Final cost:\", W_manual_reg_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f9efe4-c253-4432-8a67-0b2c399e7ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the cost as a function of number of iterations of the\n",
    "# gradient descent algorithm.\n",
    "\n",
    "plt.scatter(range(0, len(J_list)), J_list)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25bda072-8949-4800-9993-29a761ffc85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write code here to compute the accuracy of the new model (the one you just trained)\n",
    "# on the training and testing data sets.  \n",
    "# Save these values to two variables called train_acc_partC and test_acc_partC.\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "train_acc_partC = compute_accuracy(X_train, y_train, W_manual_reg)\n",
    "test_acc_partC = compute_accuracy(X_test, y_test, W_manual_reg)\n",
    "\n",
    "print(train_acc_partC)  \n",
    "print(test_acc_partC)  \n",
    "\n",
    "# You should end up with a **higher** testing accuracy in part C than in parts A and B!\n",
    "# This illustrates that regularization has helped us avoid overfitting, because the training\n",
    "# accuracy should be identical in A/B/C (or almost identical), but test accuracy will be \n",
    "# higher in C."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4b170b-4e6e-4b7c-aa53-9f5030ee506c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final checkpoint\n",
    "\n",
    "# All of these should print OK and match up with what you have above:\n",
    "\n",
    "print(\"Part A\")\n",
    "print(\"Weights:\", W_direct)\n",
    "print(\"Cost:\", W_direct_cost)\n",
    "print(\"Training accuracy:\", train_acc_partA)\n",
    "print(\"Testing accuracy:\", test_acc_partA)\n",
    "print()\n",
    "print(\"Part B\")\n",
    "print(\"Weights:\", W_manual)\n",
    "print(\"Cost:\", W_manual_cost)\n",
    "print(\"Training accuracy:\", train_acc_partB)\n",
    "print(\"Testing accuracy:\", test_acc_partB)\n",
    "print()\n",
    "print(\"Part C\")\n",
    "print(\"Weights:\", W_manual_reg)\n",
    "print(\"Cost:\", W_manual_reg_cost)\n",
    "print(\"Training accuracy:\", train_acc_partC)\n",
    "print(\"Testing accuracy:\", test_acc_partC)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c2dedc-9bb1-4dbd-b9fe-f1685ff9885a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
